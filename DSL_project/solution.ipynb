{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import helpers.processing_helpers as ph\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(\"./dataset/development.csv\")\n",
    "df_ev = pd.read_csv(\"./dataset/evaluation.csv\")\n",
    "\n",
    "acc_idxs = [1,2,3,4,5,6,8,9,10,11,13,14]\n",
    "noise_indexes = [0,7,12,15,16,17]\n",
    "\n",
    "\n",
    "\n",
    "features = ['pmax', 'negpmax', 'area', 'tmax', 'rms']\n",
    "\n",
    "drop_features = ['tmax', 'rms', 'area']\n",
    "\n",
    "df_dev = df_dev.drop(columns=ph.get_column_names(features, noise_indexes)) \n",
    "df_dev = df_dev.drop(columns=ph.get_column_names(drop_features, acc_idxs))\n",
    "\n",
    "df_ev = df_ev.drop(columns=ph.get_column_names(features, noise_indexes))\n",
    "df_ev = df_ev.drop(columns=ph.get_column_names(drop_features, acc_idxs))\n",
    "df_ev = df_ev.drop(columns=\"Id\")\n",
    "\n",
    "X_train = df_dev.drop(columns=['x', 'y'])\n",
    "Y_train = df_dev[['x', 'y']]\n",
    "\n",
    "\n",
    "features = X_train.columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = pd.DataFrame(scaler.fit_transform(X_train), columns=features)\n",
    "eval_normalized = pd.DataFrame(scaler.transform(df_ev), columns=features)\n",
    "\n",
    "\n",
    "outlier_clr = LocalOutlierFactor(contamination=0.025)\n",
    "\n",
    "outlier_scores = outlier_clr.fit_predict(X_train_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmax[1]</th>\n",
       "      <th>negpmax[1]</th>\n",
       "      <th>pmax[2]</th>\n",
       "      <th>negpmax[2]</th>\n",
       "      <th>pmax[3]</th>\n",
       "      <th>negpmax[3]</th>\n",
       "      <th>pmax[4]</th>\n",
       "      <th>negpmax[4]</th>\n",
       "      <th>pmax[5]</th>\n",
       "      <th>negpmax[5]</th>\n",
       "      <th>...</th>\n",
       "      <th>negpmax[9]</th>\n",
       "      <th>pmax[10]</th>\n",
       "      <th>negpmax[10]</th>\n",
       "      <th>pmax[11]</th>\n",
       "      <th>negpmax[11]</th>\n",
       "      <th>pmax[13]</th>\n",
       "      <th>negpmax[13]</th>\n",
       "      <th>pmax[14]</th>\n",
       "      <th>negpmax[14]</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.711719</td>\n",
       "      <td>0.106909</td>\n",
       "      <td>-0.441961</td>\n",
       "      <td>0.009510</td>\n",
       "      <td>-0.848685</td>\n",
       "      <td>0.137472</td>\n",
       "      <td>-0.832614</td>\n",
       "      <td>0.031988</td>\n",
       "      <td>-1.141005</td>\n",
       "      <td>0.858277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.748229</td>\n",
       "      <td>0.083624</td>\n",
       "      <td>-0.019371</td>\n",
       "      <td>-0.595660</td>\n",
       "      <td>0.081664</td>\n",
       "      <td>-1.210932</td>\n",
       "      <td>0.464910</td>\n",
       "      <td>-0.602589</td>\n",
       "      <td>0.039315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.684792</td>\n",
       "      <td>0.165512</td>\n",
       "      <td>-0.600365</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>-0.892459</td>\n",
       "      <td>0.131305</td>\n",
       "      <td>-0.753976</td>\n",
       "      <td>0.038710</td>\n",
       "      <td>-1.036258</td>\n",
       "      <td>0.882947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.746272</td>\n",
       "      <td>-0.001023</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>-0.519378</td>\n",
       "      <td>0.068776</td>\n",
       "      <td>-1.171136</td>\n",
       "      <td>0.448051</td>\n",
       "      <td>-0.574763</td>\n",
       "      <td>0.028081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.790196</td>\n",
       "      <td>0.113647</td>\n",
       "      <td>-0.526214</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>-0.840038</td>\n",
       "      <td>0.139111</td>\n",
       "      <td>-0.890003</td>\n",
       "      <td>0.020064</td>\n",
       "      <td>-0.959713</td>\n",
       "      <td>0.902269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.828382</td>\n",
       "      <td>0.145767</td>\n",
       "      <td>0.004424</td>\n",
       "      <td>-0.377900</td>\n",
       "      <td>0.067464</td>\n",
       "      <td>-1.161445</td>\n",
       "      <td>0.502746</td>\n",
       "      <td>-0.684512</td>\n",
       "      <td>0.029485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.761922</td>\n",
       "      <td>0.193033</td>\n",
       "      <td>-0.605966</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>-0.817495</td>\n",
       "      <td>0.138789</td>\n",
       "      <td>-0.827618</td>\n",
       "      <td>0.022207</td>\n",
       "      <td>-1.088317</td>\n",
       "      <td>0.892385</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660310</td>\n",
       "      <td>0.180483</td>\n",
       "      <td>0.015728</td>\n",
       "      <td>-0.583469</td>\n",
       "      <td>0.062709</td>\n",
       "      <td>-1.200117</td>\n",
       "      <td>0.484278</td>\n",
       "      <td>-0.725118</td>\n",
       "      <td>0.027887</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.714769</td>\n",
       "      <td>0.152889</td>\n",
       "      <td>-0.434056</td>\n",
       "      <td>0.003651</td>\n",
       "      <td>-0.865775</td>\n",
       "      <td>0.140308</td>\n",
       "      <td>-0.800334</td>\n",
       "      <td>0.029229</td>\n",
       "      <td>-0.965757</td>\n",
       "      <td>0.845611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.777175</td>\n",
       "      <td>0.129421</td>\n",
       "      <td>-0.028425</td>\n",
       "      <td>-0.537785</td>\n",
       "      <td>0.072425</td>\n",
       "      <td>-1.193584</td>\n",
       "      <td>0.503548</td>\n",
       "      <td>-0.686299</td>\n",
       "      <td>0.031184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385494</th>\n",
       "      <td>2.320114</td>\n",
       "      <td>-0.724913</td>\n",
       "      <td>7.784918</td>\n",
       "      <td>-0.164526</td>\n",
       "      <td>1.729602</td>\n",
       "      <td>-0.363508</td>\n",
       "      <td>-0.188855</td>\n",
       "      <td>0.028398</td>\n",
       "      <td>-1.071899</td>\n",
       "      <td>0.876243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063022</td>\n",
       "      <td>-1.110332</td>\n",
       "      <td>0.126261</td>\n",
       "      <td>-0.665714</td>\n",
       "      <td>0.094045</td>\n",
       "      <td>-0.772206</td>\n",
       "      <td>0.462486</td>\n",
       "      <td>-0.352577</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385495</th>\n",
       "      <td>1.984914</td>\n",
       "      <td>-0.570141</td>\n",
       "      <td>6.092468</td>\n",
       "      <td>-0.140522</td>\n",
       "      <td>1.443518</td>\n",
       "      <td>-0.335376</td>\n",
       "      <td>-0.224333</td>\n",
       "      <td>0.045489</td>\n",
       "      <td>-0.948231</td>\n",
       "      <td>0.868763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043895</td>\n",
       "      <td>-1.093250</td>\n",
       "      <td>0.132743</td>\n",
       "      <td>-0.768189</td>\n",
       "      <td>0.055052</td>\n",
       "      <td>-0.778056</td>\n",
       "      <td>0.414618</td>\n",
       "      <td>-0.608070</td>\n",
       "      <td>0.037437</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385496</th>\n",
       "      <td>2.056252</td>\n",
       "      <td>-0.565863</td>\n",
       "      <td>6.408689</td>\n",
       "      <td>-0.167727</td>\n",
       "      <td>1.319551</td>\n",
       "      <td>-0.347221</td>\n",
       "      <td>-0.253281</td>\n",
       "      <td>0.028416</td>\n",
       "      <td>-1.061089</td>\n",
       "      <td>0.844477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054937</td>\n",
       "      <td>-0.857272</td>\n",
       "      <td>0.132563</td>\n",
       "      <td>-0.319939</td>\n",
       "      <td>0.073038</td>\n",
       "      <td>-0.844124</td>\n",
       "      <td>0.471563</td>\n",
       "      <td>-0.272023</td>\n",
       "      <td>0.031033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385497</th>\n",
       "      <td>1.796743</td>\n",
       "      <td>-0.726257</td>\n",
       "      <td>6.896955</td>\n",
       "      <td>-0.173545</td>\n",
       "      <td>1.866020</td>\n",
       "      <td>-0.356328</td>\n",
       "      <td>0.144617</td>\n",
       "      <td>0.028420</td>\n",
       "      <td>-0.940133</td>\n",
       "      <td>0.899318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041449</td>\n",
       "      <td>-1.040976</td>\n",
       "      <td>0.140699</td>\n",
       "      <td>-0.776684</td>\n",
       "      <td>0.079367</td>\n",
       "      <td>-0.730563</td>\n",
       "      <td>0.448717</td>\n",
       "      <td>-0.367520</td>\n",
       "      <td>0.041807</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385499</th>\n",
       "      <td>2.233336</td>\n",
       "      <td>-0.581834</td>\n",
       "      <td>6.628403</td>\n",
       "      <td>-0.151738</td>\n",
       "      <td>1.492015</td>\n",
       "      <td>-0.343241</td>\n",
       "      <td>-0.031217</td>\n",
       "      <td>-0.080242</td>\n",
       "      <td>-0.918205</td>\n",
       "      <td>0.076523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167940</td>\n",
       "      <td>-1.095215</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.703022</td>\n",
       "      <td>-0.025023</td>\n",
       "      <td>-0.713666</td>\n",
       "      <td>0.132727</td>\n",
       "      <td>-0.548842</td>\n",
       "      <td>-0.066677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375862 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pmax[1]  negpmax[1]   pmax[2]  negpmax[2]   pmax[3]  negpmax[3]  \\\n",
       "2      -0.711719    0.106909 -0.441961    0.009510 -0.848685    0.137472   \n",
       "3      -0.684792    0.165512 -0.600365    0.000120 -0.892459    0.131305   \n",
       "4      -0.790196    0.113647 -0.526214    0.002648 -0.840038    0.139111   \n",
       "5      -0.761922    0.193033 -0.605966    0.002262 -0.817495    0.138789   \n",
       "6      -0.714769    0.152889 -0.434056    0.003651 -0.865775    0.140308   \n",
       "...          ...         ...       ...         ...       ...         ...   \n",
       "385494  2.320114   -0.724913  7.784918   -0.164526  1.729602   -0.363508   \n",
       "385495  1.984914   -0.570141  6.092468   -0.140522  1.443518   -0.335376   \n",
       "385496  2.056252   -0.565863  6.408689   -0.167727  1.319551   -0.347221   \n",
       "385497  1.796743   -0.726257  6.896955   -0.173545  1.866020   -0.356328   \n",
       "385499  2.233336   -0.581834  6.628403   -0.151738  1.492015   -0.343241   \n",
       "\n",
       "         pmax[4]  negpmax[4]   pmax[5]  negpmax[5]  ...  negpmax[9]  pmax[10]  \\\n",
       "2      -0.832614    0.031988 -1.141005    0.858277  ...   -0.748229  0.083624   \n",
       "3      -0.753976    0.038710 -1.036258    0.882947  ...   -0.746272 -0.001023   \n",
       "4      -0.890003    0.020064 -0.959713    0.902269  ...   -0.828382  0.145767   \n",
       "5      -0.827618    0.022207 -1.088317    0.892385  ...   -0.660310  0.180483   \n",
       "6      -0.800334    0.029229 -0.965757    0.845611  ...   -0.777175  0.129421   \n",
       "...          ...         ...       ...         ...  ...         ...       ...   \n",
       "385494 -0.188855    0.028398 -1.071899    0.876243  ...    0.063022 -1.110332   \n",
       "385495 -0.224333    0.045489 -0.948231    0.868763  ...    0.043895 -1.093250   \n",
       "385496 -0.253281    0.028416 -1.061089    0.844477  ...    0.054937 -0.857272   \n",
       "385497  0.144617    0.028420 -0.940133    0.899318  ...    0.041449 -1.040976   \n",
       "385499 -0.031217   -0.080242 -0.918205    0.076523  ...   -0.167940 -1.095215   \n",
       "\n",
       "        negpmax[10]  pmax[11]  negpmax[11]  pmax[13]  negpmax[13]  pmax[14]  \\\n",
       "2         -0.019371 -0.595660     0.081664 -1.210932     0.464910 -0.602589   \n",
       "3          0.000834 -0.519378     0.068776 -1.171136     0.448051 -0.574763   \n",
       "4          0.004424 -0.377900     0.067464 -1.161445     0.502746 -0.684512   \n",
       "5          0.015728 -0.583469     0.062709 -1.200117     0.484278 -0.725118   \n",
       "6         -0.028425 -0.537785     0.072425 -1.193584     0.503548 -0.686299   \n",
       "...             ...       ...          ...       ...          ...       ...   \n",
       "385494     0.126261 -0.665714     0.094045 -0.772206     0.462486 -0.352577   \n",
       "385495     0.132743 -0.768189     0.055052 -0.778056     0.414618 -0.608070   \n",
       "385496     0.132563 -0.319939     0.073038 -0.844124     0.471563 -0.272023   \n",
       "385497     0.140699 -0.776684     0.079367 -0.730563     0.448717 -0.367520   \n",
       "385499    -0.000219 -0.703022    -0.025023 -0.713666     0.132727 -0.548842   \n",
       "\n",
       "        negpmax[14]  score  \n",
       "2          0.039315      1  \n",
       "3          0.028081      1  \n",
       "4          0.029485      1  \n",
       "5          0.027887      1  \n",
       "6          0.031184      1  \n",
       "...             ...    ...  \n",
       "385494     0.034247      1  \n",
       "385495     0.037437      1  \n",
       "385496     0.031033      1  \n",
       "385497     0.041807      1  \n",
       "385499    -0.066677      1  \n",
       "\n",
       "[375862 rows x 25 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outlier_scores = pd.DataFrame(outlier_scores, columns=['score'])\n",
    "X_train_normalized = pd.concat([X_train_normalized, df_outlier_scores], axis=1)\n",
    "score_mask = X_train_normalized['score'] >= 0\n",
    "X_train_normalized = X_train_normalized[score_mask]\n",
    "X_train_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.concat([Y_train, df_outlier_scores], axis=1)\n",
    "Y_train = Y_train[score_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized.drop(columns=['score'], inplace=True)\n",
    "Y_train.drop(columns=['score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_5(n):\n",
    "    return np.round(n / 5) * 5\n",
    "\n",
    "\n",
    "rounding_vectorized = np.vectorize(round_to_nearest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1485.52852615\n",
      "Iteration 2, loss = 17.72914133\n",
      "Iteration 3, loss = 12.10251607\n",
      "Iteration 4, loss = 9.89436631\n",
      "Iteration 5, loss = 8.71635467\n",
      "Iteration 6, loss = 8.02532817\n",
      "Iteration 7, loss = 7.58205947\n",
      "Iteration 8, loss = 7.25842629\n",
      "Iteration 9, loss = 7.07422601\n",
      "Iteration 10, loss = 6.95262003\n",
      "Iteration 11, loss = 6.81105410\n",
      "Iteration 12, loss = 6.74153248\n",
      "Iteration 13, loss = 6.64740434\n",
      "Iteration 14, loss = 6.57110387\n",
      "Iteration 15, loss = 6.52566222\n",
      "Iteration 16, loss = 6.49409898\n",
      "Iteration 17, loss = 6.41563797\n",
      "Iteration 18, loss = 6.38462040\n",
      "Iteration 19, loss = 6.36518737\n",
      "Iteration 20, loss = 6.30408166\n",
      "Iteration 21, loss = 6.29002830\n",
      "Iteration 22, loss = 6.27194354\n",
      "Iteration 23, loss = 6.25119889\n",
      "Iteration 24, loss = 6.21637648\n",
      "Iteration 25, loss = 6.17304328\n",
      "Iteration 26, loss = 6.16822953\n",
      "Iteration 27, loss = 6.13691516\n",
      "Iteration 28, loss = 6.12604612\n",
      "Iteration 29, loss = 6.11212882\n",
      "Iteration 30, loss = 6.08366048\n",
      "Iteration 31, loss = 6.09056805\n",
      "Iteration 32, loss = 6.05478107\n",
      "Iteration 33, loss = 6.04716940\n",
      "Iteration 34, loss = 6.04276382\n",
      "Iteration 35, loss = 6.01562587\n",
      "Iteration 36, loss = 6.01213715\n",
      "Iteration 37, loss = 6.01189563\n",
      "Iteration 38, loss = 5.97974954\n",
      "Iteration 39, loss = 5.95584890\n",
      "Iteration 40, loss = 5.98163626\n",
      "Iteration 41, loss = 5.95337609\n",
      "Iteration 42, loss = 5.95503808\n",
      "Iteration 43, loss = 5.92321455\n",
      "Iteration 44, loss = 5.93385080\n",
      "Iteration 45, loss = 5.92197046\n",
      "Iteration 46, loss = 5.93922586\n",
      "Iteration 47, loss = 5.92160600\n",
      "Iteration 48, loss = 5.90795778\n",
      "Iteration 49, loss = 5.89517090\n",
      "Iteration 50, loss = 5.88022704\n",
      "Iteration 51, loss = 5.89046973\n",
      "Iteration 52, loss = 5.87470023\n",
      "Iteration 53, loss = 5.85648422\n",
      "Iteration 54, loss = 5.87397445\n",
      "Iteration 55, loss = 5.84937888\n",
      "Iteration 56, loss = 5.84553115\n",
      "Iteration 57, loss = 5.82851554\n",
      "Iteration 58, loss = 5.84207245\n",
      "Iteration 59, loss = 5.82554081\n",
      "Iteration 60, loss = 5.81899730\n",
      "Iteration 61, loss = 5.80215859\n",
      "Iteration 62, loss = 5.83498176\n",
      "Iteration 63, loss = 5.80380838\n",
      "Iteration 64, loss = 5.80581074\n",
      "Iteration 65, loss = 5.80642494\n",
      "Iteration 66, loss = 5.79991199\n",
      "Iteration 67, loss = 5.78914300\n",
      "Iteration 68, loss = 5.79645430\n",
      "Iteration 69, loss = 5.76685192\n",
      "Iteration 70, loss = 5.80363057\n",
      "Iteration 71, loss = 5.74966975\n",
      "Iteration 72, loss = 5.77287347\n",
      "Iteration 73, loss = 5.78276139\n",
      "Iteration 74, loss = 5.77107131\n",
      "Iteration 75, loss = 5.76048940\n",
      "Iteration 76, loss = 5.77117627\n",
      "Iteration 77, loss = 5.73748112\n",
      "Iteration 78, loss = 5.73553064\n",
      "Iteration 79, loss = 5.75561738\n",
      "Iteration 80, loss = 5.74747929\n",
      "Iteration 81, loss = 5.74502763\n",
      "Iteration 82, loss = 5.74124812\n",
      "Iteration 83, loss = 5.75590607\n",
      "Iteration 84, loss = 5.73473544\n",
      "Iteration 85, loss = 5.72212409\n",
      "Iteration 86, loss = 5.73943954\n",
      "Iteration 87, loss = 5.71978091\n",
      "Iteration 88, loss = 5.72691885\n",
      "Iteration 89, loss = 5.72018539\n",
      "Iteration 90, loss = 5.70745601\n",
      "Iteration 91, loss = 5.69287285\n",
      "Iteration 92, loss = 5.72200424\n",
      "Iteration 93, loss = 5.71407530\n",
      "Iteration 94, loss = 5.70052678\n",
      "Iteration 95, loss = 5.71494371\n",
      "Iteration 96, loss = 5.71524577\n",
      "Iteration 97, loss = 5.69598480\n",
      "Iteration 98, loss = 5.69234156\n",
      "Iteration 99, loss = 5.70579172\n",
      "Iteration 100, loss = 5.68448648\n",
      "Iteration 101, loss = 5.70389783\n",
      "Iteration 102, loss = 5.68812518\n",
      "Iteration 103, loss = 5.69074619\n",
      "Iteration 104, loss = 5.68575401\n",
      "Iteration 105, loss = 5.67462957\n",
      "Iteration 106, loss = 5.70013775\n",
      "Iteration 107, loss = 5.67266018\n",
      "Iteration 108, loss = 5.68519032\n",
      "Iteration 109, loss = 5.67447185\n",
      "Iteration 110, loss = 5.68573369\n",
      "Iteration 111, loss = 5.68889063\n",
      "Iteration 112, loss = 5.64842394\n",
      "Iteration 113, loss = 5.67885379\n",
      "Iteration 114, loss = 5.68046419\n",
      "Iteration 115, loss = 5.66710978\n",
      "Iteration 116, loss = 5.66070633\n",
      "Iteration 117, loss = 5.66402051\n",
      "Iteration 118, loss = 5.65358786\n",
      "Iteration 119, loss = 5.65028660\n",
      "Iteration 120, loss = 5.65465085\n",
      "Iteration 121, loss = 5.67055648\n",
      "Iteration 122, loss = 5.65539718\n",
      "Iteration 123, loss = 5.66431591\n",
      "Iteration 124, loss = 5.63904784\n",
      "Iteration 125, loss = 5.64470740\n",
      "Iteration 126, loss = 5.65340175\n",
      "Iteration 127, loss = 5.66436024\n",
      "Iteration 128, loss = 5.64623273\n",
      "Iteration 129, loss = 5.66267802\n",
      "Iteration 130, loss = 5.64812912\n",
      "Iteration 131, loss = 5.64866077\n",
      "Iteration 132, loss = 5.63463345\n",
      "Iteration 133, loss = 5.63946898\n",
      "Iteration 134, loss = 5.64818203\n",
      "Iteration 135, loss = 5.64115215\n",
      "Iteration 136, loss = 5.64659763\n",
      "Iteration 137, loss = 5.64939400\n",
      "Iteration 138, loss = 5.64044864\n",
      "Iteration 139, loss = 5.63983961\n",
      "Iteration 140, loss = 5.62886910\n",
      "Iteration 141, loss = 5.64761437\n",
      "Iteration 142, loss = 5.63296174\n",
      "Iteration 143, loss = 5.64584421\n",
      "Iteration 144, loss = 5.63837795\n",
      "Iteration 145, loss = 5.61617740\n",
      "Iteration 146, loss = 5.61996768\n",
      "Iteration 147, loss = 5.61936357\n",
      "Iteration 148, loss = 5.63248360\n",
      "Iteration 149, loss = 5.61832595\n",
      "Iteration 150, loss = 5.62788937\n",
      "Iteration 151, loss = 5.62212516\n",
      "Iteration 152, loss = 5.61441236\n",
      "Iteration 153, loss = 5.62576456\n",
      "Iteration 154, loss = 5.61673897\n",
      "Iteration 155, loss = 5.62540230\n",
      "Iteration 156, loss = 5.61549181\n",
      "Iteration 157, loss = 5.61423805\n",
      "Iteration 158, loss = 5.61108487\n",
      "Iteration 159, loss = 5.62316128\n",
      "Iteration 160, loss = 5.61630269\n",
      "Iteration 161, loss = 5.63419920\n",
      "Iteration 162, loss = 5.60330106\n",
      "Iteration 163, loss = 5.60326363\n",
      "Iteration 164, loss = 5.62120705\n",
      "Iteration 165, loss = 5.61855802\n",
      "Iteration 166, loss = 5.60944066\n",
      "Iteration 167, loss = 5.61633552\n",
      "Iteration 168, loss = 5.60876060\n",
      "Iteration 169, loss = 5.63154335\n",
      "Iteration 170, loss = 5.60852298\n",
      "Iteration 171, loss = 5.60589646\n",
      "Iteration 172, loss = 5.61224534\n",
      "Iteration 173, loss = 5.60875049\n",
      "Iteration 174, loss = 5.60673992\n",
      "Iteration 175, loss = 5.59298236\n",
      "Iteration 176, loss = 5.61537315\n",
      "Iteration 177, loss = 5.61368423\n",
      "Iteration 178, loss = 5.59515550\n",
      "Iteration 179, loss = 5.60641552\n",
      "Iteration 180, loss = 5.60947899\n",
      "Iteration 181, loss = 5.59631779\n",
      "Iteration 182, loss = 5.62096194\n",
      "Iteration 183, loss = 5.57689486\n",
      "Iteration 184, loss = 5.58748871\n",
      "Iteration 185, loss = 5.58575868\n",
      "Iteration 186, loss = 5.60945912\n",
      "Iteration 187, loss = 5.59338243\n",
      "Iteration 188, loss = 5.62055835\n",
      "Iteration 189, loss = 5.58777188\n",
      "Iteration 190, loss = 5.61659976\n",
      "Iteration 191, loss = 5.59322503\n",
      "Iteration 192, loss = 5.61228166\n",
      "Iteration 193, loss = 5.60135898\n",
      "Iteration 194, loss = 5.59819856\n",
      "Iteration 195, loss = 5.61030467\n",
      "Iteration 196, loss = 5.58996659\n",
      "Iteration 197, loss = 5.60629277\n",
      "Iteration 198, loss = 5.59483052\n",
      "Iteration 199, loss = 5.60236574\n",
      "Iteration 200, loss = 5.60307469\n",
      "Iteration 201, loss = 5.60229535\n",
      "Iteration 202, loss = 5.59984031\n",
      "Iteration 203, loss = 5.58358302\n",
      "Iteration 204, loss = 5.58660235\n",
      "Iteration 205, loss = 5.60431195\n",
      "Iteration 206, loss = 5.60364208\n",
      "Iteration 207, loss = 5.58047549\n",
      "Iteration 208, loss = 5.60093536\n",
      "Iteration 209, loss = 5.59284383\n",
      "Iteration 210, loss = 5.58947561\n",
      "Iteration 211, loss = 5.59684550\n",
      "Iteration 212, loss = 5.58808017\n",
      "Iteration 213, loss = 5.58938230\n",
      "Iteration 214, loss = 5.59967202\n",
      "Iteration 215, loss = 5.58944036\n",
      "Iteration 216, loss = 5.58157831\n",
      "Iteration 217, loss = 5.58482378\n",
      "Iteration 218, loss = 5.57825458\n",
      "Iteration 219, loss = 5.59799871\n",
      "Iteration 220, loss = 5.60250565\n",
      "Iteration 221, loss = 5.56160887\n",
      "Iteration 222, loss = 5.61000303\n",
      "Iteration 223, loss = 5.58029201\n",
      "Iteration 224, loss = 5.59335402\n",
      "Iteration 225, loss = 5.58598963\n",
      "Iteration 226, loss = 5.60577487\n",
      "Iteration 227, loss = 5.58712496\n",
      "Iteration 228, loss = 5.59600005\n",
      "Iteration 229, loss = 5.58062769\n",
      "Iteration 230, loss = 5.57799903\n",
      "Iteration 231, loss = 5.59451392\n",
      "Iteration 232, loss = 5.58223543\n",
      "Iteration 233, loss = 5.58154063\n",
      "Iteration 234, loss = 5.58594114\n",
      "Iteration 235, loss = 5.58483847\n",
      "Iteration 236, loss = 5.57443835\n",
      "Iteration 237, loss = 5.58673030\n",
      "Iteration 238, loss = 5.58563337\n",
      "Iteration 239, loss = 5.57241534\n",
      "Iteration 240, loss = 5.58635069\n",
      "Iteration 241, loss = 5.58904081\n",
      "Iteration 242, loss = 5.59094018\n",
      "Iteration 243, loss = 5.57694962\n",
      "Iteration 244, loss = 5.58015682\n",
      "Iteration 245, loss = 5.57676552\n",
      "Iteration 246, loss = 5.59225730\n",
      "Iteration 247, loss = 5.56725423\n",
      "Iteration 248, loss = 5.59202577\n",
      "Iteration 249, loss = 5.57834128\n",
      "Iteration 250, loss = 5.59004931\n",
      "Iteration 251, loss = 5.57800250\n",
      "Iteration 252, loss = 5.57866498\n",
      "Iteration 253, loss = 5.56518461\n",
      "Iteration 254, loss = 5.58457000\n",
      "Iteration 255, loss = 5.58828421\n",
      "Iteration 256, loss = 5.57472890\n",
      "Iteration 257, loss = 5.58201607\n",
      "Iteration 258, loss = 5.59014936\n",
      "Iteration 259, loss = 5.58903088\n",
      "Iteration 260, loss = 5.57728639\n",
      "Iteration 261, loss = 5.58431409\n",
      "Iteration 262, loss = 5.56331448\n",
      "Iteration 263, loss = 5.58875282\n",
      "Iteration 264, loss = 5.56963626\n",
      "Iteration 265, loss = 5.59808771\n",
      "Iteration 266, loss = 5.57123597\n",
      "Iteration 267, loss = 5.57432152\n",
      "Iteration 268, loss = 5.58644251\n",
      "Iteration 269, loss = 5.58282050\n",
      "Iteration 270, loss = 5.57732839\n",
      "Iteration 271, loss = 5.57124925\n",
      "Iteration 272, loss = 5.58436369\n",
      "Iteration 273, loss = 5.57584993\n",
      "Iteration 274, loss = 5.57237397\n",
      "Iteration 275, loss = 5.57750935\n",
      "Iteration 276, loss = 5.59137523\n",
      "Iteration 277, loss = 5.57795121\n",
      "Iteration 278, loss = 5.57076883\n",
      "Iteration 279, loss = 5.56445597\n",
      "Iteration 280, loss = 5.56621368\n",
      "Iteration 281, loss = 5.57012583\n",
      "Iteration 282, loss = 5.57972021\n",
      "Iteration 283, loss = 5.58547958\n",
      "Iteration 284, loss = 5.57228826\n",
      "Iteration 285, loss = 5.59106507\n",
      "Iteration 286, loss = 5.56949450\n",
      "Iteration 287, loss = 5.57520986\n",
      "Iteration 288, loss = 5.57311961\n",
      "Iteration 289, loss = 5.57614327\n",
      "Iteration 290, loss = 5.55265986\n",
      "Iteration 291, loss = 5.58520513\n",
      "Iteration 292, loss = 5.56128346\n",
      "Iteration 293, loss = 5.56934020\n",
      "Iteration 294, loss = 5.58190842\n",
      "Iteration 295, loss = 5.58712223\n",
      "Iteration 296, loss = 5.57921043\n",
      "Iteration 297, loss = 5.58140460\n",
      "Iteration 298, loss = 5.55647496\n",
      "Iteration 299, loss = 5.57344993\n",
      "Iteration 300, loss = 5.57431290\n",
      "Iteration 301, loss = 5.57072435\n",
      "Iteration 302, loss = 5.56653110\n",
      "Iteration 303, loss = 5.57431755\n",
      "Iteration 304, loss = 5.58703406\n",
      "Iteration 305, loss = 5.56703235\n",
      "Iteration 306, loss = 5.56977058\n",
      "Iteration 307, loss = 5.56923360\n",
      "Iteration 308, loss = 5.56274771\n",
      "Iteration 309, loss = 5.57026433\n",
      "Iteration 310, loss = 5.58487196\n",
      "Iteration 311, loss = 5.55939714\n",
      "Iteration 312, loss = 5.58324744\n",
      "Iteration 313, loss = 5.56371296\n",
      "Iteration 314, loss = 5.57643460\n",
      "Iteration 315, loss = 5.57612860\n",
      "Iteration 316, loss = 5.56254140\n",
      "Iteration 317, loss = 5.57526016\n",
      "Iteration 318, loss = 5.57182669\n",
      "Iteration 319, loss = 5.56183346\n",
      "Iteration 320, loss = 5.57514947\n",
      "Iteration 321, loss = 5.56926179\n",
      "Iteration 322, loss = 5.57286165\n",
      "Iteration 323, loss = 5.55760242\n",
      "Iteration 324, loss = 5.56859956\n",
      "Iteration 325, loss = 5.55811211\n",
      "Iteration 326, loss = 5.57636106\n",
      "Iteration 327, loss = 5.57742782\n",
      "Iteration 328, loss = 5.57842008\n",
      "Iteration 329, loss = 5.59202694\n",
      "Iteration 330, loss = 5.56495356\n",
      "Iteration 331, loss = 5.56159044\n",
      "Iteration 332, loss = 5.57252345\n",
      "Iteration 333, loss = 5.55844746\n",
      "Iteration 334, loss = 5.56675190\n",
      "Iteration 335, loss = 5.58486313\n",
      "Iteration 336, loss = 5.57339033\n",
      "Iteration 337, loss = 5.55718981\n",
      "Iteration 338, loss = 5.57260338\n",
      "Iteration 339, loss = 5.56388096\n",
      "Iteration 340, loss = 5.57214268\n",
      "Iteration 341, loss = 5.57437715\n",
      "Iteration 342, loss = 5.58155226\n",
      "Iteration 343, loss = 5.56327632\n",
      "Iteration 344, loss = 5.58719557\n",
      "Iteration 345, loss = 5.56760155\n",
      "Iteration 346, loss = 5.57305592\n",
      "Iteration 347, loss = 5.55791891\n",
      "Iteration 348, loss = 5.56028592\n",
      "Iteration 349, loss = 5.56800398\n",
      "Iteration 350, loss = 5.55274188\n",
      "Iteration 351, loss = 5.56599044\n",
      "Iteration 352, loss = 5.58137245\n",
      "Iteration 353, loss = 5.56203846\n",
      "Iteration 354, loss = 5.56915228\n",
      "Iteration 355, loss = 5.56756756\n",
      "Iteration 356, loss = 5.56333191\n",
      "Iteration 357, loss = 5.55628603\n",
      "Iteration 358, loss = 5.55470964\n",
      "Iteration 359, loss = 5.56685787\n",
      "Iteration 360, loss = 5.56775247\n",
      "Iteration 361, loss = 5.56014453\n",
      "Iteration 362, loss = 5.57252655\n",
      "Iteration 363, loss = 5.55637123\n",
      "Iteration 364, loss = 5.57695277\n",
      "Iteration 365, loss = 5.57777746\n",
      "Iteration 366, loss = 5.56338259\n",
      "Iteration 367, loss = 5.56515748\n",
      "Iteration 368, loss = 5.56743235\n",
      "Iteration 369, loss = 5.56089463\n",
      "Iteration 370, loss = 5.56250613\n",
      "Iteration 371, loss = 5.57582967\n",
      "Iteration 372, loss = 5.55776393\n",
      "Iteration 373, loss = 5.55475890\n",
      "Iteration 374, loss = 5.56661789\n",
      "Iteration 375, loss = 5.57385699\n",
      "Iteration 376, loss = 5.55281884\n",
      "Iteration 377, loss = 5.56047043\n",
      "Iteration 378, loss = 5.56070512\n",
      "Iteration 379, loss = 5.56037005\n",
      "Iteration 380, loss = 5.57345308\n",
      "Iteration 381, loss = 5.54926561\n",
      "Iteration 382, loss = 5.55493522\n",
      "Iteration 383, loss = 5.56118415\n",
      "Iteration 384, loss = 5.56057082\n",
      "Iteration 385, loss = 5.57403200\n",
      "Iteration 386, loss = 5.56325274\n",
      "Iteration 387, loss = 5.56190838\n",
      "Iteration 388, loss = 5.56541281\n",
      "Iteration 389, loss = 5.56779790\n",
      "Iteration 390, loss = 5.55602146\n",
      "Iteration 391, loss = 5.56362903\n",
      "Iteration 392, loss = 5.54978826\n",
      "Iteration 393, loss = 5.56883204\n",
      "Iteration 394, loss = 5.55276419\n",
      "Iteration 395, loss = 5.56315875\n",
      "Iteration 396, loss = 5.56400119\n",
      "Iteration 397, loss = 5.55562676\n",
      "Iteration 398, loss = 5.55002361\n",
      "Iteration 399, loss = 5.56079402\n",
      "Iteration 400, loss = 5.56474481\n",
      "Iteration 401, loss = 5.56700367\n",
      "Iteration 402, loss = 5.55199688\n",
      "Iteration 403, loss = 5.56471043\n",
      "Iteration 404, loss = 5.55689934\n",
      "Iteration 405, loss = 5.56512172\n",
      "Iteration 406, loss = 5.56417033\n",
      "Iteration 407, loss = 5.54962494\n",
      "Iteration 408, loss = 5.55162710\n",
      "Iteration 409, loss = 5.56757929\n",
      "Iteration 410, loss = 5.56614186\n",
      "Iteration 411, loss = 5.56933931\n",
      "Iteration 412, loss = 5.56896154\n",
      "Iteration 413, loss = 5.54930981\n",
      "Iteration 414, loss = 5.58776989\n",
      "Iteration 415, loss = 5.53876270\n",
      "Iteration 416, loss = 5.55667406\n",
      "Iteration 417, loss = 5.55315054\n",
      "Iteration 418, loss = 5.56133173\n",
      "Iteration 419, loss = 5.56216072\n",
      "Iteration 420, loss = 5.57453960\n",
      "Iteration 421, loss = 5.57312815\n",
      "Iteration 422, loss = 5.55538998\n",
      "Iteration 423, loss = 5.54933094\n",
      "Iteration 424, loss = 5.56516016\n",
      "Iteration 425, loss = 5.56754007\n",
      "Iteration 426, loss = 5.56280327\n",
      "Iteration 427, loss = 5.55578299\n",
      "Iteration 428, loss = 5.57788504\n",
      "Iteration 429, loss = 5.54268341\n",
      "Iteration 430, loss = 5.57453099\n",
      "Iteration 431, loss = 5.54963087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, hidden_layer_sizes=1000,\n",
       "             learning_rate_init=0.01, max_iter=3000, n_iter_no_change=200,\n",
       "             random_state=42, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, hidden_layer_sizes=1000,\n",
       "             learning_rate_init=0.01, max_iter=3000, n_iter_no_change=200,\n",
       "             random_state=42, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(activation='logistic', hidden_layer_sizes=1000,\n",
       "             learning_rate_init=0.01, max_iter=3000, n_iter_no_change=200,\n",
       "             random_state=42, verbose=1)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(random_state=42,hidden_layer_sizes=(500),verbose=1, n_iter_no_change=200, max_iter=3000, learning_rate_init=0.01, activation=\"logistic\")\n",
    "mlp.fit(X_train_normalized, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "y_pred_eval = mlp.predict(eval_normalized)\n",
    "print(type(y_pred_eval))\n",
    "y_pred_rounded = round_to_nearest_5(y_pred_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_eval = pd.DataFrame(y_pred_eval, columns=['x', 'y'])\n",
    "y_pred_eval['Predicted'] = y_pred_eval['x'].astype(str) + \"|\" + y_pred_eval['y'].astype(str)\n",
    "y_pred_eval.drop(columns=['x','y'], inplace=True)\n",
    "y_pred_eval.reset_index(inplace=True)\n",
    "y_pred_eval.rename(columns={'index': 'Id'}, inplace=True)\n",
    "y_pred_eval = y_pred_eval[['Id', 'Predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>491.28375592999373|326.63206358589434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>538.4383134899424|571.1806939893592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>208.8739380058313|394.8297489450418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>582.7593958532052|511.1989939723756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>360.2919177475979|362.73124847724813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128495</th>\n",
       "      <td>128495</td>\n",
       "      <td>440.2393360830835|264.42211330992217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128496</th>\n",
       "      <td>128496</td>\n",
       "      <td>279.4624012799543|224.83241596357354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128497</th>\n",
       "      <td>128497</td>\n",
       "      <td>461.2549015060042|589.7945974992823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128498</th>\n",
       "      <td>128498</td>\n",
       "      <td>230.46705585153552|384.8152619869224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128499</th>\n",
       "      <td>128499</td>\n",
       "      <td>404.37266298135916|209.33994988691595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id                              Predicted\n",
       "0            0  491.28375592999373|326.63206358589434\n",
       "1            1    538.4383134899424|571.1806939893592\n",
       "2            2    208.8739380058313|394.8297489450418\n",
       "3            3    582.7593958532052|511.1989939723756\n",
       "4            4   360.2919177475979|362.73124847724813\n",
       "...        ...                                    ...\n",
       "128495  128495   440.2393360830835|264.42211330992217\n",
       "128496  128496   279.4624012799543|224.83241596357354\n",
       "128497  128497    461.2549015060042|589.7945974992823\n",
       "128498  128498   230.46705585153552|384.8152619869224\n",
       "128499  128499  404.37266298135916|209.33994988691595\n",
       "\n",
       "[128500 rows x 2 columns]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_eval.to_csv('output.csv', columns=[\"Id\",\"Predicted\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rounded = pd.DataFrame(y_pred_rounded)\n",
    "eval_rounded['Predicted'] = eval_rounded[0].astype(str) + '|' + eval_rounded[1].astype(str)\n",
    "eval_rounded.drop(columns=[0,1], inplace=True)\n",
    "eval_rounded.reset_index(inplace=True)\n",
    "eval_rounded.rename(columns={'index': 'Id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rounded.to_csv('output_rounded.csv', columns=[\"Id\",\"Predicted\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = RandomForestRegressor(n_estimators=300, n_jobs=-1, verbose=1)\n",
    "# reg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_pred = reg.predict(df_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_eval = pd.DataFrame(reg_pred, columns=['x', 'y'])\n",
    "# reg_eval['Predicted'] = reg_eval['x'].astype(str) + '|' + reg_eval['y'].astype(str)\n",
    "# reg_eval.drop(columns=['x', 'y'], inplace=True)\n",
    "# reg_eval.reset_index(inplace=True)\n",
    "# reg_eval.rename(columns={'index': 'Id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_eval.to_csv('reg_output.csv', columns=[\"Id\",\"Predicted\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
