{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import helpers.processing_helpers as ph\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(\"./dataset/development.csv\")\n",
    "df_ev = pd.read_csv(\"./dataset/evaluation.csv\")\n",
    "\n",
    "acc_idxs = [1,2,3,4,5,6,8,9,10,11,13,14]\n",
    "noise_indexes = [0,7,12,15,16,17]\n",
    "\n",
    "\n",
    "\n",
    "features = ['pmax', 'negpmax', 'area', 'tmax', 'rms']\n",
    "\n",
    "drop_features = ['tmax', 'rms', 'area']\n",
    "\n",
    "df_dev = df_dev.drop(columns=ph.get_column_names(features, noise_indexes)) \n",
    "df_dev = df_dev.drop(columns=ph.get_column_names(drop_features, acc_idxs))\n",
    "\n",
    "df_ev = df_ev.drop(columns=ph.get_column_names(features, noise_indexes))\n",
    "df_ev = df_ev.drop(columns=ph.get_column_names(drop_features, acc_idxs))\n",
    "df_ev = df_ev.drop(columns=\"Id\")\n",
    "\n",
    "X_train = df_dev.drop(columns=['x', 'y'])\n",
    "Y_train = df_dev[['x', 'y']]\n",
    "\n",
    "\n",
    "features = X_train.columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = pd.DataFrame(scaler.fit_transform(X_train), columns=features)\n",
    "eval_normalized = pd.DataFrame(scaler.transform(df_ev), columns=features)\n",
    "\n",
    "\n",
    "outlier_clr = LocalOutlierFactor(contamination=0.025)\n",
    "\n",
    "outlier_scores = outlier_clr.fit_predict(X_train_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmax[1]</th>\n",
       "      <th>negpmax[1]</th>\n",
       "      <th>pmax[2]</th>\n",
       "      <th>negpmax[2]</th>\n",
       "      <th>pmax[3]</th>\n",
       "      <th>negpmax[3]</th>\n",
       "      <th>pmax[4]</th>\n",
       "      <th>negpmax[4]</th>\n",
       "      <th>pmax[5]</th>\n",
       "      <th>negpmax[5]</th>\n",
       "      <th>...</th>\n",
       "      <th>negpmax[9]</th>\n",
       "      <th>pmax[10]</th>\n",
       "      <th>negpmax[10]</th>\n",
       "      <th>pmax[11]</th>\n",
       "      <th>negpmax[11]</th>\n",
       "      <th>pmax[13]</th>\n",
       "      <th>negpmax[13]</th>\n",
       "      <th>pmax[14]</th>\n",
       "      <th>negpmax[14]</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.711719</td>\n",
       "      <td>0.106909</td>\n",
       "      <td>-0.441961</td>\n",
       "      <td>0.009510</td>\n",
       "      <td>-0.848685</td>\n",
       "      <td>0.137472</td>\n",
       "      <td>-0.832614</td>\n",
       "      <td>0.031988</td>\n",
       "      <td>-1.141005</td>\n",
       "      <td>0.858277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.748229</td>\n",
       "      <td>0.083624</td>\n",
       "      <td>-0.019371</td>\n",
       "      <td>-0.595660</td>\n",
       "      <td>0.081664</td>\n",
       "      <td>-1.210932</td>\n",
       "      <td>0.464910</td>\n",
       "      <td>-0.602589</td>\n",
       "      <td>0.039315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.684792</td>\n",
       "      <td>0.165512</td>\n",
       "      <td>-0.600365</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>-0.892459</td>\n",
       "      <td>0.131305</td>\n",
       "      <td>-0.753976</td>\n",
       "      <td>0.038710</td>\n",
       "      <td>-1.036258</td>\n",
       "      <td>0.882947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.746272</td>\n",
       "      <td>-0.001023</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>-0.519378</td>\n",
       "      <td>0.068776</td>\n",
       "      <td>-1.171136</td>\n",
       "      <td>0.448051</td>\n",
       "      <td>-0.574763</td>\n",
       "      <td>0.028081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.790196</td>\n",
       "      <td>0.113647</td>\n",
       "      <td>-0.526214</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>-0.840038</td>\n",
       "      <td>0.139111</td>\n",
       "      <td>-0.890003</td>\n",
       "      <td>0.020064</td>\n",
       "      <td>-0.959713</td>\n",
       "      <td>0.902269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.828382</td>\n",
       "      <td>0.145767</td>\n",
       "      <td>0.004424</td>\n",
       "      <td>-0.377900</td>\n",
       "      <td>0.067464</td>\n",
       "      <td>-1.161445</td>\n",
       "      <td>0.502746</td>\n",
       "      <td>-0.684512</td>\n",
       "      <td>0.029485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.761922</td>\n",
       "      <td>0.193033</td>\n",
       "      <td>-0.605966</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>-0.817495</td>\n",
       "      <td>0.138789</td>\n",
       "      <td>-0.827618</td>\n",
       "      <td>0.022207</td>\n",
       "      <td>-1.088317</td>\n",
       "      <td>0.892385</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660310</td>\n",
       "      <td>0.180483</td>\n",
       "      <td>0.015728</td>\n",
       "      <td>-0.583469</td>\n",
       "      <td>0.062709</td>\n",
       "      <td>-1.200117</td>\n",
       "      <td>0.484278</td>\n",
       "      <td>-0.725118</td>\n",
       "      <td>0.027887</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.714769</td>\n",
       "      <td>0.152889</td>\n",
       "      <td>-0.434056</td>\n",
       "      <td>0.003651</td>\n",
       "      <td>-0.865775</td>\n",
       "      <td>0.140308</td>\n",
       "      <td>-0.800334</td>\n",
       "      <td>0.029229</td>\n",
       "      <td>-0.965757</td>\n",
       "      <td>0.845611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.777175</td>\n",
       "      <td>0.129421</td>\n",
       "      <td>-0.028425</td>\n",
       "      <td>-0.537785</td>\n",
       "      <td>0.072425</td>\n",
       "      <td>-1.193584</td>\n",
       "      <td>0.503548</td>\n",
       "      <td>-0.686299</td>\n",
       "      <td>0.031184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385494</th>\n",
       "      <td>2.320114</td>\n",
       "      <td>-0.724913</td>\n",
       "      <td>7.784918</td>\n",
       "      <td>-0.164526</td>\n",
       "      <td>1.729602</td>\n",
       "      <td>-0.363508</td>\n",
       "      <td>-0.188855</td>\n",
       "      <td>0.028398</td>\n",
       "      <td>-1.071899</td>\n",
       "      <td>0.876243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063022</td>\n",
       "      <td>-1.110332</td>\n",
       "      <td>0.126261</td>\n",
       "      <td>-0.665714</td>\n",
       "      <td>0.094045</td>\n",
       "      <td>-0.772206</td>\n",
       "      <td>0.462486</td>\n",
       "      <td>-0.352577</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385495</th>\n",
       "      <td>1.984914</td>\n",
       "      <td>-0.570141</td>\n",
       "      <td>6.092468</td>\n",
       "      <td>-0.140522</td>\n",
       "      <td>1.443518</td>\n",
       "      <td>-0.335376</td>\n",
       "      <td>-0.224333</td>\n",
       "      <td>0.045489</td>\n",
       "      <td>-0.948231</td>\n",
       "      <td>0.868763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043895</td>\n",
       "      <td>-1.093250</td>\n",
       "      <td>0.132743</td>\n",
       "      <td>-0.768189</td>\n",
       "      <td>0.055052</td>\n",
       "      <td>-0.778056</td>\n",
       "      <td>0.414618</td>\n",
       "      <td>-0.608070</td>\n",
       "      <td>0.037437</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385496</th>\n",
       "      <td>2.056252</td>\n",
       "      <td>-0.565863</td>\n",
       "      <td>6.408689</td>\n",
       "      <td>-0.167727</td>\n",
       "      <td>1.319551</td>\n",
       "      <td>-0.347221</td>\n",
       "      <td>-0.253281</td>\n",
       "      <td>0.028416</td>\n",
       "      <td>-1.061089</td>\n",
       "      <td>0.844477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054937</td>\n",
       "      <td>-0.857272</td>\n",
       "      <td>0.132563</td>\n",
       "      <td>-0.319939</td>\n",
       "      <td>0.073038</td>\n",
       "      <td>-0.844124</td>\n",
       "      <td>0.471563</td>\n",
       "      <td>-0.272023</td>\n",
       "      <td>0.031033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385497</th>\n",
       "      <td>1.796743</td>\n",
       "      <td>-0.726257</td>\n",
       "      <td>6.896955</td>\n",
       "      <td>-0.173545</td>\n",
       "      <td>1.866020</td>\n",
       "      <td>-0.356328</td>\n",
       "      <td>0.144617</td>\n",
       "      <td>0.028420</td>\n",
       "      <td>-0.940133</td>\n",
       "      <td>0.899318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041449</td>\n",
       "      <td>-1.040976</td>\n",
       "      <td>0.140699</td>\n",
       "      <td>-0.776684</td>\n",
       "      <td>0.079367</td>\n",
       "      <td>-0.730563</td>\n",
       "      <td>0.448717</td>\n",
       "      <td>-0.367520</td>\n",
       "      <td>0.041807</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385499</th>\n",
       "      <td>2.233336</td>\n",
       "      <td>-0.581834</td>\n",
       "      <td>6.628403</td>\n",
       "      <td>-0.151738</td>\n",
       "      <td>1.492015</td>\n",
       "      <td>-0.343241</td>\n",
       "      <td>-0.031217</td>\n",
       "      <td>-0.080242</td>\n",
       "      <td>-0.918205</td>\n",
       "      <td>0.076523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167940</td>\n",
       "      <td>-1.095215</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.703022</td>\n",
       "      <td>-0.025023</td>\n",
       "      <td>-0.713666</td>\n",
       "      <td>0.132727</td>\n",
       "      <td>-0.548842</td>\n",
       "      <td>-0.066677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375862 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pmax[1]  negpmax[1]   pmax[2]  negpmax[2]   pmax[3]  negpmax[3]  \\\n",
       "2      -0.711719    0.106909 -0.441961    0.009510 -0.848685    0.137472   \n",
       "3      -0.684792    0.165512 -0.600365    0.000120 -0.892459    0.131305   \n",
       "4      -0.790196    0.113647 -0.526214    0.002648 -0.840038    0.139111   \n",
       "5      -0.761922    0.193033 -0.605966    0.002262 -0.817495    0.138789   \n",
       "6      -0.714769    0.152889 -0.434056    0.003651 -0.865775    0.140308   \n",
       "...          ...         ...       ...         ...       ...         ...   \n",
       "385494  2.320114   -0.724913  7.784918   -0.164526  1.729602   -0.363508   \n",
       "385495  1.984914   -0.570141  6.092468   -0.140522  1.443518   -0.335376   \n",
       "385496  2.056252   -0.565863  6.408689   -0.167727  1.319551   -0.347221   \n",
       "385497  1.796743   -0.726257  6.896955   -0.173545  1.866020   -0.356328   \n",
       "385499  2.233336   -0.581834  6.628403   -0.151738  1.492015   -0.343241   \n",
       "\n",
       "         pmax[4]  negpmax[4]   pmax[5]  negpmax[5]  ...  negpmax[9]  pmax[10]  \\\n",
       "2      -0.832614    0.031988 -1.141005    0.858277  ...   -0.748229  0.083624   \n",
       "3      -0.753976    0.038710 -1.036258    0.882947  ...   -0.746272 -0.001023   \n",
       "4      -0.890003    0.020064 -0.959713    0.902269  ...   -0.828382  0.145767   \n",
       "5      -0.827618    0.022207 -1.088317    0.892385  ...   -0.660310  0.180483   \n",
       "6      -0.800334    0.029229 -0.965757    0.845611  ...   -0.777175  0.129421   \n",
       "...          ...         ...       ...         ...  ...         ...       ...   \n",
       "385494 -0.188855    0.028398 -1.071899    0.876243  ...    0.063022 -1.110332   \n",
       "385495 -0.224333    0.045489 -0.948231    0.868763  ...    0.043895 -1.093250   \n",
       "385496 -0.253281    0.028416 -1.061089    0.844477  ...    0.054937 -0.857272   \n",
       "385497  0.144617    0.028420 -0.940133    0.899318  ...    0.041449 -1.040976   \n",
       "385499 -0.031217   -0.080242 -0.918205    0.076523  ...   -0.167940 -1.095215   \n",
       "\n",
       "        negpmax[10]  pmax[11]  negpmax[11]  pmax[13]  negpmax[13]  pmax[14]  \\\n",
       "2         -0.019371 -0.595660     0.081664 -1.210932     0.464910 -0.602589   \n",
       "3          0.000834 -0.519378     0.068776 -1.171136     0.448051 -0.574763   \n",
       "4          0.004424 -0.377900     0.067464 -1.161445     0.502746 -0.684512   \n",
       "5          0.015728 -0.583469     0.062709 -1.200117     0.484278 -0.725118   \n",
       "6         -0.028425 -0.537785     0.072425 -1.193584     0.503548 -0.686299   \n",
       "...             ...       ...          ...       ...          ...       ...   \n",
       "385494     0.126261 -0.665714     0.094045 -0.772206     0.462486 -0.352577   \n",
       "385495     0.132743 -0.768189     0.055052 -0.778056     0.414618 -0.608070   \n",
       "385496     0.132563 -0.319939     0.073038 -0.844124     0.471563 -0.272023   \n",
       "385497     0.140699 -0.776684     0.079367 -0.730563     0.448717 -0.367520   \n",
       "385499    -0.000219 -0.703022    -0.025023 -0.713666     0.132727 -0.548842   \n",
       "\n",
       "        negpmax[14]  score  \n",
       "2          0.039315      1  \n",
       "3          0.028081      1  \n",
       "4          0.029485      1  \n",
       "5          0.027887      1  \n",
       "6          0.031184      1  \n",
       "...             ...    ...  \n",
       "385494     0.034247      1  \n",
       "385495     0.037437      1  \n",
       "385496     0.031033      1  \n",
       "385497     0.041807      1  \n",
       "385499    -0.066677      1  \n",
       "\n",
       "[375862 rows x 25 columns]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outlier_scores = pd.DataFrame(outlier_scores, columns=['score'])\n",
    "X_train_normalized = pd.concat([X_train_normalized, df_outlier_scores], axis=1)\n",
    "score_mask = X_train_normalized['score'] >= 0\n",
    "X_train_normalized = X_train_normalized[score_mask]\n",
    "X_train_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.concat([Y_train, df_outlier_scores], axis=1)\n",
    "Y_train = Y_train[score_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized.drop(columns=['score'], inplace=True)\n",
    "Y_train.drop(columns=['score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2735.67655480\n",
      "Iteration 2, loss = 19.94618917\n",
      "Iteration 3, loss = 12.82177277\n",
      "Iteration 4, loss = 10.22372587\n",
      "Iteration 5, loss = 8.85961349\n",
      "Iteration 6, loss = 8.10760726\n",
      "Iteration 7, loss = 7.65889021\n",
      "Iteration 8, loss = 7.38769725\n",
      "Iteration 9, loss = 7.14485781\n",
      "Iteration 10, loss = 6.99602409\n",
      "Iteration 11, loss = 6.86520675\n",
      "Iteration 12, loss = 6.77984790\n",
      "Iteration 13, loss = 6.66800194\n",
      "Iteration 14, loss = 6.60699318\n",
      "Iteration 15, loss = 6.54218998\n",
      "Iteration 16, loss = 6.48961924\n",
      "Iteration 17, loss = 6.45062009\n",
      "Iteration 18, loss = 6.42232811\n",
      "Iteration 19, loss = 6.37072551\n",
      "Iteration 20, loss = 6.32525930\n",
      "Iteration 21, loss = 6.31652125\n",
      "Iteration 22, loss = 6.29179722\n",
      "Iteration 23, loss = 6.26817226\n",
      "Iteration 24, loss = 6.25211587\n",
      "Iteration 25, loss = 6.21585744\n",
      "Iteration 26, loss = 6.19423263\n",
      "Iteration 27, loss = 6.18594447\n",
      "Iteration 28, loss = 6.16900677\n",
      "Iteration 29, loss = 6.14359470\n",
      "Iteration 30, loss = 6.13028422\n",
      "Iteration 31, loss = 6.12543620\n",
      "Iteration 32, loss = 6.10283660\n",
      "Iteration 33, loss = 6.08083428\n",
      "Iteration 34, loss = 6.07506600\n",
      "Iteration 35, loss = 6.04932320\n",
      "Iteration 36, loss = 6.05265512\n",
      "Iteration 37, loss = 6.04762801\n",
      "Iteration 38, loss = 6.04049105\n",
      "Iteration 39, loss = 6.02134395\n",
      "Iteration 40, loss = 6.01633481\n",
      "Iteration 41, loss = 6.00754854\n",
      "Iteration 42, loss = 5.99460778\n",
      "Iteration 43, loss = 6.00499213\n",
      "Iteration 44, loss = 5.97912413\n",
      "Iteration 45, loss = 5.99074445\n",
      "Iteration 46, loss = 5.97574785\n",
      "Iteration 47, loss = 5.95592558\n",
      "Iteration 48, loss = 5.96115404\n",
      "Iteration 49, loss = 5.95981595\n",
      "Iteration 50, loss = 5.94017066\n",
      "Iteration 51, loss = 5.94845567\n",
      "Iteration 52, loss = 5.93119391\n",
      "Iteration 53, loss = 5.91818191\n",
      "Iteration 54, loss = 5.91156714\n",
      "Iteration 55, loss = 5.92269489\n",
      "Iteration 56, loss = 5.92070275\n",
      "Iteration 57, loss = 5.90030634\n",
      "Iteration 58, loss = 5.89816903\n",
      "Iteration 59, loss = 5.89083471\n",
      "Iteration 60, loss = 5.89387888\n",
      "Iteration 61, loss = 5.88984056\n",
      "Iteration 62, loss = 5.87831436\n",
      "Iteration 63, loss = 5.88076242\n",
      "Iteration 64, loss = 5.88290098\n",
      "Iteration 65, loss = 5.87063080\n",
      "Iteration 66, loss = 5.87390850\n",
      "Iteration 67, loss = 5.86285525\n",
      "Iteration 68, loss = 5.85837895\n",
      "Iteration 69, loss = 5.85544829\n",
      "Iteration 70, loss = 5.86537335\n",
      "Iteration 71, loss = 5.84731964\n",
      "Iteration 72, loss = 5.84050497\n",
      "Iteration 73, loss = 5.84896637\n",
      "Iteration 74, loss = 5.84944746\n",
      "Iteration 75, loss = 5.83938679\n",
      "Iteration 76, loss = 5.83582373\n",
      "Iteration 77, loss = 5.84186499\n",
      "Iteration 78, loss = 5.82977540\n",
      "Iteration 79, loss = 5.82615626\n",
      "Iteration 80, loss = 5.81991536\n",
      "Iteration 81, loss = 5.82419184\n",
      "Iteration 82, loss = 5.81604422\n",
      "Iteration 83, loss = 5.81802934\n",
      "Iteration 84, loss = 5.81655069\n",
      "Iteration 85, loss = 5.82537092\n",
      "Iteration 86, loss = 5.80367847\n",
      "Iteration 87, loss = 5.81482494\n",
      "Iteration 88, loss = 5.80779269\n",
      "Iteration 89, loss = 5.79641913\n",
      "Iteration 90, loss = 5.80662146\n",
      "Iteration 91, loss = 5.80835124\n",
      "Iteration 92, loss = 5.79615334\n",
      "Iteration 93, loss = 5.78932751\n",
      "Iteration 94, loss = 5.78784162\n",
      "Iteration 95, loss = 5.79980283\n",
      "Iteration 96, loss = 5.79252190\n",
      "Iteration 97, loss = 5.78528566\n",
      "Iteration 98, loss = 5.78725426\n",
      "Iteration 99, loss = 5.77895972\n",
      "Iteration 100, loss = 5.79155756\n",
      "Iteration 101, loss = 5.77640420\n",
      "Iteration 102, loss = 5.78827965\n",
      "Iteration 103, loss = 5.77821452\n",
      "Iteration 104, loss = 5.79008264\n",
      "Iteration 105, loss = 5.78305532\n",
      "Iteration 106, loss = 5.76050790\n",
      "Iteration 107, loss = 5.77872068\n",
      "Iteration 108, loss = 5.77162587\n",
      "Iteration 109, loss = 5.78167795\n",
      "Iteration 110, loss = 5.76710233\n",
      "Iteration 111, loss = 5.77208233\n",
      "Iteration 112, loss = 5.76027875\n",
      "Iteration 113, loss = 5.77226694\n",
      "Iteration 114, loss = 5.75593278\n",
      "Iteration 115, loss = 5.77705243\n",
      "Iteration 116, loss = 5.76322821\n",
      "Iteration 117, loss = 5.76715771\n",
      "Iteration 118, loss = 5.76436492\n",
      "Iteration 119, loss = 5.75309225\n",
      "Iteration 120, loss = 5.77382514\n",
      "Iteration 121, loss = 5.76158290\n",
      "Iteration 122, loss = 5.75041644\n",
      "Iteration 123, loss = 5.75149993\n",
      "Iteration 124, loss = 5.75390168\n",
      "Iteration 125, loss = 5.75241637\n",
      "Iteration 126, loss = 5.74844878\n",
      "Iteration 127, loss = 5.75081054\n",
      "Iteration 128, loss = 5.75045737\n",
      "Iteration 129, loss = 5.74454935\n",
      "Iteration 130, loss = 5.74995582\n",
      "Iteration 131, loss = 5.73627877\n",
      "Iteration 132, loss = 5.75389857\n",
      "Iteration 133, loss = 5.74915867\n",
      "Iteration 134, loss = 5.73783458\n",
      "Iteration 135, loss = 5.74738483\n",
      "Iteration 136, loss = 5.75466125\n",
      "Iteration 137, loss = 5.75089955\n",
      "Iteration 138, loss = 5.73097129\n",
      "Iteration 139, loss = 5.73525758\n",
      "Iteration 140, loss = 5.74087643\n",
      "Iteration 141, loss = 5.73506344\n",
      "Iteration 142, loss = 5.74331045\n",
      "Iteration 143, loss = 5.73901831\n",
      "Iteration 144, loss = 5.74730991\n",
      "Iteration 145, loss = 5.73284883\n",
      "Iteration 146, loss = 5.72894388\n",
      "Iteration 147, loss = 5.73492789\n",
      "Iteration 148, loss = 5.73531992\n",
      "Iteration 149, loss = 5.72353318\n",
      "Iteration 150, loss = 5.74284847\n",
      "Iteration 151, loss = 5.74290119\n",
      "Iteration 152, loss = 5.73006497\n",
      "Iteration 153, loss = 5.73001537\n",
      "Iteration 154, loss = 5.72991669\n",
      "Iteration 155, loss = 5.71635486\n",
      "Iteration 156, loss = 5.72524789\n",
      "Iteration 157, loss = 5.72753938\n",
      "Iteration 158, loss = 5.73564145\n",
      "Iteration 159, loss = 5.72664326\n",
      "Iteration 160, loss = 5.72556849\n",
      "Iteration 161, loss = 5.73312992\n",
      "Iteration 162, loss = 5.72999431\n",
      "Iteration 163, loss = 5.72754382\n",
      "Iteration 164, loss = 5.72744326\n",
      "Iteration 165, loss = 5.72291941\n",
      "Iteration 166, loss = 5.72114608\n",
      "Iteration 167, loss = 5.72452510\n",
      "Iteration 168, loss = 5.71911619\n",
      "Iteration 169, loss = 5.72877165\n",
      "Iteration 170, loss = 5.72473816\n",
      "Iteration 171, loss = 5.72129590\n",
      "Iteration 172, loss = 5.72312730\n",
      "Iteration 173, loss = 5.72609432\n",
      "Iteration 174, loss = 5.71202676\n",
      "Iteration 175, loss = 5.72331031\n",
      "Iteration 176, loss = 5.71638682\n",
      "Iteration 177, loss = 5.71829436\n",
      "Iteration 178, loss = 5.72336640\n",
      "Iteration 179, loss = 5.72029857\n",
      "Iteration 180, loss = 5.71949225\n",
      "Iteration 181, loss = 5.71521222\n",
      "Iteration 182, loss = 5.71785290\n",
      "Iteration 183, loss = 5.72317026\n",
      "Iteration 184, loss = 5.72215989\n",
      "Iteration 185, loss = 5.71980218\n",
      "Iteration 186, loss = 5.72305215\n",
      "Iteration 187, loss = 5.72326528\n",
      "Iteration 188, loss = 5.70876015\n",
      "Iteration 189, loss = 5.71335496\n",
      "Iteration 190, loss = 5.71671057\n",
      "Iteration 191, loss = 5.70372925\n",
      "Iteration 192, loss = 5.71595867\n",
      "Iteration 193, loss = 5.70421767\n",
      "Iteration 194, loss = 5.71836296\n",
      "Iteration 195, loss = 5.71328708\n",
      "Iteration 196, loss = 5.70805632\n",
      "Iteration 197, loss = 5.70734933\n",
      "Iteration 198, loss = 5.70769529\n",
      "Iteration 199, loss = 5.71578082\n",
      "Iteration 200, loss = 5.71178823\n",
      "Iteration 201, loss = 5.69977267\n",
      "Iteration 202, loss = 5.69867867\n",
      "Iteration 203, loss = 5.70642935\n",
      "Iteration 204, loss = 5.70376008\n",
      "Iteration 205, loss = 5.71698433\n",
      "Iteration 206, loss = 5.70944229\n",
      "Iteration 207, loss = 5.71390218\n",
      "Iteration 208, loss = 5.70727234\n",
      "Iteration 209, loss = 5.70771514\n",
      "Iteration 210, loss = 5.71442424\n",
      "Iteration 211, loss = 5.72876672\n",
      "Iteration 212, loss = 5.70717841\n",
      "Iteration 213, loss = 5.71126291\n",
      "Iteration 214, loss = 5.69620175\n",
      "Iteration 215, loss = 5.70921273\n",
      "Iteration 216, loss = 5.70948407\n",
      "Iteration 217, loss = 5.70806741\n",
      "Iteration 218, loss = 5.70638259\n",
      "Iteration 219, loss = 5.69839271\n",
      "Iteration 220, loss = 5.70330815\n",
      "Iteration 221, loss = 5.70546155\n",
      "Iteration 222, loss = 5.69672252\n",
      "Iteration 223, loss = 5.69990282\n",
      "Iteration 224, loss = 5.71414919\n",
      "Iteration 225, loss = 5.69748376\n",
      "Iteration 226, loss = 5.70828249\n",
      "Iteration 227, loss = 5.70112826\n",
      "Iteration 228, loss = 5.69354083\n",
      "Iteration 229, loss = 5.69625853\n",
      "Iteration 230, loss = 5.70782288\n",
      "Iteration 231, loss = 5.69380658\n",
      "Iteration 232, loss = 5.69008573\n",
      "Iteration 233, loss = 5.70302209\n",
      "Iteration 234, loss = 5.70729342\n",
      "Iteration 235, loss = 5.70253965\n",
      "Iteration 236, loss = 5.70662081\n",
      "Iteration 237, loss = 5.70394268\n",
      "Iteration 238, loss = 5.69775419\n",
      "Iteration 239, loss = 5.70044815\n",
      "Iteration 240, loss = 5.70609202\n",
      "Iteration 241, loss = 5.69377308\n",
      "Iteration 242, loss = 5.69385289\n",
      "Iteration 243, loss = 5.69819450\n",
      "Iteration 244, loss = 5.70018090\n",
      "Iteration 245, loss = 5.70856819\n",
      "Iteration 246, loss = 5.69622296\n",
      "Iteration 247, loss = 5.70111656\n",
      "Iteration 248, loss = 5.71107076\n",
      "Iteration 249, loss = 5.69569934\n",
      "Iteration 250, loss = 5.68537004\n",
      "Iteration 251, loss = 5.69764035\n",
      "Iteration 252, loss = 5.69699480\n",
      "Iteration 253, loss = 5.70128110\n",
      "Iteration 254, loss = 5.70422599\n",
      "Iteration 255, loss = 5.69160961\n",
      "Iteration 256, loss = 5.70847123\n",
      "Iteration 257, loss = 5.69407678\n",
      "Iteration 258, loss = 5.69984900\n",
      "Iteration 259, loss = 5.68533910\n",
      "Iteration 260, loss = 5.70387820\n",
      "Iteration 261, loss = 5.69378867\n",
      "Iteration 262, loss = 5.70030762\n",
      "Iteration 263, loss = 5.70126438\n",
      "Iteration 264, loss = 5.69890103\n",
      "Iteration 265, loss = 5.69110850\n",
      "Iteration 266, loss = 5.70185738\n",
      "Iteration 267, loss = 5.68553785\n",
      "Iteration 268, loss = 5.69347109\n",
      "Iteration 269, loss = 5.69821431\n",
      "Iteration 270, loss = 5.69541213\n",
      "Iteration 271, loss = 5.69444171\n",
      "Iteration 272, loss = 5.69573735\n",
      "Iteration 273, loss = 5.69073654\n",
      "Iteration 274, loss = 5.69912638\n",
      "Iteration 275, loss = 5.68451404\n",
      "Iteration 276, loss = 5.68771568\n",
      "Iteration 277, loss = 5.69462076\n",
      "Iteration 278, loss = 5.69465986\n",
      "Iteration 279, loss = 5.69356594\n",
      "Iteration 280, loss = 5.69333837\n",
      "Iteration 281, loss = 5.69090329\n",
      "Iteration 282, loss = 5.68758628\n",
      "Iteration 283, loss = 5.68760440\n",
      "Iteration 284, loss = 5.69198778\n",
      "Iteration 285, loss = 5.69248020\n",
      "Iteration 286, loss = 5.70076792\n",
      "Iteration 287, loss = 5.69547521\n",
      "Iteration 288, loss = 5.68017185\n",
      "Iteration 289, loss = 5.69036155\n",
      "Iteration 290, loss = 5.70513519\n",
      "Iteration 291, loss = 5.69622023\n",
      "Iteration 292, loss = 5.69006900\n",
      "Iteration 293, loss = 5.69749392\n",
      "Iteration 294, loss = 5.69072383\n",
      "Iteration 295, loss = 5.69824669\n",
      "Iteration 296, loss = 5.69957826\n",
      "Iteration 297, loss = 5.70004438\n",
      "Iteration 298, loss = 5.68581300\n",
      "Iteration 299, loss = 5.68677100\n",
      "Iteration 300, loss = 5.68597667\n",
      "Iteration 301, loss = 5.70096707\n",
      "Iteration 302, loss = 5.69652530\n",
      "Iteration 303, loss = 5.68855707\n",
      "Iteration 304, loss = 5.69388853\n",
      "Iteration 305, loss = 5.69112028\n",
      "Iteration 306, loss = 5.69229452\n",
      "Iteration 307, loss = 5.69247719\n",
      "Iteration 308, loss = 5.69558177\n",
      "Iteration 309, loss = 5.68914263\n",
      "Iteration 310, loss = 5.68990660\n",
      "Iteration 311, loss = 5.69678295\n",
      "Iteration 312, loss = 5.69326220\n",
      "Iteration 313, loss = 5.68811025\n",
      "Iteration 314, loss = 5.69044370\n",
      "Iteration 315, loss = 5.68636050\n",
      "Iteration 316, loss = 5.68788293\n",
      "Iteration 317, loss = 5.68958398\n",
      "Iteration 318, loss = 5.69002266\n",
      "Iteration 319, loss = 5.69298400\n",
      "Iteration 320, loss = 5.68358408\n",
      "Iteration 321, loss = 5.68613895\n",
      "Iteration 322, loss = 5.68359836\n",
      "Iteration 323, loss = 5.69293460\n",
      "Iteration 324, loss = 5.69366657\n",
      "Iteration 325, loss = 5.67940006\n",
      "Iteration 326, loss = 5.70907320\n",
      "Iteration 327, loss = 5.68387231\n",
      "Iteration 328, loss = 5.69006064\n",
      "Iteration 329, loss = 5.68739734\n",
      "Iteration 330, loss = 5.68852722\n",
      "Iteration 331, loss = 5.68899725\n",
      "Iteration 332, loss = 5.68622807\n",
      "Iteration 333, loss = 5.68829859\n",
      "Iteration 334, loss = 5.68225380\n",
      "Iteration 335, loss = 5.68003342\n",
      "Iteration 336, loss = 5.69171641\n",
      "Iteration 337, loss = 5.68334299\n",
      "Iteration 338, loss = 5.68108287\n",
      "Iteration 339, loss = 5.68471580\n",
      "Iteration 340, loss = 5.68707520\n",
      "Iteration 341, loss = 5.68772959\n",
      "Iteration 342, loss = 5.68365388\n",
      "Iteration 343, loss = 5.69175769\n",
      "Iteration 344, loss = 5.67846791\n",
      "Iteration 345, loss = 5.70437037\n",
      "Iteration 346, loss = 5.67874545\n",
      "Iteration 347, loss = 5.69260654\n",
      "Iteration 348, loss = 5.69101374\n",
      "Iteration 349, loss = 5.68933415\n",
      "Iteration 350, loss = 5.68670262\n",
      "Iteration 351, loss = 5.68413643\n",
      "Iteration 352, loss = 5.67506215\n",
      "Iteration 353, loss = 5.68787187\n",
      "Iteration 354, loss = 5.68558341\n",
      "Iteration 355, loss = 5.68340068\n",
      "Iteration 356, loss = 5.68922223\n",
      "Iteration 357, loss = 5.68676052\n",
      "Iteration 358, loss = 5.69707262\n",
      "Iteration 359, loss = 5.67953897\n",
      "Iteration 360, loss = 5.69240979\n",
      "Iteration 361, loss = 5.67976822\n",
      "Iteration 362, loss = 5.68197765\n",
      "Iteration 363, loss = 5.69610640\n",
      "Iteration 364, loss = 5.69727370\n",
      "Iteration 365, loss = 5.67295230\n",
      "Iteration 366, loss = 5.68194343\n",
      "Iteration 367, loss = 5.68683708\n",
      "Iteration 368, loss = 5.69980119\n",
      "Iteration 369, loss = 5.68554468\n",
      "Iteration 370, loss = 5.67555650\n",
      "Iteration 371, loss = 5.68887751\n",
      "Iteration 372, loss = 5.69370539\n",
      "Iteration 373, loss = 5.68960530\n",
      "Iteration 374, loss = 5.69034081\n",
      "Iteration 375, loss = 5.68498290\n",
      "Iteration 376, loss = 5.68187796\n",
      "Iteration 377, loss = 5.69224210\n",
      "Iteration 378, loss = 5.68979885\n",
      "Iteration 379, loss = 5.67361333\n",
      "Iteration 380, loss = 5.68541751\n",
      "Iteration 381, loss = 5.68953566\n",
      "Iteration 382, loss = 5.67400171\n",
      "Iteration 383, loss = 5.67373310\n",
      "Iteration 384, loss = 5.68806061\n",
      "Iteration 385, loss = 5.68553924\n",
      "Iteration 386, loss = 5.68109789\n",
      "Iteration 387, loss = 5.68483362\n",
      "Iteration 388, loss = 5.68632207\n",
      "Iteration 389, loss = 5.68168542\n",
      "Iteration 390, loss = 5.68776152\n",
      "Iteration 391, loss = 5.68925382\n",
      "Iteration 392, loss = 5.68022019\n",
      "Iteration 393, loss = 5.68126961\n",
      "Iteration 394, loss = 5.69384526\n",
      "Iteration 395, loss = 5.68714676\n",
      "Iteration 396, loss = 5.69138723\n",
      "Iteration 397, loss = 5.68317400\n",
      "Iteration 398, loss = 5.67824835\n",
      "Iteration 399, loss = 5.69802445\n",
      "Iteration 400, loss = 5.67957801\n",
      "Iteration 401, loss = 5.68413012\n",
      "Iteration 402, loss = 5.69413229\n",
      "Iteration 403, loss = 5.67865570\n",
      "Iteration 404, loss = 5.69206567\n",
      "Iteration 405, loss = 5.68648356\n",
      "Iteration 406, loss = 5.68018018\n",
      "Iteration 407, loss = 5.69482894\n",
      "Iteration 408, loss = 5.68408747\n",
      "Iteration 409, loss = 5.68069594\n",
      "Iteration 410, loss = 5.68051459\n",
      "Iteration 411, loss = 5.68912443\n",
      "Iteration 412, loss = 5.68217211\n",
      "Iteration 413, loss = 5.67681122\n",
      "Iteration 414, loss = 5.69307960\n",
      "Iteration 415, loss = 5.68530523\n",
      "Iteration 416, loss = 5.68512226\n",
      "Iteration 417, loss = 5.67699299\n",
      "Iteration 418, loss = 5.68406296\n",
      "Iteration 419, loss = 5.68168535\n",
      "Iteration 420, loss = 5.68466563\n",
      "Iteration 421, loss = 5.68721281\n",
      "Iteration 422, loss = 5.68820945\n",
      "Iteration 423, loss = 5.68913111\n",
      "Iteration 424, loss = 5.68524396\n",
      "Iteration 425, loss = 5.67925234\n",
      "Iteration 426, loss = 5.68727959\n",
      "Iteration 427, loss = 5.68822725\n",
      "Iteration 428, loss = 5.68151516\n",
      "Iteration 429, loss = 5.68720372\n",
      "Iteration 430, loss = 5.67432017\n",
      "Iteration 431, loss = 5.67851521\n",
      "Iteration 432, loss = 5.68334878\n",
      "Iteration 433, loss = 5.69241486\n",
      "Iteration 434, loss = 5.68241756\n",
      "Iteration 435, loss = 5.67832927\n",
      "Iteration 436, loss = 5.68359396\n",
      "Iteration 437, loss = 5.68182993\n",
      "Iteration 438, loss = 5.67644818\n",
      "Iteration 439, loss = 5.69357498\n",
      "Iteration 440, loss = 5.68652034\n",
      "Iteration 441, loss = 5.68621525\n",
      "Iteration 442, loss = 5.69222431\n",
      "Iteration 443, loss = 5.67242518\n",
      "Iteration 444, loss = 5.68972788\n",
      "Iteration 445, loss = 5.68365394\n",
      "Iteration 446, loss = 5.68637948\n",
      "Iteration 447, loss = 5.67903409\n",
      "Iteration 448, loss = 5.68629019\n",
      "Iteration 449, loss = 5.68134405\n",
      "Iteration 450, loss = 5.68740903\n",
      "Iteration 451, loss = 5.68559031\n",
      "Iteration 452, loss = 5.67060384\n",
      "Iteration 453, loss = 5.68677397\n",
      "Iteration 454, loss = 5.68353348\n",
      "Iteration 455, loss = 5.67196255\n",
      "Iteration 456, loss = 5.68238210\n",
      "Iteration 457, loss = 5.69616322\n",
      "Iteration 458, loss = 5.68879529\n",
      "Iteration 459, loss = 5.68742991\n",
      "Iteration 460, loss = 5.68299439\n",
      "Iteration 461, loss = 5.66883783\n",
      "Iteration 462, loss = 5.68064788\n",
      "Iteration 463, loss = 5.68013626\n",
      "Iteration 464, loss = 5.68798080\n",
      "Iteration 465, loss = 5.68557761\n",
      "Iteration 466, loss = 5.69184535\n",
      "Iteration 467, loss = 5.67445795\n",
      "Iteration 468, loss = 5.68652947\n",
      "Iteration 469, loss = 5.67726747\n",
      "Iteration 470, loss = 5.67899642\n",
      "Iteration 471, loss = 5.68749993\n",
      "Iteration 472, loss = 5.67793582\n",
      "Iteration 473, loss = 5.68360767\n",
      "Iteration 474, loss = 5.68710317\n",
      "Iteration 475, loss = 5.68245893\n",
      "Iteration 476, loss = 5.68490192\n",
      "Iteration 477, loss = 5.67790256\n",
      "Iteration 478, loss = 5.68438870\n",
      "Iteration 479, loss = 5.67134642\n",
      "Iteration 480, loss = 5.68549956\n",
      "Iteration 481, loss = 5.67457097\n",
      "Iteration 482, loss = 5.68523115\n",
      "Iteration 483, loss = 5.67864415\n",
      "Iteration 484, loss = 5.67634582\n",
      "Iteration 485, loss = 5.68224298\n",
      "Iteration 486, loss = 5.68215046\n",
      "Iteration 487, loss = 5.68683746\n",
      "Iteration 488, loss = 5.66504174\n",
      "Iteration 489, loss = 5.68771591\n",
      "Iteration 490, loss = 5.67390759\n",
      "Iteration 491, loss = 5.68609050\n",
      "Iteration 492, loss = 5.67963441\n",
      "Iteration 493, loss = 5.67578346\n",
      "Iteration 494, loss = 5.68347617\n",
      "Iteration 495, loss = 5.67485801\n",
      "Iteration 496, loss = 5.67879268\n",
      "Iteration 497, loss = 5.69038421\n",
      "Iteration 498, loss = 5.68801882\n",
      "Iteration 499, loss = 5.68399514\n",
      "Iteration 500, loss = 5.67537668\n",
      "Iteration 501, loss = 5.68792308\n",
      "Iteration 502, loss = 5.67266196\n",
      "Iteration 503, loss = 5.68129590\n",
      "Iteration 504, loss = 5.67791734\n",
      "Iteration 505, loss = 5.68030511\n",
      "Iteration 506, loss = 5.68025708\n",
      "Iteration 507, loss = 5.67883459\n",
      "Iteration 508, loss = 5.67665240\n",
      "Iteration 509, loss = 5.67828503\n",
      "Iteration 510, loss = 5.67969919\n",
      "Iteration 511, loss = 5.67633107\n",
      "Iteration 512, loss = 5.68602927\n",
      "Iteration 513, loss = 5.67278842\n",
      "Iteration 514, loss = 5.67520820\n",
      "Iteration 515, loss = 5.68958776\n",
      "Iteration 516, loss = 5.69538357\n",
      "Iteration 517, loss = 5.68072115\n",
      "Iteration 518, loss = 5.67314931\n",
      "Iteration 519, loss = 5.66885190\n",
      "Iteration 520, loss = 5.68571254\n",
      "Iteration 521, loss = 5.67264592\n",
      "Iteration 522, loss = 5.68142908\n",
      "Iteration 523, loss = 5.67760290\n",
      "Iteration 524, loss = 5.68030178\n",
      "Iteration 525, loss = 5.66700020\n",
      "Iteration 526, loss = 5.68713933\n",
      "Iteration 527, loss = 5.68535000\n",
      "Iteration 528, loss = 5.66814959\n",
      "Iteration 529, loss = 5.68681847\n",
      "Iteration 530, loss = 5.67859396\n",
      "Iteration 531, loss = 5.67270274\n",
      "Iteration 532, loss = 5.67694548\n",
      "Iteration 533, loss = 5.67163945\n",
      "Iteration 534, loss = 5.68672982\n",
      "Iteration 535, loss = 5.67888325\n",
      "Iteration 536, loss = 5.67800284\n",
      "Iteration 537, loss = 5.68013891\n",
      "Iteration 538, loss = 5.67426598\n",
      "Iteration 539, loss = 5.67872110\n",
      "Iteration 540, loss = 5.68272768\n",
      "Iteration 541, loss = 5.67559967\n",
      "Iteration 542, loss = 5.68638123\n",
      "Iteration 543, loss = 5.68087740\n",
      "Iteration 544, loss = 5.67654531\n",
      "Iteration 545, loss = 5.68154174\n",
      "Iteration 546, loss = 5.68062572\n",
      "Iteration 547, loss = 5.67358720\n",
      "Iteration 548, loss = 5.67655712\n",
      "Iteration 549, loss = 5.67164882\n",
      "Iteration 550, loss = 5.68434519\n",
      "Iteration 551, loss = 5.68248233\n",
      "Iteration 552, loss = 5.68818705\n",
      "Iteration 553, loss = 5.66963974\n",
      "Iteration 554, loss = 5.68132525\n",
      "Iteration 555, loss = 5.69506913\n",
      "Iteration 556, loss = 5.66280366\n",
      "Iteration 557, loss = 5.67638922\n",
      "Iteration 558, loss = 5.67467768\n",
      "Iteration 559, loss = 5.67833064\n",
      "Iteration 560, loss = 5.67934453\n",
      "Iteration 561, loss = 5.67455899\n",
      "Iteration 562, loss = 5.68006743\n",
      "Iteration 563, loss = 5.67912634\n",
      "Iteration 564, loss = 5.67417932\n",
      "Iteration 565, loss = 5.68218656\n",
      "Iteration 566, loss = 5.68105531\n",
      "Iteration 567, loss = 5.67761687\n",
      "Iteration 568, loss = 5.68210782\n",
      "Iteration 569, loss = 5.66826840\n",
      "Iteration 570, loss = 5.67211225\n",
      "Iteration 571, loss = 5.68799857\n",
      "Iteration 572, loss = 5.67190426\n",
      "Iteration 573, loss = 5.67934628\n",
      "Iteration 574, loss = 5.66097849\n",
      "Iteration 575, loss = 5.67697638\n",
      "Iteration 576, loss = 5.67631528\n",
      "Iteration 577, loss = 5.68123124\n",
      "Iteration 578, loss = 5.67709825\n",
      "Iteration 579, loss = 5.68529995\n",
      "Iteration 580, loss = 5.68164154\n",
      "Iteration 581, loss = 5.67574366\n",
      "Iteration 582, loss = 5.68082412\n",
      "Iteration 583, loss = 5.68037301\n",
      "Iteration 584, loss = 5.68253408\n",
      "Iteration 585, loss = 5.67055767\n",
      "Iteration 586, loss = 5.67969810\n",
      "Iteration 587, loss = 5.67868807\n",
      "Iteration 588, loss = 5.67819386\n",
      "Iteration 589, loss = 5.67683972\n",
      "Iteration 590, loss = 5.69008284\n",
      "Iteration 591, loss = 5.66742464\n",
      "Iteration 592, loss = 5.67814266\n",
      "Iteration 593, loss = 5.68257110\n",
      "Iteration 594, loss = 5.66906162\n",
      "Iteration 595, loss = 5.66892388\n",
      "Iteration 596, loss = 5.67811612\n",
      "Iteration 597, loss = 5.67083574\n",
      "Iteration 598, loss = 5.67465051\n",
      "Iteration 599, loss = 5.68251710\n",
      "Iteration 600, loss = 5.68032535\n",
      "Iteration 601, loss = 5.68138298\n",
      "Iteration 602, loss = 5.67734852\n",
      "Iteration 603, loss = 5.67487907\n",
      "Iteration 604, loss = 5.66879811\n",
      "Iteration 605, loss = 5.69013778\n",
      "Iteration 606, loss = 5.66151413\n",
      "Iteration 607, loss = 5.67318756\n",
      "Iteration 608, loss = 5.68342744\n",
      "Iteration 609, loss = 5.67338412\n",
      "Iteration 610, loss = 5.68265478\n",
      "Iteration 611, loss = 5.68267117\n",
      "Iteration 612, loss = 5.67689286\n",
      "Iteration 613, loss = 5.67411505\n",
      "Iteration 614, loss = 5.68069560\n",
      "Iteration 615, loss = 5.68212670\n",
      "Iteration 616, loss = 5.66519781\n",
      "Iteration 617, loss = 5.67096313\n",
      "Iteration 618, loss = 5.68957077\n",
      "Iteration 619, loss = 5.67107963\n",
      "Iteration 620, loss = 5.67580222\n",
      "Iteration 621, loss = 5.67355360\n",
      "Iteration 622, loss = 5.67931574\n",
      "Iteration 623, loss = 5.67374338\n",
      "Iteration 624, loss = 5.67890194\n",
      "Iteration 625, loss = 5.66334900\n",
      "Iteration 626, loss = 5.67216226\n",
      "Iteration 627, loss = 5.67736585\n",
      "Iteration 628, loss = 5.66987097\n",
      "Iteration 629, loss = 5.67510199\n",
      "Iteration 630, loss = 5.67302223\n",
      "Iteration 631, loss = 5.67676672\n",
      "Iteration 632, loss = 5.67744951\n",
      "Iteration 633, loss = 5.67490771\n",
      "Iteration 634, loss = 5.66776198\n",
      "Iteration 635, loss = 5.67425138\n",
      "Iteration 636, loss = 5.66577397\n",
      "Iteration 637, loss = 5.68490755\n",
      "Iteration 638, loss = 5.67450681\n",
      "Iteration 639, loss = 5.66494664\n",
      "Iteration 640, loss = 5.68226763\n",
      "Iteration 641, loss = 5.66383626\n",
      "Iteration 642, loss = 5.66849099\n",
      "Iteration 643, loss = 5.66599374\n",
      "Iteration 644, loss = 5.67487751\n",
      "Iteration 645, loss = 5.66702773\n",
      "Iteration 646, loss = 5.67939027\n",
      "Iteration 647, loss = 5.68008376\n",
      "Iteration 648, loss = 5.68114173\n",
      "Iteration 649, loss = 5.67460465\n",
      "Iteration 650, loss = 5.68353278\n",
      "Iteration 651, loss = 5.67677824\n",
      "Iteration 652, loss = 5.69349979\n",
      "Iteration 653, loss = 5.67007347\n",
      "Iteration 654, loss = 5.67422547\n",
      "Iteration 655, loss = 5.67617422\n",
      "Iteration 656, loss = 5.67379630\n",
      "Iteration 657, loss = 5.67012966\n",
      "Iteration 658, loss = 5.66883946\n",
      "Iteration 659, loss = 5.67479849\n",
      "Iteration 660, loss = 5.67849181\n",
      "Iteration 661, loss = 5.66959739\n",
      "Iteration 662, loss = 5.66687783\n",
      "Iteration 663, loss = 5.67559498\n",
      "Iteration 664, loss = 5.67703513\n",
      "Iteration 665, loss = 5.68194608\n",
      "Iteration 666, loss = 5.67396185\n",
      "Iteration 667, loss = 5.68388769\n",
      "Iteration 668, loss = 5.66901046\n",
      "Iteration 669, loss = 5.68053609\n",
      "Iteration 670, loss = 5.67196092\n",
      "Iteration 671, loss = 5.67182060\n",
      "Iteration 672, loss = 5.67169901\n",
      "Iteration 673, loss = 5.67525235\n",
      "Iteration 674, loss = 5.66106882\n",
      "Iteration 675, loss = 5.68492702\n",
      "Iteration 676, loss = 5.67261032\n",
      "Iteration 677, loss = 5.66230821\n",
      "Iteration 678, loss = 5.67731126\n",
      "Iteration 679, loss = 5.67557744\n",
      "Iteration 680, loss = 5.67237494\n",
      "Iteration 681, loss = 5.67857817\n",
      "Iteration 682, loss = 5.67226358\n",
      "Iteration 683, loss = 5.67710235\n",
      "Iteration 684, loss = 5.67747460\n",
      "Iteration 685, loss = 5.66400807\n",
      "Iteration 686, loss = 5.66642423\n",
      "Iteration 687, loss = 5.66589261\n",
      "Iteration 688, loss = 5.67335308\n",
      "Iteration 689, loss = 5.67490137\n",
      "Iteration 690, loss = 5.67138992\n",
      "Iteration 691, loss = 5.68076697\n",
      "Iteration 692, loss = 5.66840136\n",
      "Iteration 693, loss = 5.66928487\n",
      "Iteration 694, loss = 5.67205093\n",
      "Iteration 695, loss = 5.66390137\n",
      "Iteration 696, loss = 5.66774565\n",
      "Iteration 697, loss = 5.67565186\n",
      "Iteration 698, loss = 5.68509148\n",
      "Iteration 699, loss = 5.67014450\n",
      "Iteration 700, loss = 5.67137224\n",
      "Iteration 701, loss = 5.66140248\n",
      "Iteration 702, loss = 5.68354123\n",
      "Iteration 703, loss = 5.67462096\n",
      "Iteration 704, loss = 5.67859731\n",
      "Iteration 705, loss = 5.66730023\n",
      "Iteration 706, loss = 5.68583392\n",
      "Iteration 707, loss = 5.66543087\n",
      "Iteration 708, loss = 5.67045003\n",
      "Iteration 709, loss = 5.67884179\n",
      "Iteration 710, loss = 5.68067177\n",
      "Iteration 711, loss = 5.67163795\n",
      "Iteration 712, loss = 5.66827664\n",
      "Iteration 713, loss = 5.67626708\n",
      "Iteration 714, loss = 5.67046307\n",
      "Iteration 715, loss = 5.67655384\n",
      "Iteration 716, loss = 5.66989397\n",
      "Iteration 717, loss = 5.67204522\n",
      "Iteration 718, loss = 5.66299587\n",
      "Iteration 719, loss = 5.67930973\n",
      "Iteration 720, loss = 5.67409229\n",
      "Iteration 721, loss = 5.67727538\n",
      "Iteration 722, loss = 5.66779435\n",
      "Iteration 723, loss = 5.67298836\n",
      "Iteration 724, loss = 5.67162550\n",
      "Iteration 725, loss = 5.66823631\n",
      "Iteration 726, loss = 5.67195879\n",
      "Iteration 727, loss = 5.66950452\n",
      "Iteration 728, loss = 5.67502104\n",
      "Iteration 729, loss = 5.66841451\n",
      "Iteration 730, loss = 5.67103956\n",
      "Iteration 731, loss = 5.66729684\n",
      "Iteration 732, loss = 5.67871758\n",
      "Iteration 733, loss = 5.67952883\n",
      "Iteration 734, loss = 5.67013501\n",
      "Iteration 735, loss = 5.66752816\n",
      "Iteration 736, loss = 5.68413734\n",
      "Iteration 737, loss = 5.66973823\n",
      "Iteration 738, loss = 5.66919991\n",
      "Iteration 739, loss = 5.66919212\n",
      "Iteration 740, loss = 5.66136409\n",
      "Iteration 741, loss = 5.66467263\n",
      "Iteration 742, loss = 5.66329799\n",
      "Iteration 743, loss = 5.67436141\n",
      "Iteration 744, loss = 5.66133632\n",
      "Iteration 745, loss = 5.67327538\n",
      "Iteration 746, loss = 5.67145518\n",
      "Iteration 747, loss = 5.66472011\n",
      "Iteration 748, loss = 5.67821265\n",
      "Iteration 749, loss = 5.67197614\n",
      "Iteration 750, loss = 5.67332658\n",
      "Iteration 751, loss = 5.66137436\n",
      "Iteration 752, loss = 5.66279743\n",
      "Iteration 753, loss = 5.67388061\n",
      "Iteration 754, loss = 5.66599181\n",
      "Iteration 755, loss = 5.66443709\n",
      "Iteration 756, loss = 5.67631956\n",
      "Iteration 757, loss = 5.66938752\n",
      "Iteration 758, loss = 5.67044801\n",
      "Iteration 759, loss = 5.66653214\n",
      "Iteration 760, loss = 5.66598942\n",
      "Iteration 761, loss = 5.66699691\n",
      "Iteration 762, loss = 5.67594734\n",
      "Iteration 763, loss = 5.65704781\n",
      "Iteration 764, loss = 5.67556858\n",
      "Iteration 765, loss = 5.67861046\n",
      "Iteration 766, loss = 5.67613888\n",
      "Iteration 767, loss = 5.67456873\n",
      "Iteration 768, loss = 5.67307617\n",
      "Iteration 769, loss = 5.67254790\n",
      "Iteration 770, loss = 5.67980464\n",
      "Iteration 771, loss = 5.66559380\n",
      "Iteration 772, loss = 5.66231708\n",
      "Iteration 773, loss = 5.66710650\n",
      "Iteration 774, loss = 5.66721499\n",
      "Iteration 775, loss = 5.66274510\n",
      "Iteration 776, loss = 5.67151635\n",
      "Iteration 777, loss = 5.67109855\n",
      "Iteration 778, loss = 5.67382519\n",
      "Iteration 779, loss = 5.67212107\n",
      "Iteration 780, loss = 5.67354706\n",
      "Iteration 781, loss = 5.66277789\n",
      "Iteration 782, loss = 5.67095667\n",
      "Iteration 783, loss = 5.66567875\n",
      "Iteration 784, loss = 5.67111600\n",
      "Iteration 785, loss = 5.67364944\n",
      "Iteration 786, loss = 5.66874830\n",
      "Iteration 787, loss = 5.66973540\n",
      "Iteration 788, loss = 5.67705292\n",
      "Iteration 789, loss = 5.67015219\n",
      "Iteration 790, loss = 5.66441213\n",
      "Iteration 791, loss = 5.66941140\n",
      "Iteration 792, loss = 5.66182864\n",
      "Iteration 793, loss = 5.67509293\n",
      "Iteration 794, loss = 5.66744122\n",
      "Iteration 795, loss = 5.67173729\n",
      "Iteration 796, loss = 5.67748857\n",
      "Iteration 797, loss = 5.66445711\n",
      "Iteration 798, loss = 5.67351607\n",
      "Iteration 799, loss = 5.66462738\n",
      "Iteration 800, loss = 5.67052924\n",
      "Iteration 801, loss = 5.66042184\n",
      "Iteration 802, loss = 5.67992687\n",
      "Iteration 803, loss = 5.66834929\n",
      "Iteration 804, loss = 5.67624408\n",
      "Iteration 805, loss = 5.67453372\n",
      "Iteration 806, loss = 5.67277872\n",
      "Iteration 807, loss = 5.66597312\n",
      "Iteration 808, loss = 5.68040209\n",
      "Iteration 809, loss = 5.67686086\n",
      "Iteration 810, loss = 5.67009031\n",
      "Iteration 811, loss = 5.66347136\n",
      "Iteration 812, loss = 5.68310412\n",
      "Iteration 813, loss = 5.66873297\n",
      "Iteration 814, loss = 5.67385872\n",
      "Iteration 815, loss = 5.66607643\n",
      "Iteration 816, loss = 5.66835269\n",
      "Iteration 817, loss = 5.67253006\n",
      "Iteration 818, loss = 5.66435865\n",
      "Iteration 819, loss = 5.66715087\n",
      "Iteration 820, loss = 5.67274209\n",
      "Iteration 821, loss = 5.66897869\n",
      "Iteration 822, loss = 5.67903648\n",
      "Iteration 823, loss = 5.67366064\n",
      "Iteration 824, loss = 5.66126085\n",
      "Iteration 825, loss = 5.66488866\n",
      "Iteration 826, loss = 5.66988674\n",
      "Iteration 827, loss = 5.66542289\n",
      "Iteration 828, loss = 5.66424522\n",
      "Iteration 829, loss = 5.67084434\n",
      "Iteration 830, loss = 5.66588172\n",
      "Iteration 831, loss = 5.66864359\n",
      "Iteration 832, loss = 5.67353827\n",
      "Iteration 833, loss = 5.67515572\n",
      "Iteration 834, loss = 5.66009346\n",
      "Iteration 835, loss = 5.67048248\n",
      "Iteration 836, loss = 5.66702762\n",
      "Iteration 837, loss = 5.66403275\n",
      "Iteration 838, loss = 5.67465823\n",
      "Iteration 839, loss = 5.66916905\n",
      "Iteration 840, loss = 5.67239916\n",
      "Iteration 841, loss = 5.66839303\n",
      "Iteration 842, loss = 5.66834544\n",
      "Iteration 843, loss = 5.67297557\n",
      "Iteration 844, loss = 5.65862986\n",
      "Iteration 845, loss = 5.66534713\n",
      "Iteration 846, loss = 5.66703085\n",
      "Iteration 847, loss = 5.66498834\n",
      "Iteration 848, loss = 5.67452318\n",
      "Iteration 849, loss = 5.66743176\n",
      "Iteration 850, loss = 5.66186000\n",
      "Iteration 851, loss = 5.67399555\n",
      "Iteration 852, loss = 5.67196523\n",
      "Iteration 853, loss = 5.66781440\n",
      "Iteration 854, loss = 5.66689404\n",
      "Iteration 855, loss = 5.68831729\n",
      "Iteration 856, loss = 5.66480411\n",
      "Iteration 857, loss = 5.65985908\n",
      "Iteration 858, loss = 5.66400662\n",
      "Iteration 859, loss = 5.67087526\n",
      "Iteration 860, loss = 5.66682150\n",
      "Iteration 861, loss = 5.66524741\n",
      "Iteration 862, loss = 5.65688057\n",
      "Iteration 863, loss = 5.67638857\n",
      "Iteration 864, loss = 5.66178494\n",
      "Iteration 865, loss = 5.67013480\n",
      "Iteration 866, loss = 5.67044270\n",
      "Iteration 867, loss = 5.66724955\n",
      "Iteration 868, loss = 5.68318642\n",
      "Iteration 869, loss = 5.65426361\n",
      "Iteration 870, loss = 5.66798054\n",
      "Iteration 871, loss = 5.67799503\n",
      "Iteration 872, loss = 5.66605422\n",
      "Iteration 873, loss = 5.66171594\n",
      "Iteration 874, loss = 5.67523459\n",
      "Iteration 875, loss = 5.66470773\n",
      "Iteration 876, loss = 5.66270254\n",
      "Iteration 877, loss = 5.67710649\n",
      "Iteration 878, loss = 5.67135484\n",
      "Iteration 879, loss = 5.66141898\n",
      "Iteration 880, loss = 5.66092764\n",
      "Iteration 881, loss = 5.67516522\n",
      "Iteration 882, loss = 5.66417182\n",
      "Iteration 883, loss = 5.66447860\n",
      "Iteration 884, loss = 5.67927382\n",
      "Iteration 885, loss = 5.65387064\n",
      "Iteration 886, loss = 5.68143968\n",
      "Iteration 887, loss = 5.66205540\n",
      "Iteration 888, loss = 5.67107740\n",
      "Iteration 889, loss = 5.66212941\n",
      "Iteration 890, loss = 5.66603051\n",
      "Iteration 891, loss = 5.67023977\n",
      "Iteration 892, loss = 5.66789400\n",
      "Iteration 893, loss = 5.67115430\n",
      "Iteration 894, loss = 5.65892505\n",
      "Iteration 895, loss = 5.65490056\n",
      "Iteration 896, loss = 5.66820975\n",
      "Iteration 897, loss = 5.67752680\n",
      "Iteration 898, loss = 5.67149190\n",
      "Iteration 899, loss = 5.66697560\n",
      "Iteration 900, loss = 5.65589187\n",
      "Iteration 901, loss = 5.66315314\n",
      "Iteration 902, loss = 5.66693136\n",
      "Iteration 903, loss = 5.66347947\n",
      "Iteration 904, loss = 5.66763177\n",
      "Iteration 905, loss = 5.66752154\n",
      "Iteration 906, loss = 5.66146496\n",
      "Iteration 907, loss = 5.67101588\n",
      "Iteration 908, loss = 5.67782315\n",
      "Iteration 909, loss = 5.66748405\n",
      "Iteration 910, loss = 5.67520669\n",
      "Iteration 911, loss = 5.67159647\n",
      "Iteration 912, loss = 5.65999111\n",
      "Iteration 913, loss = 5.66866976\n",
      "Iteration 914, loss = 5.68032955\n",
      "Iteration 915, loss = 5.66648264\n",
      "Iteration 916, loss = 5.66603477\n",
      "Iteration 917, loss = 5.65748655\n",
      "Iteration 918, loss = 5.66687716\n",
      "Iteration 919, loss = 5.66616432\n",
      "Iteration 920, loss = 5.66438935\n",
      "Iteration 921, loss = 5.68403766\n",
      "Iteration 922, loss = 5.67046357\n",
      "Iteration 923, loss = 5.66660472\n",
      "Iteration 924, loss = 5.66578831\n",
      "Iteration 925, loss = 5.66153733\n",
      "Iteration 926, loss = 5.66227784\n",
      "Iteration 927, loss = 5.66083456\n",
      "Iteration 928, loss = 5.67832647\n",
      "Iteration 929, loss = 5.66834239\n",
      "Iteration 930, loss = 5.66433713\n",
      "Iteration 931, loss = 5.66718997\n",
      "Iteration 932, loss = 5.67433263\n",
      "Iteration 933, loss = 5.66767692\n",
      "Iteration 934, loss = 5.65504759\n",
      "Iteration 935, loss = 5.65901549\n",
      "Iteration 936, loss = 5.67200416\n",
      "Iteration 937, loss = 5.67039671\n",
      "Iteration 938, loss = 5.66315383\n",
      "Iteration 939, loss = 5.67554591\n",
      "Iteration 940, loss = 5.66586960\n",
      "Iteration 941, loss = 5.65847966\n",
      "Iteration 942, loss = 5.66159393\n",
      "Iteration 943, loss = 5.65943538\n",
      "Iteration 944, loss = 5.66933771\n",
      "Iteration 945, loss = 5.66233720\n",
      "Iteration 946, loss = 5.66156907\n",
      "Iteration 947, loss = 5.67026047\n",
      "Iteration 948, loss = 5.67113755\n",
      "Iteration 949, loss = 5.66817412\n",
      "Iteration 950, loss = 5.66247062\n",
      "Iteration 951, loss = 5.66241714\n",
      "Iteration 952, loss = 5.66216986\n",
      "Iteration 953, loss = 5.66630460\n",
      "Iteration 954, loss = 5.67819287\n",
      "Iteration 955, loss = 5.66070179\n",
      "Iteration 956, loss = 5.66442356\n",
      "Iteration 957, loss = 5.66785497\n",
      "Iteration 958, loss = 5.67021452\n",
      "Iteration 959, loss = 5.67102458\n",
      "Iteration 960, loss = 5.65737637\n",
      "Iteration 961, loss = 5.66541418\n",
      "Iteration 962, loss = 5.65264678\n",
      "Iteration 963, loss = 5.67339003\n",
      "Iteration 964, loss = 5.67561692\n",
      "Iteration 965, loss = 5.66888834\n",
      "Iteration 966, loss = 5.67416679\n",
      "Iteration 967, loss = 5.65847438\n",
      "Iteration 968, loss = 5.66902431\n",
      "Iteration 969, loss = 5.66925113\n",
      "Iteration 970, loss = 5.67363086\n",
      "Iteration 971, loss = 5.66858823\n",
      "Iteration 972, loss = 5.66046752\n",
      "Iteration 973, loss = 5.67201412\n",
      "Iteration 974, loss = 5.66728242\n",
      "Iteration 975, loss = 5.66532874\n",
      "Iteration 976, loss = 5.66181637\n",
      "Iteration 977, loss = 5.66552397\n",
      "Iteration 978, loss = 5.66450981\n",
      "Iteration 979, loss = 5.66191504\n",
      "Iteration 980, loss = 5.65699251\n",
      "Iteration 981, loss = 5.66597857\n",
      "Iteration 982, loss = 5.66828122\n",
      "Iteration 983, loss = 5.66959857\n",
      "Iteration 984, loss = 5.66746361\n",
      "Iteration 985, loss = 5.67793541\n",
      "Iteration 986, loss = 5.66245515\n",
      "Iteration 987, loss = 5.67230702\n",
      "Iteration 988, loss = 5.65689071\n",
      "Iteration 989, loss = 5.65966078\n",
      "Iteration 990, loss = 5.65799692\n",
      "Iteration 991, loss = 5.65820138\n",
      "Iteration 992, loss = 5.66177103\n",
      "Iteration 993, loss = 5.66806750\n",
      "Iteration 994, loss = 5.65559034\n",
      "Iteration 995, loss = 5.66649867\n",
      "Iteration 996, loss = 5.65942110\n",
      "Iteration 997, loss = 5.66608231\n",
      "Iteration 998, loss = 5.67411474\n",
      "Iteration 999, loss = 5.66452292\n",
      "Iteration 1000, loss = 5.66856139\n",
      "Iteration 1001, loss = 5.67124431\n",
      "Iteration 1002, loss = 5.66786959\n",
      "Iteration 1003, loss = 5.65307560\n",
      "Iteration 1004, loss = 5.65688686\n",
      "Iteration 1005, loss = 5.66232614\n",
      "Iteration 1006, loss = 5.66865624\n",
      "Iteration 1007, loss = 5.66482934\n",
      "Iteration 1008, loss = 5.66024752\n",
      "Iteration 1009, loss = 5.67062623\n",
      "Iteration 1010, loss = 5.66835420\n",
      "Iteration 1011, loss = 5.66379907\n",
      "Iteration 1012, loss = 5.65960844\n",
      "Iteration 1013, loss = 5.65834216\n",
      "Iteration 1014, loss = 5.66248546\n",
      "Iteration 1015, loss = 5.66016907\n",
      "Iteration 1016, loss = 5.67129856\n",
      "Iteration 1017, loss = 5.66137963\n",
      "Iteration 1018, loss = 5.67068400\n",
      "Iteration 1019, loss = 5.65846767\n",
      "Iteration 1020, loss = 5.66332736\n",
      "Iteration 1021, loss = 5.66710741\n",
      "Iteration 1022, loss = 5.65282592\n",
      "Iteration 1023, loss = 5.66073024\n",
      "Iteration 1024, loss = 5.66403135\n",
      "Iteration 1025, loss = 5.66507022\n",
      "Iteration 1026, loss = 5.65969249\n",
      "Iteration 1027, loss = 5.66642400\n",
      "Iteration 1028, loss = 5.67028202\n",
      "Iteration 1029, loss = 5.66562032\n",
      "Iteration 1030, loss = 5.66896936\n",
      "Iteration 1031, loss = 5.66146383\n",
      "Iteration 1032, loss = 5.65472252\n",
      "Iteration 1033, loss = 5.66175993\n",
      "Iteration 1034, loss = 5.66257559\n",
      "Iteration 1035, loss = 5.65965959\n",
      "Iteration 1036, loss = 5.66737442\n",
      "Iteration 1037, loss = 5.65643360\n",
      "Iteration 1038, loss = 5.65505485\n",
      "Iteration 1039, loss = 5.67257127\n",
      "Iteration 1040, loss = 5.66249474\n",
      "Iteration 1041, loss = 5.65241217\n",
      "Iteration 1042, loss = 5.66404511\n",
      "Iteration 1043, loss = 5.66538329\n",
      "Iteration 1044, loss = 5.66272266\n",
      "Iteration 1045, loss = 5.65803870\n",
      "Iteration 1046, loss = 5.66675633\n",
      "Iteration 1047, loss = 5.65976547\n",
      "Iteration 1048, loss = 5.67025751\n",
      "Iteration 1049, loss = 5.65295801\n",
      "Iteration 1050, loss = 5.65877904\n",
      "Iteration 1051, loss = 5.65915490\n",
      "Iteration 1052, loss = 5.65326826\n",
      "Iteration 1053, loss = 5.66632639\n",
      "Iteration 1054, loss = 5.65887164\n",
      "Iteration 1055, loss = 5.66406054\n",
      "Iteration 1056, loss = 5.66687631\n",
      "Iteration 1057, loss = 5.66399673\n",
      "Iteration 1058, loss = 5.66899211\n",
      "Iteration 1059, loss = 5.67459649\n",
      "Iteration 1060, loss = 5.65457916\n",
      "Iteration 1061, loss = 5.65909356\n",
      "Iteration 1062, loss = 5.67211564\n",
      "Iteration 1063, loss = 5.65611892\n",
      "Iteration 1064, loss = 5.66047144\n",
      "Iteration 1065, loss = 5.65979948\n",
      "Iteration 1066, loss = 5.66836335\n",
      "Iteration 1067, loss = 5.65563269\n",
      "Iteration 1068, loss = 5.65956249\n",
      "Iteration 1069, loss = 5.66029849\n",
      "Iteration 1070, loss = 5.66150259\n",
      "Iteration 1071, loss = 5.65359566\n",
      "Iteration 1072, loss = 5.65936441\n",
      "Iteration 1073, loss = 5.67422876\n",
      "Iteration 1074, loss = 5.65441549\n",
      "Iteration 1075, loss = 5.65190289\n",
      "Iteration 1076, loss = 5.66217181\n",
      "Iteration 1077, loss = 5.65742175\n",
      "Iteration 1078, loss = 5.66121336\n",
      "Iteration 1079, loss = 5.65480202\n",
      "Iteration 1080, loss = 5.66013799\n",
      "Iteration 1081, loss = 5.65908334\n",
      "Iteration 1082, loss = 5.65749890\n",
      "Iteration 1083, loss = 5.65361555\n",
      "Iteration 1084, loss = 5.66953253\n",
      "Iteration 1085, loss = 5.65971201\n",
      "Iteration 1086, loss = 5.66475918\n",
      "Iteration 1087, loss = 5.65979113\n",
      "Iteration 1088, loss = 5.65831680\n",
      "Iteration 1089, loss = 5.65911608\n",
      "Iteration 1090, loss = 5.65374896\n",
      "Iteration 1091, loss = 5.65088388\n",
      "Iteration 1092, loss = 5.65815210\n",
      "Iteration 1093, loss = 5.66145939\n",
      "Iteration 1094, loss = 5.65758161\n",
      "Iteration 1095, loss = 5.66249267\n",
      "Iteration 1096, loss = 5.66384884\n",
      "Iteration 1097, loss = 5.65467786\n",
      "Iteration 1098, loss = 5.66018695\n",
      "Iteration 1099, loss = 5.65676532\n",
      "Iteration 1100, loss = 5.66528529\n",
      "Iteration 1101, loss = 5.65793715\n",
      "Iteration 1102, loss = 5.65802295\n",
      "Iteration 1103, loss = 5.66547605\n",
      "Iteration 1104, loss = 5.66557545\n",
      "Iteration 1105, loss = 5.66425266\n",
      "Iteration 1106, loss = 5.65595290\n",
      "Iteration 1107, loss = 5.66330825\n",
      "Iteration 1108, loss = 5.66029043\n",
      "Iteration 1109, loss = 5.65876217\n",
      "Iteration 1110, loss = 5.65103930\n",
      "Iteration 1111, loss = 5.65919248\n",
      "Iteration 1112, loss = 5.64735828\n",
      "Iteration 1113, loss = 5.65832322\n",
      "Iteration 1114, loss = 5.65740239\n",
      "Iteration 1115, loss = 5.66148032\n",
      "Iteration 1116, loss = 5.66564626\n",
      "Iteration 1117, loss = 5.66466108\n",
      "Iteration 1118, loss = 5.65758608\n",
      "Iteration 1119, loss = 5.65339768\n",
      "Iteration 1120, loss = 5.66479302\n",
      "Iteration 1121, loss = 5.66363750\n",
      "Iteration 1122, loss = 5.66744817\n",
      "Iteration 1123, loss = 5.65716570\n",
      "Iteration 1124, loss = 5.66174445\n",
      "Iteration 1125, loss = 5.66500937\n",
      "Iteration 1126, loss = 5.64657595\n",
      "Iteration 1127, loss = 5.66378804\n",
      "Iteration 1128, loss = 5.65813560\n",
      "Iteration 1129, loss = 5.65480936\n",
      "Iteration 1130, loss = 5.66325507\n",
      "Iteration 1131, loss = 5.65958251\n",
      "Iteration 1132, loss = 5.65740613\n",
      "Iteration 1133, loss = 5.66646256\n",
      "Iteration 1134, loss = 5.65907932\n",
      "Iteration 1135, loss = 5.66615922\n",
      "Iteration 1136, loss = 5.65313003\n",
      "Iteration 1137, loss = 5.65566353\n",
      "Iteration 1138, loss = 5.66339576\n",
      "Iteration 1139, loss = 5.66257496\n",
      "Iteration 1140, loss = 5.65598505\n",
      "Iteration 1141, loss = 5.65513895\n",
      "Iteration 1142, loss = 5.66654620\n",
      "Iteration 1143, loss = 5.65179095\n",
      "Iteration 1144, loss = 5.65929788\n",
      "Iteration 1145, loss = 5.65879071\n",
      "Iteration 1146, loss = 5.66057977\n",
      "Iteration 1147, loss = 5.65381877\n",
      "Iteration 1148, loss = 5.65217452\n",
      "Iteration 1149, loss = 5.65859545\n",
      "Iteration 1150, loss = 5.65924443\n",
      "Iteration 1151, loss = 5.65927866\n",
      "Iteration 1152, loss = 5.65351450\n",
      "Iteration 1153, loss = 5.66224941\n",
      "Iteration 1154, loss = 5.65101504\n",
      "Iteration 1155, loss = 5.65797703\n",
      "Iteration 1156, loss = 5.65724601\n",
      "Iteration 1157, loss = 5.66848044\n",
      "Iteration 1158, loss = 5.66370937\n",
      "Iteration 1159, loss = 5.65319161\n",
      "Iteration 1160, loss = 5.65924551\n",
      "Iteration 1161, loss = 5.66301205\n",
      "Iteration 1162, loss = 5.64717204\n",
      "Iteration 1163, loss = 5.65686851\n",
      "Iteration 1164, loss = 5.65506141\n",
      "Iteration 1165, loss = 5.65611870\n",
      "Iteration 1166, loss = 5.65698788\n",
      "Iteration 1167, loss = 5.65018219\n",
      "Iteration 1168, loss = 5.66492871\n",
      "Iteration 1169, loss = 5.66541586\n",
      "Iteration 1170, loss = 5.66371457\n",
      "Iteration 1171, loss = 5.65105727\n",
      "Iteration 1172, loss = 5.65674908\n",
      "Iteration 1173, loss = 5.64614948\n",
      "Iteration 1174, loss = 5.65564764\n",
      "Iteration 1175, loss = 5.65978447\n",
      "Iteration 1176, loss = 5.67109020\n",
      "Iteration 1177, loss = 5.65959923\n",
      "Iteration 1178, loss = 5.65591591\n",
      "Iteration 1179, loss = 5.65733666\n",
      "Iteration 1180, loss = 5.65581486\n",
      "Iteration 1181, loss = 5.66097564\n",
      "Iteration 1182, loss = 5.65565889\n",
      "Iteration 1183, loss = 5.64872886\n",
      "Iteration 1184, loss = 5.66194734\n",
      "Iteration 1185, loss = 5.65682822\n",
      "Iteration 1186, loss = 5.65745305\n",
      "Iteration 1187, loss = 5.66349213\n",
      "Iteration 1188, loss = 5.64527653\n",
      "Iteration 1189, loss = 5.65440748\n",
      "Iteration 1190, loss = 5.66576023\n",
      "Iteration 1191, loss = 5.65441669\n",
      "Iteration 1192, loss = 5.65798935\n",
      "Iteration 1193, loss = 5.66112737\n",
      "Iteration 1194, loss = 5.65691231\n",
      "Iteration 1195, loss = 5.65374573\n",
      "Iteration 1196, loss = 5.65241810\n",
      "Iteration 1197, loss = 5.65476495\n",
      "Iteration 1198, loss = 5.65888281\n",
      "Iteration 1199, loss = 5.65793906\n",
      "Iteration 1200, loss = 5.65903127\n",
      "Iteration 1201, loss = 5.65736058\n",
      "Iteration 1202, loss = 5.65714137\n",
      "Iteration 1203, loss = 5.66193677\n",
      "Iteration 1204, loss = 5.67071524\n",
      "Iteration 1205, loss = 5.64840818\n",
      "Iteration 1206, loss = 5.65835555\n",
      "Iteration 1207, loss = 5.65974707\n",
      "Iteration 1208, loss = 5.65158821\n",
      "Iteration 1209, loss = 5.65711633\n",
      "Iteration 1210, loss = 5.65076724\n",
      "Iteration 1211, loss = 5.65164725\n",
      "Iteration 1212, loss = 5.65225111\n",
      "Iteration 1213, loss = 5.65181030\n",
      "Iteration 1214, loss = 5.65427640\n",
      "Iteration 1215, loss = 5.65373643\n",
      "Iteration 1216, loss = 5.66085402\n",
      "Iteration 1217, loss = 5.65099494\n",
      "Iteration 1218, loss = 5.64758621\n",
      "Iteration 1219, loss = 5.65523626\n",
      "Iteration 1220, loss = 5.66043912\n",
      "Iteration 1221, loss = 5.64920719\n",
      "Iteration 1222, loss = 5.65589956\n",
      "Iteration 1223, loss = 5.64606600\n",
      "Iteration 1224, loss = 5.65930802\n",
      "Iteration 1225, loss = 5.67055200\n",
      "Iteration 1226, loss = 5.65387137\n",
      "Iteration 1227, loss = 5.64550982\n",
      "Iteration 1228, loss = 5.66551084\n",
      "Iteration 1229, loss = 5.64509419\n",
      "Iteration 1230, loss = 5.65633534\n",
      "Iteration 1231, loss = 5.65785249\n",
      "Iteration 1232, loss = 5.66408356\n",
      "Iteration 1233, loss = 5.64738907\n",
      "Iteration 1234, loss = 5.65637754\n",
      "Iteration 1235, loss = 5.64490194\n",
      "Iteration 1236, loss = 5.65739810\n",
      "Iteration 1237, loss = 5.65270800\n",
      "Iteration 1238, loss = 5.65648991\n",
      "Iteration 1239, loss = 5.65982447\n",
      "Iteration 1240, loss = 5.65093424\n",
      "Iteration 1241, loss = 5.65689476\n",
      "Iteration 1242, loss = 5.65357548\n",
      "Iteration 1243, loss = 5.65725138\n",
      "Iteration 1244, loss = 5.64926234\n",
      "Iteration 1245, loss = 5.65126304\n",
      "Iteration 1246, loss = 5.65463224\n",
      "Iteration 1247, loss = 5.65611616\n",
      "Iteration 1248, loss = 5.66843899\n",
      "Iteration 1249, loss = 5.66238261\n",
      "Iteration 1250, loss = 5.65650841\n",
      "Iteration 1251, loss = 5.65617179\n",
      "Iteration 1252, loss = 5.64575391\n",
      "Iteration 1253, loss = 5.65287815\n",
      "Iteration 1254, loss = 5.64784808\n",
      "Iteration 1255, loss = 5.65546966\n",
      "Iteration 1256, loss = 5.65565059\n",
      "Iteration 1257, loss = 5.65501739\n",
      "Iteration 1258, loss = 5.64920003\n",
      "Iteration 1259, loss = 5.65406950\n",
      "Iteration 1260, loss = 5.65175635\n",
      "Iteration 1261, loss = 5.65695371\n",
      "Iteration 1262, loss = 5.64742845\n",
      "Iteration 1263, loss = 5.65344683\n",
      "Iteration 1264, loss = 5.65965543\n",
      "Iteration 1265, loss = 5.64391138\n",
      "Iteration 1266, loss = 5.65372040\n",
      "Iteration 1267, loss = 5.65768388\n",
      "Iteration 1268, loss = 5.65094061\n",
      "Iteration 1269, loss = 5.65230882\n",
      "Iteration 1270, loss = 5.65392840\n",
      "Iteration 1271, loss = 5.66124404\n",
      "Iteration 1272, loss = 5.67095320\n",
      "Iteration 1273, loss = 5.64859583\n",
      "Iteration 1274, loss = 5.65006853\n",
      "Iteration 1275, loss = 5.64863168\n",
      "Iteration 1276, loss = 5.65958412\n",
      "Iteration 1277, loss = 5.63788049\n",
      "Iteration 1278, loss = 5.65947733\n",
      "Iteration 1279, loss = 5.64586835\n",
      "Iteration 1280, loss = 5.65147519\n",
      "Iteration 1281, loss = 5.64864831\n",
      "Iteration 1282, loss = 5.66195384\n",
      "Iteration 1283, loss = 5.64704991\n",
      "Iteration 1284, loss = 5.66133877\n",
      "Iteration 1285, loss = 5.65437821\n",
      "Iteration 1286, loss = 5.66060236\n",
      "Iteration 1287, loss = 5.64139813\n",
      "Iteration 1288, loss = 5.64306006\n",
      "Iteration 1289, loss = 5.64875259\n",
      "Iteration 1290, loss = 5.65664099\n",
      "Iteration 1291, loss = 5.65212665\n",
      "Iteration 1292, loss = 5.65959013\n",
      "Iteration 1293, loss = 5.65038541\n",
      "Iteration 1294, loss = 5.65633512\n",
      "Iteration 1295, loss = 5.64645759\n",
      "Iteration 1296, loss = 5.65131594\n",
      "Iteration 1297, loss = 5.65230084\n",
      "Iteration 1298, loss = 5.65381849\n",
      "Iteration 1299, loss = 5.65503969\n",
      "Iteration 1300, loss = 5.65562218\n",
      "Iteration 1301, loss = 5.65641113\n",
      "Iteration 1302, loss = 5.64080261\n",
      "Iteration 1303, loss = 5.65937449\n",
      "Iteration 1304, loss = 5.64763786\n",
      "Iteration 1305, loss = 5.65076653\n",
      "Iteration 1306, loss = 5.65159370\n",
      "Iteration 1307, loss = 5.64144230\n",
      "Iteration 1308, loss = 5.65055456\n",
      "Iteration 1309, loss = 5.63977346\n",
      "Iteration 1310, loss = 5.65618153\n",
      "Iteration 1311, loss = 5.66391245\n",
      "Iteration 1312, loss = 5.64459175\n",
      "Iteration 1313, loss = 5.65871561\n",
      "Iteration 1314, loss = 5.65512860\n",
      "Iteration 1315, loss = 5.64949567\n",
      "Iteration 1316, loss = 5.64495267\n",
      "Iteration 1317, loss = 5.66174341\n",
      "Iteration 1318, loss = 5.65073356\n",
      "Iteration 1319, loss = 5.65791952\n",
      "Iteration 1320, loss = 5.64897257\n",
      "Iteration 1321, loss = 5.65964664\n",
      "Iteration 1322, loss = 5.64400480\n",
      "Iteration 1323, loss = 5.65147387\n",
      "Iteration 1324, loss = 5.65020839\n",
      "Iteration 1325, loss = 5.65849029\n",
      "Iteration 1326, loss = 5.64633360\n",
      "Iteration 1327, loss = 5.65572444\n",
      "Iteration 1328, loss = 5.64435782\n",
      "Iteration 1329, loss = 5.65223163\n",
      "Iteration 1330, loss = 5.65175312\n",
      "Iteration 1331, loss = 5.64489632\n",
      "Iteration 1332, loss = 5.65499065\n",
      "Iteration 1333, loss = 5.63040885\n",
      "Iteration 1334, loss = 5.65608954\n",
      "Iteration 1335, loss = 5.65218354\n",
      "Iteration 1336, loss = 5.65706379\n",
      "Iteration 1337, loss = 5.64315572\n",
      "Iteration 1338, loss = 5.65059442\n",
      "Iteration 1339, loss = 5.64557373\n",
      "Iteration 1340, loss = 5.64634854\n",
      "Iteration 1341, loss = 5.66802513\n",
      "Iteration 1342, loss = 5.64364712\n",
      "Iteration 1343, loss = 5.64375054\n",
      "Iteration 1344, loss = 5.64738002\n",
      "Iteration 1345, loss = 5.64907347\n",
      "Iteration 1346, loss = 5.65310801\n",
      "Iteration 1347, loss = 5.63884284\n",
      "Iteration 1348, loss = 5.65243309\n",
      "Iteration 1349, loss = 5.65420312\n",
      "Iteration 1350, loss = 5.64944516\n",
      "Iteration 1351, loss = 5.64957786\n",
      "Iteration 1352, loss = 5.65581036\n",
      "Iteration 1353, loss = 5.65576600\n",
      "Iteration 1354, loss = 5.65473782\n",
      "Iteration 1355, loss = 5.63997584\n",
      "Iteration 1356, loss = 5.64724062\n",
      "Iteration 1357, loss = 5.65317266\n",
      "Iteration 1358, loss = 5.64457857\n",
      "Iteration 1359, loss = 5.64877086\n",
      "Iteration 1360, loss = 5.64933704\n",
      "Iteration 1361, loss = 5.66462259\n",
      "Iteration 1362, loss = 5.63435033\n",
      "Iteration 1363, loss = 5.66007498\n",
      "Iteration 1364, loss = 5.64333995\n",
      "Iteration 1365, loss = 5.64722370\n",
      "Iteration 1366, loss = 5.65592724\n",
      "Iteration 1367, loss = 5.64492242\n",
      "Iteration 1368, loss = 5.64494614\n",
      "Iteration 1369, loss = 5.64813125\n",
      "Iteration 1370, loss = 5.64707239\n",
      "Iteration 1371, loss = 5.65694043\n",
      "Iteration 1372, loss = 5.64152378\n",
      "Iteration 1373, loss = 5.65476773\n",
      "Iteration 1374, loss = 5.65694420\n",
      "Iteration 1375, loss = 5.64614076\n",
      "Iteration 1376, loss = 5.65554215\n",
      "Iteration 1377, loss = 5.64806780\n",
      "Iteration 1378, loss = 5.64940221\n",
      "Iteration 1379, loss = 5.64088522\n",
      "Iteration 1380, loss = 5.64454001\n",
      "Iteration 1381, loss = 5.65185924\n",
      "Iteration 1382, loss = 5.65485977\n",
      "Iteration 1383, loss = 5.64498401\n",
      "Iteration 1384, loss = 5.65383500\n",
      "Iteration 1385, loss = 5.64290259\n",
      "Iteration 1386, loss = 5.65257156\n",
      "Iteration 1387, loss = 5.64630473\n",
      "Iteration 1388, loss = 5.63243654\n",
      "Iteration 1389, loss = 5.65470624\n",
      "Iteration 1390, loss = 5.64526053\n",
      "Iteration 1391, loss = 5.63873313\n",
      "Iteration 1392, loss = 5.64751254\n",
      "Iteration 1393, loss = 5.64990386\n",
      "Iteration 1394, loss = 5.65690107\n",
      "Iteration 1395, loss = 5.64180995\n",
      "Iteration 1396, loss = 5.64746440\n",
      "Iteration 1397, loss = 5.64546347\n",
      "Iteration 1398, loss = 5.65623238\n",
      "Iteration 1399, loss = 5.64915023\n",
      "Iteration 1400, loss = 5.64483185\n",
      "Iteration 1401, loss = 5.65356561\n",
      "Iteration 1402, loss = 5.64494433\n",
      "Iteration 1403, loss = 5.65850543\n",
      "Iteration 1404, loss = 5.64665371\n",
      "Iteration 1405, loss = 5.65459306\n",
      "Iteration 1406, loss = 5.63940967\n",
      "Iteration 1407, loss = 5.65383551\n",
      "Iteration 1408, loss = 5.64687809\n",
      "Iteration 1409, loss = 5.64249017\n",
      "Iteration 1410, loss = 5.65655910\n",
      "Iteration 1411, loss = 5.63909144\n",
      "Iteration 1412, loss = 5.65265981\n",
      "Iteration 1413, loss = 5.65180117\n",
      "Iteration 1414, loss = 5.64504902\n",
      "Iteration 1415, loss = 5.64481026\n",
      "Iteration 1416, loss = 5.65291500\n",
      "Iteration 1417, loss = 5.64381014\n",
      "Iteration 1418, loss = 5.63445467\n",
      "Iteration 1419, loss = 5.64993197\n",
      "Iteration 1420, loss = 5.65326929\n",
      "Iteration 1421, loss = 5.65145642\n",
      "Iteration 1422, loss = 5.63663362\n",
      "Iteration 1423, loss = 5.64387602\n",
      "Iteration 1424, loss = 5.64570230\n",
      "Iteration 1425, loss = 5.65766921\n",
      "Iteration 1426, loss = 5.65383848\n",
      "Iteration 1427, loss = 5.64795454\n",
      "Iteration 1428, loss = 5.64864499\n",
      "Iteration 1429, loss = 5.63598880\n",
      "Iteration 1430, loss = 5.64178220\n",
      "Iteration 1431, loss = 5.65325882\n",
      "Iteration 1432, loss = 5.64983330\n",
      "Iteration 1433, loss = 5.65253763\n",
      "Iteration 1434, loss = 5.64304609\n",
      "Iteration 1435, loss = 5.64864612\n",
      "Iteration 1436, loss = 5.64645093\n",
      "Iteration 1437, loss = 5.64547454\n",
      "Iteration 1438, loss = 5.64411522\n",
      "Iteration 1439, loss = 5.65290141\n",
      "Iteration 1440, loss = 5.64424635\n",
      "Iteration 1441, loss = 5.65370253\n",
      "Iteration 1442, loss = 5.65663846\n",
      "Iteration 1443, loss = 5.64333742\n",
      "Iteration 1444, loss = 5.64716659\n",
      "Iteration 1445, loss = 5.62904149\n",
      "Iteration 1446, loss = 5.64753143\n",
      "Iteration 1447, loss = 5.64383017\n",
      "Iteration 1448, loss = 5.64368265\n",
      "Iteration 1449, loss = 5.64714215\n",
      "Iteration 1450, loss = 5.65375788\n",
      "Iteration 1451, loss = 5.64998487\n",
      "Iteration 1452, loss = 5.64698256\n",
      "Iteration 1453, loss = 5.64343468\n",
      "Iteration 1454, loss = 5.65384830\n",
      "Iteration 1455, loss = 5.63483015\n",
      "Iteration 1456, loss = 5.64312257\n",
      "Iteration 1457, loss = 5.63965542\n",
      "Iteration 1458, loss = 5.64439407\n",
      "Iteration 1459, loss = 5.64557868\n",
      "Iteration 1460, loss = 5.65003526\n",
      "Iteration 1461, loss = 5.64264341\n",
      "Iteration 1462, loss = 5.64492140\n",
      "Iteration 1463, loss = 5.64548258\n",
      "Iteration 1464, loss = 5.64188730\n",
      "Iteration 1465, loss = 5.64887492\n",
      "Iteration 1466, loss = 5.64723545\n",
      "Iteration 1467, loss = 5.64344175\n",
      "Iteration 1468, loss = 5.65750345\n",
      "Iteration 1469, loss = 5.63854615\n",
      "Iteration 1470, loss = 5.64537449\n",
      "Iteration 1471, loss = 5.65245279\n",
      "Iteration 1472, loss = 5.64280222\n",
      "Iteration 1473, loss = 5.64718722\n",
      "Iteration 1474, loss = 5.65051494\n",
      "Iteration 1475, loss = 5.64253442\n",
      "Iteration 1476, loss = 5.66452975\n",
      "Iteration 1477, loss = 5.64893482\n",
      "Iteration 1478, loss = 5.64806232\n",
      "Iteration 1479, loss = 5.65389224\n",
      "Iteration 1480, loss = 5.64224018\n",
      "Iteration 1481, loss = 5.64343853\n",
      "Iteration 1482, loss = 5.64468771\n",
      "Iteration 1483, loss = 5.64796167\n",
      "Iteration 1484, loss = 5.65074668\n",
      "Iteration 1485, loss = 5.64245628\n",
      "Iteration 1486, loss = 5.64832514\n",
      "Iteration 1487, loss = 5.65541794\n",
      "Iteration 1488, loss = 5.64883251\n",
      "Iteration 1489, loss = 5.64462742\n",
      "Iteration 1490, loss = 5.64586258\n",
      "Iteration 1491, loss = 5.64931357\n",
      "Iteration 1492, loss = 5.65301226\n",
      "Iteration 1493, loss = 5.64495981\n",
      "Iteration 1494, loss = 5.64653373\n",
      "Iteration 1495, loss = 5.64169143\n",
      "Iteration 1496, loss = 5.64974131\n",
      "Iteration 1497, loss = 5.63906810\n",
      "Iteration 1498, loss = 5.64873890\n",
      "Iteration 1499, loss = 5.64326697\n",
      "Iteration 1500, loss = 5.64533511\n",
      "Iteration 1501, loss = 5.64539623\n",
      "Iteration 1502, loss = 5.65992280\n",
      "Iteration 1503, loss = 5.64639137\n",
      "Iteration 1504, loss = 5.65259348\n",
      "Iteration 1505, loss = 5.64120912\n",
      "Iteration 1506, loss = 5.64837330\n",
      "Iteration 1507, loss = 5.63959352\n",
      "Iteration 1508, loss = 5.64071259\n",
      "Iteration 1509, loss = 5.65058250\n",
      "Iteration 1510, loss = 5.63896545\n",
      "Iteration 1511, loss = 5.64836558\n",
      "Iteration 1512, loss = 5.64360507\n",
      "Iteration 1513, loss = 5.64555709\n",
      "Iteration 1514, loss = 5.64989789\n",
      "Iteration 1515, loss = 5.64508922\n",
      "Iteration 1516, loss = 5.64662834\n",
      "Iteration 1517, loss = 5.64675001\n",
      "Iteration 1518, loss = 5.65197538\n",
      "Iteration 1519, loss = 5.64226221\n",
      "Iteration 1520, loss = 5.65219447\n",
      "Iteration 1521, loss = 5.64896292\n",
      "Iteration 1522, loss = 5.65298963\n",
      "Iteration 1523, loss = 5.64438752\n",
      "Iteration 1524, loss = 5.65140303\n",
      "Iteration 1525, loss = 5.64878492\n",
      "Iteration 1526, loss = 5.64581763\n",
      "Iteration 1527, loss = 5.64572090\n",
      "Iteration 1528, loss = 5.65500666\n",
      "Iteration 1529, loss = 5.64685990\n",
      "Iteration 1530, loss = 5.64224914\n",
      "Iteration 1531, loss = 5.64327556\n",
      "Iteration 1532, loss = 5.64573720\n",
      "Iteration 1533, loss = 5.64262274\n",
      "Iteration 1534, loss = 5.64474124\n",
      "Iteration 1535, loss = 5.64627878\n",
      "Iteration 1536, loss = 5.63486762\n",
      "Iteration 1537, loss = 5.64932985\n",
      "Iteration 1538, loss = 5.65025963\n",
      "Iteration 1539, loss = 5.64423882\n",
      "Iteration 1540, loss = 5.64282268\n",
      "Iteration 1541, loss = 5.64865798\n",
      "Iteration 1542, loss = 5.63760402\n",
      "Iteration 1543, loss = 5.64859725\n",
      "Iteration 1544, loss = 5.63722283\n",
      "Iteration 1545, loss = 5.65301775\n",
      "Iteration 1546, loss = 5.65352728\n",
      "Iteration 1547, loss = 5.64735107\n",
      "Iteration 1548, loss = 5.64109779\n",
      "Iteration 1549, loss = 5.64291288\n",
      "Iteration 1550, loss = 5.65110468\n",
      "Iteration 1551, loss = 5.64258309\n",
      "Iteration 1552, loss = 5.63917338\n",
      "Iteration 1553, loss = 5.64844940\n",
      "Iteration 1554, loss = 5.65928486\n",
      "Iteration 1555, loss = 5.63483225\n",
      "Iteration 1556, loss = 5.64671040\n",
      "Iteration 1557, loss = 5.64875009\n",
      "Iteration 1558, loss = 5.64784635\n",
      "Iteration 1559, loss = 5.64871278\n",
      "Iteration 1560, loss = 5.64682298\n",
      "Iteration 1561, loss = 5.64473456\n",
      "Iteration 1562, loss = 5.64307146\n",
      "Iteration 1563, loss = 5.65477177\n",
      "Iteration 1564, loss = 5.64817053\n",
      "Iteration 1565, loss = 5.64502181\n",
      "Iteration 1566, loss = 5.64447059\n",
      "Iteration 1567, loss = 5.63723611\n",
      "Iteration 1568, loss = 5.64598093\n",
      "Iteration 1569, loss = 5.65034110\n",
      "Iteration 1570, loss = 5.64448800\n",
      "Iteration 1571, loss = 5.64397832\n",
      "Iteration 1572, loss = 5.65120231\n",
      "Iteration 1573, loss = 5.63936644\n",
      "Iteration 1574, loss = 5.63921462\n",
      "Iteration 1575, loss = 5.64136969\n",
      "Iteration 1576, loss = 5.64053689\n",
      "Iteration 1577, loss = 5.65412961\n",
      "Iteration 1578, loss = 5.65351269\n",
      "Iteration 1579, loss = 5.64355608\n",
      "Iteration 1580, loss = 5.64623418\n",
      "Iteration 1581, loss = 5.63350171\n",
      "Iteration 1582, loss = 5.63737955\n",
      "Iteration 1583, loss = 5.63960386\n",
      "Iteration 1584, loss = 5.64484521\n",
      "Iteration 1585, loss = 5.65081813\n",
      "Iteration 1586, loss = 5.62979629\n",
      "Iteration 1587, loss = 5.64140635\n",
      "Iteration 1588, loss = 5.65388069\n",
      "Iteration 1589, loss = 5.64693316\n",
      "Iteration 1590, loss = 5.64322532\n",
      "Iteration 1591, loss = 5.64323577\n",
      "Iteration 1592, loss = 5.64703287\n",
      "Iteration 1593, loss = 5.66205137\n",
      "Iteration 1594, loss = 5.64262909\n",
      "Iteration 1595, loss = 5.63293569\n",
      "Iteration 1596, loss = 5.63849172\n",
      "Iteration 1597, loss = 5.65349136\n",
      "Iteration 1598, loss = 5.63137481\n",
      "Iteration 1599, loss = 5.65223311\n",
      "Iteration 1600, loss = 5.63989399\n",
      "Iteration 1601, loss = 5.64304495\n",
      "Iteration 1602, loss = 5.64499461\n",
      "Iteration 1603, loss = 5.64006749\n",
      "Iteration 1604, loss = 5.64514955\n",
      "Iteration 1605, loss = 5.65036693\n",
      "Iteration 1606, loss = 5.65000641\n",
      "Iteration 1607, loss = 5.64942014\n",
      "Iteration 1608, loss = 5.64967126\n",
      "Iteration 1609, loss = 5.63402612\n",
      "Iteration 1610, loss = 5.63601337\n",
      "Iteration 1611, loss = 5.65036387\n",
      "Iteration 1612, loss = 5.64916901\n",
      "Iteration 1613, loss = 5.65571595\n",
      "Iteration 1614, loss = 5.63821537\n",
      "Iteration 1615, loss = 5.63896954\n",
      "Iteration 1616, loss = 5.64494253\n",
      "Iteration 1617, loss = 5.66075558\n",
      "Iteration 1618, loss = 5.64541312\n",
      "Iteration 1619, loss = 5.64160750\n",
      "Iteration 1620, loss = 5.65221548\n",
      "Iteration 1621, loss = 5.64949449\n",
      "Iteration 1622, loss = 5.64601559\n",
      "Iteration 1623, loss = 5.65303783\n",
      "Iteration 1624, loss = 5.64157594\n",
      "Iteration 1625, loss = 5.64060732\n",
      "Iteration 1626, loss = 5.65203482\n",
      "Iteration 1627, loss = 5.64808172\n",
      "Iteration 1628, loss = 5.64281053\n",
      "Iteration 1629, loss = 5.63078249\n",
      "Iteration 1630, loss = 5.64047993\n",
      "Iteration 1631, loss = 5.64373879\n",
      "Iteration 1632, loss = 5.64439774\n",
      "Iteration 1633, loss = 5.63914097\n",
      "Iteration 1634, loss = 5.64389049\n",
      "Iteration 1635, loss = 5.64652951\n",
      "Iteration 1636, loss = 5.64189105\n",
      "Iteration 1637, loss = 5.65050287\n",
      "Iteration 1638, loss = 5.64022886\n",
      "Iteration 1639, loss = 5.64591342\n",
      "Iteration 1640, loss = 5.65189326\n",
      "Iteration 1641, loss = 5.64423278\n",
      "Iteration 1642, loss = 5.65155422\n",
      "Iteration 1643, loss = 5.64139952\n",
      "Iteration 1644, loss = 5.64666436\n",
      "Iteration 1645, loss = 5.63761728\n",
      "Iteration 1646, loss = 5.63979778\n",
      "Training loss did not improve more than tol=0.000100 for 200 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-18 {color: black;background-color: white;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, hidden_layer_sizes=500,\n",
       "             learning_rate_init=0.01, max_iter=3000, n_iter_no_change=200,\n",
       "             random_state=42, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, hidden_layer_sizes=500,\n",
       "             learning_rate_init=0.01, max_iter=3000, n_iter_no_change=200,\n",
       "             random_state=42, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(activation='logistic', hidden_layer_sizes=500,\n",
       "             learning_rate_init=0.01, max_iter=3000, n_iter_no_change=200,\n",
       "             random_state=42, verbose=1)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(random_state=42,hidden_layer_sizes=(500),verbose=1, n_iter_no_change=200, max_iter=3000, learning_rate_init=0.01, activation=\"logistic\")\n",
    "mlp.fit(X_train_normalized, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "y_pred_eval = mlp.predict(eval_normalized)\n",
    "print(type(y_pred_eval))\n",
    "y_pred_rounded = round_to_nearest_5(y_pred_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_eval = pd.DataFrame(y_pred_eval, columns=['x', 'y'])\n",
    "y_pred_eval['Predicted'] = y_pred_eval['x'].astype(str) + \"|\" + y_pred_eval['y'].astype(str)\n",
    "y_pred_eval.drop(columns=['x','y'], inplace=True)\n",
    "y_pred_eval.reset_index(inplace=True)\n",
    "y_pred_eval.rename(columns={'index': 'Id'}, inplace=True)\n",
    "y_pred_eval = y_pred_eval[['Id', 'Predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_eval.to_csv('output.csv', columns=[\"Id\",\"Predicted\"], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
