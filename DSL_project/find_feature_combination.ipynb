{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import helpers.processing_helpers as ph\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_5(n):\n",
    "    return np.round(n / 5) * 5\n",
    "\n",
    "\n",
    "rounding_vectorized = np.vectorize(round_to_nearest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./development.csv\")\n",
    "\n",
    "noise_indexes = [0,7,12,15,16,17]\n",
    "\n",
    "\n",
    "\n",
    "features = ['pmax', 'negpmax', 'area', 'tmax', 'rms']\n",
    "\n",
    "drop_features = ['area', 'tmax', 'rms']\n",
    "\n",
    "df = df.drop(columns=ph.get_column_names(features, noise_indexes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_feature_reduction(df: pd.DataFrame):\n",
    "    \n",
    "    feature_to_loss = {}\n",
    "    drop_features_list = [('area',), ('tmax',), ('rms',), ('area', 'tmax'), ('area', 'rms'), ('rms', 'tmax'), ('area', 'tmax', 'rms')]\n",
    "    acc_idxs = [1,2,3,4,5,6,8,9,10,11,13,14]\n",
    "\n",
    "    for drop_features in drop_features_list:\n",
    "        \n",
    "        df_dev = df.drop(columns=ph.get_column_names(drop_features, acc_idxs))\n",
    "        print(df_dev.columns)\n",
    "        y_train_valid = df_dev[['x', 'y']].copy()\n",
    "\n",
    "        X_train_valid = df_dev.drop(columns=['x', 'y'])\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle=True)\n",
    "\n",
    "        means = X_train.mean()\n",
    "        stds = X_train.std()\n",
    "\n",
    "        X_train_normalized = (X_train - means) / stds\n",
    "\n",
    "        X_valid_normalized = (X_valid - means) / stds\n",
    "\n",
    "        mlp = MLPRegressor(random_state=42, verbose=1, n_iter_no_change=500, max_iter=100, learning_rate_init=0.01, activation=\"logistic\", learning_rate=\"adaptive\")\n",
    "        mlp.fit(X_train_normalized, y_train)\n",
    "\n",
    "        y_pred = mlp.predict(X_valid_normalized)\n",
    "\n",
    "        y_pred_rounded = round_to_nearest_5(y_pred)\n",
    "        result = (ph.mean_euclid_dist(y_valid, y_pred), ph.mean_euclid_dist(y_valid, y_pred_rounded))\n",
    "        feature_to_loss[drop_features] = result\n",
    "    return feature_to_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x', 'y', 'pmax[1]', 'negpmax[1]', 'tmax[1]', 'rms[1]', 'pmax[2]',\n",
      "       'negpmax[2]', 'tmax[2]', 'rms[2]', 'pmax[3]', 'negpmax[3]', 'tmax[3]',\n",
      "       'rms[3]', 'pmax[4]', 'negpmax[4]', 'tmax[4]', 'rms[4]', 'pmax[5]',\n",
      "       'negpmax[5]', 'tmax[5]', 'rms[5]', 'pmax[6]', 'negpmax[6]', 'tmax[6]',\n",
      "       'rms[6]', 'pmax[8]', 'negpmax[8]', 'tmax[8]', 'rms[8]', 'pmax[9]',\n",
      "       'negpmax[9]', 'tmax[9]', 'rms[9]', 'pmax[10]', 'negpmax[10]',\n",
      "       'tmax[10]', 'rms[10]', 'pmax[11]', 'negpmax[11]', 'tmax[11]', 'rms[11]',\n",
      "       'pmax[13]', 'negpmax[13]', 'tmax[13]', 'rms[13]', 'pmax[14]',\n",
      "       'negpmax[14]', 'tmax[14]', 'rms[14]'],\n",
      "      dtype='object')\n",
      "Iteration 1, loss = 13507.62061816\n",
      "Iteration 2, loss = 885.12672794\n",
      "Iteration 3, loss = 119.08186807\n",
      "Iteration 4, loss = 37.31128624\n",
      "Iteration 5, loss = 23.06834553\n",
      "Iteration 6, loss = 18.56599539\n",
      "Iteration 7, loss = 16.16446853\n",
      "Iteration 8, loss = 14.81654097\n",
      "Iteration 9, loss = 13.81522094\n",
      "Iteration 10, loss = 13.10786424\n",
      "Iteration 11, loss = 12.56004548\n",
      "Iteration 12, loss = 12.27256033\n",
      "Iteration 13, loss = 12.02000445\n",
      "Iteration 14, loss = 11.70955957\n",
      "Iteration 15, loss = 11.57578027\n",
      "Iteration 16, loss = 11.39076558\n",
      "Iteration 17, loss = 11.19598164\n",
      "Iteration 18, loss = 11.19801917\n",
      "Iteration 19, loss = 11.16545812\n",
      "Iteration 20, loss = 10.94481511\n",
      "Iteration 21, loss = 10.86023238\n",
      "Iteration 22, loss = 10.94258831\n",
      "Iteration 23, loss = 10.70606392\n",
      "Iteration 24, loss = 10.76406086\n",
      "Iteration 25, loss = 10.72227384\n",
      "Iteration 26, loss = 10.62588265\n",
      "Iteration 27, loss = 10.52709323\n",
      "Iteration 28, loss = 10.45782631\n",
      "Iteration 29, loss = 10.50914713\n",
      "Iteration 30, loss = 10.43467406\n",
      "Iteration 31, loss = 10.48973006\n",
      "Iteration 32, loss = 10.43346304\n",
      "Iteration 33, loss = 10.42673490\n",
      "Iteration 34, loss = 10.44674562\n",
      "Iteration 35, loss = 10.44121400\n",
      "Iteration 36, loss = 10.37702570\n",
      "Iteration 37, loss = 10.30561149\n",
      "Iteration 38, loss = 10.30378186\n",
      "Iteration 39, loss = 10.29247254\n",
      "Iteration 40, loss = 10.31061635\n",
      "Iteration 41, loss = 10.24149338\n",
      "Iteration 42, loss = 10.23288538\n",
      "Iteration 43, loss = 10.26462481\n",
      "Iteration 44, loss = 10.22687945\n",
      "Iteration 45, loss = 10.14412972\n",
      "Iteration 46, loss = 10.16094893\n",
      "Iteration 47, loss = 10.13046552\n",
      "Iteration 48, loss = 10.18309989\n",
      "Iteration 49, loss = 10.10941154\n",
      "Iteration 50, loss = 10.11671504\n",
      "Iteration 51, loss = 10.16846978\n",
      "Iteration 52, loss = 10.12786805\n",
      "Iteration 53, loss = 10.06286570\n",
      "Iteration 54, loss = 10.12868916\n",
      "Iteration 55, loss = 10.00527370\n",
      "Iteration 56, loss = 10.07500934\n",
      "Iteration 57, loss = 10.07392015\n",
      "Iteration 58, loss = 9.95591511\n",
      "Iteration 59, loss = 9.97293788\n",
      "Iteration 60, loss = 10.00757241\n",
      "Iteration 61, loss = 10.02989457\n",
      "Iteration 62, loss = 10.02821158\n",
      "Iteration 63, loss = 9.97117837\n",
      "Iteration 64, loss = 9.99857713\n",
      "Iteration 65, loss = 10.02842021\n",
      "Iteration 66, loss = 9.99652615\n",
      "Iteration 67, loss = 9.92718221\n",
      "Iteration 68, loss = 9.93012670\n",
      "Iteration 69, loss = 10.04907934\n",
      "Iteration 70, loss = 10.05163740\n",
      "Iteration 71, loss = 10.00085790\n",
      "Iteration 72, loss = 9.89911863\n",
      "Iteration 73, loss = 9.92835511\n",
      "Iteration 74, loss = 9.89733771\n",
      "Iteration 75, loss = 9.82948366\n",
      "Iteration 76, loss = 9.91381413\n",
      "Iteration 77, loss = 9.88411151\n",
      "Iteration 78, loss = 9.86382292\n",
      "Iteration 79, loss = 9.88972283\n",
      "Iteration 80, loss = 9.86989179\n",
      "Iteration 81, loss = 9.87031058\n",
      "Iteration 82, loss = 9.81330692\n",
      "Iteration 83, loss = 9.85471398\n",
      "Iteration 84, loss = 9.85725417\n",
      "Iteration 85, loss = 9.83711857\n",
      "Iteration 86, loss = 9.87976799\n",
      "Iteration 87, loss = 9.87668619\n",
      "Iteration 88, loss = 9.80483482\n",
      "Iteration 89, loss = 9.95569290\n",
      "Iteration 90, loss = 9.86418913\n",
      "Iteration 91, loss = 9.86079843\n",
      "Iteration 92, loss = 9.81054079\n",
      "Iteration 93, loss = 9.82377839\n",
      "Iteration 94, loss = 9.82442953\n",
      "Iteration 95, loss = 9.79865832\n",
      "Iteration 96, loss = 9.81975399\n",
      "Iteration 97, loss = 9.83835238\n",
      "Iteration 98, loss = 9.88999358\n",
      "Iteration 99, loss = 9.86364316\n",
      "Iteration 100, loss = 9.85408041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x', 'y', 'pmax[1]', 'negpmax[1]', 'area[1]', 'rms[1]', 'pmax[2]',\n",
      "       'negpmax[2]', 'area[2]', 'rms[2]', 'pmax[3]', 'negpmax[3]', 'area[3]',\n",
      "       'rms[3]', 'pmax[4]', 'negpmax[4]', 'area[4]', 'rms[4]', 'pmax[5]',\n",
      "       'negpmax[5]', 'area[5]', 'rms[5]', 'pmax[6]', 'negpmax[6]', 'area[6]',\n",
      "       'rms[6]', 'pmax[8]', 'negpmax[8]', 'area[8]', 'rms[8]', 'pmax[9]',\n",
      "       'negpmax[9]', 'area[9]', 'rms[9]', 'pmax[10]', 'negpmax[10]',\n",
      "       'area[10]', 'rms[10]', 'pmax[11]', 'negpmax[11]', 'area[11]', 'rms[11]',\n",
      "       'pmax[13]', 'negpmax[13]', 'area[13]', 'rms[13]', 'pmax[14]',\n",
      "       'negpmax[14]', 'area[14]', 'rms[14]'],\n",
      "      dtype='object')\n",
      "Iteration 1, loss = 13411.32971624\n",
      "Iteration 2, loss = 839.57247158\n",
      "Iteration 3, loss = 117.52817128\n",
      "Iteration 4, loss = 37.41240647\n",
      "Iteration 5, loss = 22.74099172\n",
      "Iteration 6, loss = 17.62524301\n",
      "Iteration 7, loss = 15.13833726\n",
      "Iteration 8, loss = 13.74925110\n",
      "Iteration 9, loss = 12.84028109\n",
      "Iteration 10, loss = 12.23641489\n",
      "Iteration 11, loss = 11.76882850\n",
      "Iteration 12, loss = 11.39848064\n",
      "Iteration 13, loss = 11.21301741\n",
      "Iteration 14, loss = 10.99263066\n",
      "Iteration 15, loss = 10.86790781\n",
      "Iteration 16, loss = 10.72539654\n",
      "Iteration 17, loss = 10.63692844\n",
      "Iteration 18, loss = 10.53411552\n",
      "Iteration 19, loss = 10.49364171\n",
      "Iteration 20, loss = 10.39872773\n",
      "Iteration 21, loss = 10.35771844\n",
      "Iteration 22, loss = 10.33365941\n",
      "Iteration 23, loss = 10.34320605\n",
      "Iteration 24, loss = 10.25088465\n",
      "Iteration 25, loss = 10.23257265\n",
      "Iteration 26, loss = 10.13850750\n",
      "Iteration 27, loss = 10.05061330\n",
      "Iteration 28, loss = 10.02835240\n",
      "Iteration 29, loss = 10.06417175\n",
      "Iteration 30, loss = 9.94917378\n",
      "Iteration 31, loss = 9.93239553\n",
      "Iteration 32, loss = 9.90536736\n",
      "Iteration 33, loss = 9.99205113\n",
      "Iteration 34, loss = 9.86631636\n",
      "Iteration 35, loss = 9.84404618\n",
      "Iteration 36, loss = 9.82969981\n",
      "Iteration 37, loss = 9.83857300\n",
      "Iteration 38, loss = 9.73701107\n",
      "Iteration 39, loss = 9.76991768\n",
      "Iteration 40, loss = 9.75945559\n",
      "Iteration 41, loss = 9.78534673\n",
      "Iteration 42, loss = 9.78508866\n",
      "Iteration 43, loss = 9.74627683\n",
      "Iteration 44, loss = 9.82206398\n",
      "Iteration 45, loss = 9.72322172\n",
      "Iteration 46, loss = 9.70484750\n",
      "Iteration 47, loss = 9.65057869\n",
      "Iteration 48, loss = 9.73899302\n",
      "Iteration 49, loss = 9.60824629\n",
      "Iteration 50, loss = 9.64255109\n",
      "Iteration 51, loss = 9.64402356\n",
      "Iteration 52, loss = 9.59868238\n",
      "Iteration 53, loss = 9.58504526\n",
      "Iteration 54, loss = 9.53994622\n",
      "Iteration 55, loss = 9.58169439\n",
      "Iteration 56, loss = 9.58267243\n",
      "Iteration 57, loss = 9.57239630\n",
      "Iteration 58, loss = 9.61555131\n",
      "Iteration 59, loss = 9.56831201\n",
      "Iteration 60, loss = 9.56754512\n",
      "Iteration 61, loss = 9.52413972\n",
      "Iteration 62, loss = 9.52393253\n",
      "Iteration 63, loss = 9.58469556\n",
      "Iteration 64, loss = 9.52178726\n",
      "Iteration 65, loss = 9.46638041\n",
      "Iteration 66, loss = 9.49079471\n",
      "Iteration 67, loss = 9.52189423\n",
      "Iteration 68, loss = 9.45669384\n",
      "Iteration 69, loss = 9.48802227\n",
      "Iteration 70, loss = 9.42274806\n",
      "Iteration 71, loss = 9.37553921\n",
      "Iteration 72, loss = 9.44275391\n",
      "Iteration 73, loss = 9.34723854\n",
      "Iteration 74, loss = 9.42468522\n",
      "Iteration 75, loss = 9.41914608\n",
      "Iteration 76, loss = 9.46413779\n",
      "Iteration 77, loss = 9.50491836\n",
      "Iteration 78, loss = 9.45332866\n",
      "Iteration 79, loss = 9.36961634\n",
      "Iteration 80, loss = 9.43325477\n",
      "Iteration 81, loss = 9.42717070\n",
      "Iteration 82, loss = 9.37401860\n",
      "Iteration 83, loss = 9.37391862\n",
      "Iteration 84, loss = 9.41166734\n",
      "Iteration 85, loss = 9.40717635\n",
      "Iteration 86, loss = 9.36796316\n",
      "Iteration 87, loss = 9.40241961\n",
      "Iteration 88, loss = 9.33563143\n",
      "Iteration 89, loss = 9.37748396\n",
      "Iteration 90, loss = 9.46698694\n",
      "Iteration 91, loss = 9.32933135\n",
      "Iteration 92, loss = 9.37315017\n",
      "Iteration 93, loss = 9.41868584\n",
      "Iteration 94, loss = 9.36470048\n",
      "Iteration 95, loss = 9.32445739\n",
      "Iteration 96, loss = 9.44230155\n",
      "Iteration 97, loss = 9.37985594\n",
      "Iteration 98, loss = 9.32365291\n",
      "Iteration 99, loss = 9.37889195\n",
      "Iteration 100, loss = 9.29239145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x', 'y', 'pmax[1]', 'negpmax[1]', 'area[1]', 'tmax[1]', 'pmax[2]',\n",
      "       'negpmax[2]', 'area[2]', 'tmax[2]', 'pmax[3]', 'negpmax[3]', 'area[3]',\n",
      "       'tmax[3]', 'pmax[4]', 'negpmax[4]', 'area[4]', 'tmax[4]', 'pmax[5]',\n",
      "       'negpmax[5]', 'area[5]', 'tmax[5]', 'pmax[6]', 'negpmax[6]', 'area[6]',\n",
      "       'tmax[6]', 'pmax[8]', 'negpmax[8]', 'area[8]', 'tmax[8]', 'pmax[9]',\n",
      "       'negpmax[9]', 'area[9]', 'tmax[9]', 'pmax[10]', 'negpmax[10]',\n",
      "       'area[10]', 'tmax[10]', 'pmax[11]', 'negpmax[11]', 'area[11]',\n",
      "       'tmax[11]', 'pmax[13]', 'negpmax[13]', 'area[13]', 'tmax[13]',\n",
      "       'pmax[14]', 'negpmax[14]', 'area[14]', 'tmax[14]'],\n",
      "      dtype='object')\n",
      "Iteration 1, loss = 13372.78021842\n",
      "Iteration 2, loss = 828.68824817\n",
      "Iteration 3, loss = 113.70713589\n",
      "Iteration 4, loss = 36.18780409\n",
      "Iteration 5, loss = 21.80480868\n",
      "Iteration 6, loss = 17.03874323\n",
      "Iteration 7, loss = 14.77282706\n",
      "Iteration 8, loss = 13.42454443\n",
      "Iteration 9, loss = 12.57347053\n",
      "Iteration 10, loss = 12.05659122\n",
      "Iteration 11, loss = 11.68649029\n",
      "Iteration 12, loss = 11.37580704\n",
      "Iteration 13, loss = 11.16623086\n",
      "Iteration 14, loss = 11.06517409\n",
      "Iteration 15, loss = 10.90152389\n",
      "Iteration 16, loss = 10.74160065\n",
      "Iteration 17, loss = 10.55202776\n",
      "Iteration 18, loss = 10.49413655\n",
      "Iteration 19, loss = 10.48139265\n",
      "Iteration 20, loss = 10.43249602\n",
      "Iteration 21, loss = 10.35268343\n",
      "Iteration 22, loss = 10.25804894\n",
      "Iteration 23, loss = 10.20938129\n",
      "Iteration 24, loss = 10.16435142\n",
      "Iteration 25, loss = 10.07048793\n",
      "Iteration 26, loss = 10.11005127\n",
      "Iteration 27, loss = 10.01453227\n",
      "Iteration 28, loss = 10.03793905\n",
      "Iteration 29, loss = 9.99769586\n",
      "Iteration 30, loss = 10.02149567\n",
      "Iteration 31, loss = 9.90567767\n",
      "Iteration 32, loss = 9.88954430\n",
      "Iteration 33, loss = 9.85958421\n",
      "Iteration 34, loss = 9.77489402\n",
      "Iteration 35, loss = 9.93885284\n",
      "Iteration 36, loss = 9.81452143\n",
      "Iteration 37, loss = 9.79569358\n",
      "Iteration 38, loss = 9.71807575\n",
      "Iteration 39, loss = 9.71040133\n",
      "Iteration 40, loss = 9.78133652\n",
      "Iteration 41, loss = 9.60345121\n",
      "Iteration 42, loss = 9.64864073\n",
      "Iteration 43, loss = 9.53304766\n",
      "Iteration 44, loss = 9.63467260\n",
      "Iteration 45, loss = 9.50802832\n",
      "Iteration 46, loss = 9.53212802\n",
      "Iteration 47, loss = 9.59041611\n",
      "Iteration 48, loss = 9.53372188\n",
      "Iteration 49, loss = 9.52338623\n",
      "Iteration 50, loss = 9.48474674\n",
      "Iteration 51, loss = 9.47068483\n",
      "Iteration 52, loss = 9.44193090\n",
      "Iteration 53, loss = 9.51160064\n",
      "Iteration 54, loss = 9.52286183\n",
      "Iteration 55, loss = 9.43415913\n",
      "Iteration 56, loss = 9.41200401\n",
      "Iteration 57, loss = 9.43042616\n",
      "Iteration 58, loss = 9.41192707\n",
      "Iteration 59, loss = 9.46335254\n",
      "Iteration 60, loss = 9.42216756\n",
      "Iteration 61, loss = 9.42621343\n",
      "Iteration 62, loss = 9.48225135\n",
      "Iteration 63, loss = 9.43219100\n",
      "Iteration 64, loss = 9.45608931\n",
      "Iteration 65, loss = 9.39607610\n",
      "Iteration 66, loss = 9.38205359\n",
      "Iteration 67, loss = 9.45652794\n",
      "Iteration 68, loss = 9.37039248\n",
      "Iteration 69, loss = 9.37577730\n",
      "Iteration 70, loss = 9.38331925\n",
      "Iteration 71, loss = 9.35336725\n",
      "Iteration 72, loss = 9.35142476\n",
      "Iteration 73, loss = 9.38910356\n",
      "Iteration 74, loss = 9.40522229\n",
      "Iteration 75, loss = 9.29672727\n",
      "Iteration 76, loss = 9.28157735\n",
      "Iteration 77, loss = 9.35326318\n",
      "Iteration 78, loss = 9.34528504\n",
      "Iteration 79, loss = 9.35943360\n",
      "Iteration 80, loss = 9.30700409\n",
      "Iteration 81, loss = 9.37300674\n",
      "Iteration 82, loss = 9.36398326\n",
      "Iteration 83, loss = 9.35452850\n",
      "Iteration 84, loss = 9.30208986\n",
      "Iteration 85, loss = 9.25933276\n",
      "Iteration 86, loss = 9.27318814\n",
      "Iteration 87, loss = 9.26411679\n",
      "Iteration 88, loss = 9.28026247\n",
      "Iteration 89, loss = 9.27773354\n",
      "Iteration 90, loss = 9.33833985\n",
      "Iteration 91, loss = 9.24904684\n",
      "Iteration 92, loss = 9.27835377\n",
      "Iteration 93, loss = 9.34006539\n",
      "Iteration 94, loss = 9.24476878\n",
      "Iteration 95, loss = 9.31629693\n",
      "Iteration 96, loss = 9.24032361\n",
      "Iteration 97, loss = 9.29433911\n",
      "Iteration 98, loss = 9.26302843\n",
      "Iteration 99, loss = 9.26215365\n",
      "Iteration 100, loss = 9.22489710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x', 'y', 'pmax[1]', 'negpmax[1]', 'rms[1]', 'pmax[2]', 'negpmax[2]',\n",
      "       'rms[2]', 'pmax[3]', 'negpmax[3]', 'rms[3]', 'pmax[4]', 'negpmax[4]',\n",
      "       'rms[4]', 'pmax[5]', 'negpmax[5]', 'rms[5]', 'pmax[6]', 'negpmax[6]',\n",
      "       'rms[6]', 'pmax[8]', 'negpmax[8]', 'rms[8]', 'pmax[9]', 'negpmax[9]',\n",
      "       'rms[9]', 'pmax[10]', 'negpmax[10]', 'rms[10]', 'pmax[11]',\n",
      "       'negpmax[11]', 'rms[11]', 'pmax[13]', 'negpmax[13]', 'rms[13]',\n",
      "       'pmax[14]', 'negpmax[14]', 'rms[14]'],\n",
      "      dtype='object')\n",
      "Iteration 1, loss = 13352.69549254\n",
      "Iteration 2, loss = 888.49911928\n",
      "Iteration 3, loss = 120.06250395\n",
      "Iteration 4, loss = 37.18674663\n",
      "Iteration 5, loss = 22.66463063\n",
      "Iteration 6, loss = 17.82851789\n",
      "Iteration 7, loss = 15.40173028\n",
      "Iteration 8, loss = 13.99850841\n",
      "Iteration 9, loss = 12.89236855\n",
      "Iteration 10, loss = 12.18897762\n",
      "Iteration 11, loss = 11.63695960\n",
      "Iteration 12, loss = 11.31123756\n",
      "Iteration 13, loss = 11.07388327\n",
      "Iteration 14, loss = 10.83587408\n",
      "Iteration 15, loss = 10.63831209\n",
      "Iteration 16, loss = 10.54091618\n",
      "Iteration 17, loss = 10.47377312\n",
      "Iteration 18, loss = 10.34356003\n",
      "Iteration 19, loss = 10.24506270\n",
      "Iteration 20, loss = 10.28489934\n",
      "Iteration 21, loss = 10.12958258\n",
      "Iteration 22, loss = 10.06114667\n",
      "Iteration 23, loss = 10.08938002\n",
      "Iteration 24, loss = 10.01540490\n",
      "Iteration 25, loss = 9.92777163\n",
      "Iteration 26, loss = 9.95944794\n",
      "Iteration 27, loss = 9.92484722\n",
      "Iteration 28, loss = 9.88038379\n",
      "Iteration 29, loss = 9.86211923\n",
      "Iteration 30, loss = 9.80934745\n",
      "Iteration 31, loss = 9.77819904\n",
      "Iteration 32, loss = 9.75671529\n",
      "Iteration 33, loss = 9.66694830\n",
      "Iteration 34, loss = 9.64467536\n",
      "Iteration 35, loss = 9.63662363\n",
      "Iteration 36, loss = 9.54978323\n",
      "Iteration 37, loss = 9.60006749\n",
      "Iteration 38, loss = 9.52159816\n",
      "Iteration 39, loss = 9.52052670\n",
      "Iteration 40, loss = 9.50439078\n",
      "Iteration 41, loss = 9.49252782\n",
      "Iteration 42, loss = 9.52724792\n",
      "Iteration 43, loss = 9.48715893\n",
      "Iteration 44, loss = 9.39377180\n",
      "Iteration 45, loss = 9.41314526\n",
      "Iteration 46, loss = 9.37413223\n",
      "Iteration 47, loss = 9.37151922\n",
      "Iteration 48, loss = 9.31040483\n",
      "Iteration 49, loss = 9.33283026\n",
      "Iteration 50, loss = 9.28781894\n",
      "Iteration 51, loss = 9.30151968\n",
      "Iteration 52, loss = 9.26767130\n",
      "Iteration 53, loss = 9.27406587\n",
      "Iteration 54, loss = 9.24974376\n",
      "Iteration 55, loss = 9.26841766\n",
      "Iteration 56, loss = 9.31710140\n",
      "Iteration 57, loss = 9.27331865\n",
      "Iteration 58, loss = 9.28747037\n",
      "Iteration 59, loss = 9.28247831\n",
      "Iteration 60, loss = 9.21359071\n",
      "Iteration 61, loss = 9.14037866\n",
      "Iteration 62, loss = 9.18838094\n",
      "Iteration 63, loss = 9.17583338\n",
      "Iteration 64, loss = 9.18486152\n",
      "Iteration 65, loss = 9.12496930\n",
      "Iteration 66, loss = 9.13437852\n",
      "Iteration 67, loss = 9.10981398\n",
      "Iteration 68, loss = 9.09128945\n",
      "Iteration 69, loss = 9.16518290\n",
      "Iteration 70, loss = 9.19532406\n",
      "Iteration 71, loss = 9.16127896\n",
      "Iteration 72, loss = 9.15203420\n",
      "Iteration 73, loss = 9.11056465\n",
      "Iteration 74, loss = 9.14315221\n",
      "Iteration 75, loss = 9.17646489\n",
      "Iteration 76, loss = 9.09982890\n",
      "Iteration 77, loss = 9.07286869\n",
      "Iteration 78, loss = 9.06424118\n",
      "Iteration 79, loss = 9.05228837\n",
      "Iteration 80, loss = 9.08931645\n",
      "Iteration 81, loss = 9.10544619\n",
      "Iteration 82, loss = 9.09442859\n",
      "Iteration 83, loss = 9.02679961\n",
      "Iteration 84, loss = 9.04537405\n",
      "Iteration 85, loss = 9.03647526\n",
      "Iteration 86, loss = 9.04722873\n",
      "Iteration 87, loss = 9.01709639\n",
      "Iteration 88, loss = 9.05741598\n",
      "Iteration 89, loss = 9.06678333\n",
      "Iteration 90, loss = 9.00870235\n",
      "Iteration 91, loss = 9.00680732\n",
      "Iteration 92, loss = 9.06341366\n",
      "Iteration 93, loss = 9.07398031\n",
      "Iteration 94, loss = 9.01584702\n",
      "Iteration 95, loss = 9.02437071\n",
      "Iteration 96, loss = 9.00568555\n",
      "Iteration 97, loss = 9.01533126\n",
      "Iteration 98, loss = 9.05009748\n",
      "Iteration 99, loss = 8.99807572\n",
      "Iteration 100, loss = 8.98370276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x', 'y', 'pmax[1]', 'negpmax[1]', 'tmax[1]', 'pmax[2]', 'negpmax[2]',\n",
      "       'tmax[2]', 'pmax[3]', 'negpmax[3]', 'tmax[3]', 'pmax[4]', 'negpmax[4]',\n",
      "       'tmax[4]', 'pmax[5]', 'negpmax[5]', 'tmax[5]', 'pmax[6]', 'negpmax[6]',\n",
      "       'tmax[6]', 'pmax[8]', 'negpmax[8]', 'tmax[8]', 'pmax[9]', 'negpmax[9]',\n",
      "       'tmax[9]', 'pmax[10]', 'negpmax[10]', 'tmax[10]', 'pmax[11]',\n",
      "       'negpmax[11]', 'tmax[11]', 'pmax[13]', 'negpmax[13]', 'tmax[13]',\n",
      "       'pmax[14]', 'negpmax[14]', 'tmax[14]'],\n",
      "      dtype='object')\n",
      "Iteration 1, loss = 13561.01783641\n",
      "Iteration 2, loss = 895.94042744\n",
      "Iteration 3, loss = 119.40604295\n",
      "Iteration 4, loss = 36.64331148\n",
      "Iteration 5, loss = 22.26149038\n",
      "Iteration 6, loss = 17.43506881\n",
      "Iteration 7, loss = 15.20112032\n",
      "Iteration 8, loss = 13.67042344\n",
      "Iteration 9, loss = 12.76109699\n",
      "Iteration 10, loss = 12.11696768\n",
      "Iteration 11, loss = 11.69149686\n",
      "Iteration 12, loss = 11.34395734\n",
      "Iteration 13, loss = 11.20549008\n",
      "Iteration 14, loss = 10.95486566\n",
      "Iteration 15, loss = 10.92350519\n",
      "Iteration 16, loss = 10.68149019\n",
      "Iteration 17, loss = 10.64034656\n",
      "Iteration 18, loss = 10.49946909\n",
      "Iteration 19, loss = 10.41822837\n",
      "Iteration 20, loss = 10.45488715\n",
      "Iteration 21, loss = 10.41334685\n",
      "Iteration 22, loss = 10.38965693\n",
      "Iteration 23, loss = 10.29873345\n",
      "Iteration 24, loss = 10.26975208\n",
      "Iteration 25, loss = 10.25925605\n",
      "Iteration 26, loss = 10.15561646\n",
      "Iteration 27, loss = 10.04266659\n",
      "Iteration 28, loss = 10.02115030\n",
      "Iteration 29, loss = 9.95059874\n",
      "Iteration 30, loss = 10.04151076\n",
      "Iteration 31, loss = 9.98597694\n",
      "Iteration 32, loss = 9.90345076\n",
      "Iteration 33, loss = 9.92604057\n",
      "Iteration 34, loss = 9.89068597\n",
      "Iteration 35, loss = 9.89331643\n",
      "Iteration 36, loss = 9.81251726\n",
      "Iteration 37, loss = 9.84058513\n",
      "Iteration 38, loss = 9.82758166\n",
      "Iteration 39, loss = 9.71644033\n",
      "Iteration 40, loss = 9.81738344\n",
      "Iteration 41, loss = 9.72662541\n",
      "Iteration 42, loss = 9.79040446\n",
      "Iteration 43, loss = 9.71600329\n",
      "Iteration 44, loss = 9.70896250\n",
      "Iteration 45, loss = 9.65984961\n",
      "Iteration 46, loss = 9.64281387\n",
      "Iteration 47, loss = 9.61930428\n",
      "Iteration 48, loss = 9.65279518\n",
      "Iteration 49, loss = 9.61254308\n",
      "Iteration 50, loss = 9.63755026\n",
      "Iteration 51, loss = 9.52247292\n",
      "Iteration 52, loss = 9.57145327\n",
      "Iteration 53, loss = 9.51250641\n",
      "Iteration 54, loss = 9.51177565\n",
      "Iteration 55, loss = 9.50742553\n",
      "Iteration 56, loss = 9.49217368\n",
      "Iteration 57, loss = 9.49514836\n",
      "Iteration 58, loss = 9.51215069\n",
      "Iteration 59, loss = 9.45430872\n",
      "Iteration 60, loss = 9.54844637\n",
      "Iteration 61, loss = 9.46754254\n",
      "Iteration 62, loss = 9.41983880\n",
      "Iteration 63, loss = 9.45346100\n",
      "Iteration 64, loss = 9.47986561\n",
      "Iteration 65, loss = 9.45254067\n",
      "Iteration 66, loss = 9.48499348\n",
      "Iteration 67, loss = 9.39920126\n",
      "Iteration 68, loss = 9.49951235\n",
      "Iteration 69, loss = 9.41253761\n",
      "Iteration 70, loss = 9.40277025\n",
      "Iteration 71, loss = 9.38882716\n",
      "Iteration 72, loss = 9.40995342\n",
      "Iteration 73, loss = 9.34974112\n",
      "Iteration 74, loss = 9.33388736\n",
      "Iteration 75, loss = 9.38045385\n",
      "Iteration 76, loss = 9.35597333\n",
      "Iteration 77, loss = 9.47384224\n",
      "Iteration 78, loss = 9.35392096\n",
      "Iteration 79, loss = 9.35801868\n",
      "Iteration 80, loss = 9.41145920\n",
      "Iteration 81, loss = 9.35139386\n",
      "Iteration 82, loss = 9.42705596\n",
      "Iteration 83, loss = 9.37061910\n",
      "Iteration 84, loss = 9.35609552\n",
      "Iteration 85, loss = 9.31088376\n",
      "Iteration 86, loss = 9.35508475\n",
      "Iteration 87, loss = 9.39931991\n",
      "Iteration 88, loss = 9.34102117\n",
      "Iteration 89, loss = 9.34162844\n",
      "Iteration 90, loss = 9.35461601\n",
      "Iteration 91, loss = 9.34308087\n",
      "Iteration 92, loss = 9.29590080\n",
      "Iteration 93, loss = 9.29282186\n",
      "Iteration 94, loss = 9.31318153\n",
      "Iteration 95, loss = 9.29249146\n",
      "Iteration 96, loss = 9.25573086\n",
      "Iteration 97, loss = 9.34458860\n",
      "Iteration 98, loss = 9.33865252\n",
      "Iteration 99, loss = 9.33927327\n",
      "Iteration 100, loss = 9.31984457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x', 'y', 'pmax[1]', 'negpmax[1]', 'area[1]', 'pmax[2]', 'negpmax[2]',\n",
      "       'area[2]', 'pmax[3]', 'negpmax[3]', 'area[3]', 'pmax[4]', 'negpmax[4]',\n",
      "       'area[4]', 'pmax[5]', 'negpmax[5]', 'area[5]', 'pmax[6]', 'negpmax[6]',\n",
      "       'area[6]', 'pmax[8]', 'negpmax[8]', 'area[8]', 'pmax[9]', 'negpmax[9]',\n",
      "       'area[9]', 'pmax[10]', 'negpmax[10]', 'area[10]', 'pmax[11]',\n",
      "       'negpmax[11]', 'area[11]', 'pmax[13]', 'negpmax[13]', 'area[13]',\n",
      "       'pmax[14]', 'negpmax[14]', 'area[14]'],\n",
      "      dtype='object')\n",
      "Iteration 1, loss = 13349.24901947\n",
      "Iteration 2, loss = 841.76775636\n",
      "Iteration 3, loss = 117.18889060\n",
      "Iteration 4, loss = 37.09317777\n",
      "Iteration 5, loss = 22.47312942\n",
      "Iteration 6, loss = 17.53559989\n",
      "Iteration 7, loss = 15.05256805\n",
      "Iteration 8, loss = 13.69614286\n",
      "Iteration 9, loss = 12.73823331\n",
      "Iteration 10, loss = 12.05641993\n",
      "Iteration 11, loss = 11.68913826\n",
      "Iteration 12, loss = 11.21905943\n",
      "Iteration 13, loss = 10.96438600\n",
      "Iteration 14, loss = 10.64894344\n",
      "Iteration 15, loss = 10.54850605\n",
      "Iteration 16, loss = 10.44176501\n",
      "Iteration 17, loss = 10.37735322\n",
      "Iteration 18, loss = 10.16795373\n",
      "Iteration 19, loss = 10.10932208\n",
      "Iteration 20, loss = 10.07602705\n",
      "Iteration 21, loss = 9.98026904\n",
      "Iteration 22, loss = 9.92415212\n",
      "Iteration 23, loss = 9.86409464\n",
      "Iteration 24, loss = 9.80227462\n",
      "Iteration 25, loss = 9.78905706\n",
      "Iteration 26, loss = 9.70136688\n",
      "Iteration 27, loss = 9.69934580\n",
      "Iteration 28, loss = 9.62019901\n",
      "Iteration 29, loss = 9.66681653\n",
      "Iteration 30, loss = 9.57879937\n",
      "Iteration 31, loss = 9.59501164\n",
      "Iteration 32, loss = 9.53657277\n",
      "Iteration 33, loss = 9.54704631\n",
      "Iteration 34, loss = 9.48601438\n",
      "Iteration 35, loss = 9.47324874\n",
      "Iteration 36, loss = 9.46918285\n",
      "Iteration 37, loss = 9.41075282\n",
      "Iteration 38, loss = 9.39273548\n",
      "Iteration 39, loss = 9.33377026\n",
      "Iteration 40, loss = 9.35523002\n",
      "Iteration 41, loss = 9.35184213\n",
      "Iteration 42, loss = 9.31233032\n",
      "Iteration 43, loss = 9.31040890\n",
      "Iteration 44, loss = 9.31337814\n",
      "Iteration 45, loss = 9.28476369\n",
      "Iteration 46, loss = 9.22937787\n",
      "Iteration 47, loss = 9.30758749\n",
      "Iteration 48, loss = 9.23080682\n",
      "Iteration 49, loss = 9.26957860\n",
      "Iteration 50, loss = 9.19454708\n",
      "Iteration 51, loss = 9.17662346\n",
      "Iteration 52, loss = 9.19440517\n",
      "Iteration 53, loss = 9.10035595\n",
      "Iteration 54, loss = 9.09258770\n",
      "Iteration 55, loss = 9.12649538\n",
      "Iteration 56, loss = 9.08195593\n",
      "Iteration 57, loss = 9.08068900\n",
      "Iteration 58, loss = 9.07729754\n",
      "Iteration 59, loss = 9.09674077\n",
      "Iteration 60, loss = 9.07473603\n",
      "Iteration 61, loss = 9.03230516\n",
      "Iteration 62, loss = 9.09594754\n",
      "Iteration 63, loss = 9.03209674\n",
      "Iteration 64, loss = 8.98917049\n",
      "Iteration 65, loss = 8.99569520\n",
      "Iteration 66, loss = 9.04910575\n",
      "Iteration 67, loss = 8.98572795\n",
      "Iteration 68, loss = 9.02285878\n",
      "Iteration 69, loss = 8.97293548\n",
      "Iteration 70, loss = 8.95726720\n",
      "Iteration 71, loss = 8.97459554\n",
      "Iteration 72, loss = 8.92210856\n",
      "Iteration 73, loss = 8.91280812\n",
      "Iteration 74, loss = 8.90617045\n",
      "Iteration 75, loss = 8.86923669\n",
      "Iteration 76, loss = 8.90279480\n",
      "Iteration 77, loss = 8.85211906\n",
      "Iteration 78, loss = 8.86135887\n",
      "Iteration 79, loss = 8.86777193\n",
      "Iteration 80, loss = 8.83285705\n",
      "Iteration 81, loss = 8.82494684\n",
      "Iteration 82, loss = 8.84929201\n",
      "Iteration 83, loss = 8.82414342\n",
      "Iteration 84, loss = 8.86380581\n",
      "Iteration 85, loss = 8.78570869\n",
      "Iteration 86, loss = 8.88175634\n",
      "Iteration 87, loss = 8.82537899\n",
      "Iteration 88, loss = 8.93217311\n",
      "Iteration 89, loss = 8.82660292\n",
      "Iteration 90, loss = 8.83234829\n",
      "Iteration 91, loss = 8.76953972\n",
      "Iteration 92, loss = 8.82939196\n",
      "Iteration 93, loss = 8.78049961\n",
      "Iteration 94, loss = 8.81552241\n",
      "Iteration 95, loss = 8.81426695\n",
      "Iteration 96, loss = 8.79221843\n",
      "Iteration 97, loss = 8.76908746\n",
      "Iteration 98, loss = 8.75016885\n",
      "Iteration 99, loss = 8.79850321\n",
      "Iteration 100, loss = 8.76744338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x', 'y', 'pmax[1]', 'negpmax[1]', 'pmax[2]', 'negpmax[2]', 'pmax[3]',\n",
      "       'negpmax[3]', 'pmax[4]', 'negpmax[4]', 'pmax[5]', 'negpmax[5]',\n",
      "       'pmax[6]', 'negpmax[6]', 'pmax[8]', 'negpmax[8]', 'pmax[9]',\n",
      "       'negpmax[9]', 'pmax[10]', 'negpmax[10]', 'pmax[11]', 'negpmax[11]',\n",
      "       'pmax[13]', 'negpmax[13]', 'pmax[14]', 'negpmax[14]'],\n",
      "      dtype='object')\n",
      "Iteration 1, loss = 13360.53858218\n",
      "Iteration 2, loss = 889.79243839\n",
      "Iteration 3, loss = 118.72699082\n",
      "Iteration 4, loss = 36.08456234\n",
      "Iteration 5, loss = 22.00425010\n",
      "Iteration 6, loss = 17.57088248\n",
      "Iteration 7, loss = 15.15619741\n",
      "Iteration 8, loss = 13.58161321\n",
      "Iteration 9, loss = 12.47983900\n",
      "Iteration 10, loss = 11.78310370\n",
      "Iteration 11, loss = 11.25664175\n",
      "Iteration 12, loss = 10.85100426\n",
      "Iteration 13, loss = 10.60091156\n",
      "Iteration 14, loss = 10.38468552\n",
      "Iteration 15, loss = 10.19388020\n",
      "Iteration 16, loss = 10.09303296\n",
      "Iteration 17, loss = 9.94501888\n",
      "Iteration 18, loss = 9.87897838\n",
      "Iteration 19, loss = 9.85612909\n",
      "Iteration 20, loss = 9.68163509\n",
      "Iteration 21, loss = 9.66361448\n",
      "Iteration 22, loss = 9.56473052\n",
      "Iteration 23, loss = 9.47236543\n",
      "Iteration 24, loss = 9.43330791\n",
      "Iteration 25, loss = 9.39778301\n",
      "Iteration 26, loss = 9.37813546\n",
      "Iteration 27, loss = 9.32639953\n",
      "Iteration 28, loss = 9.22917127\n",
      "Iteration 29, loss = 9.24389913\n",
      "Iteration 30, loss = 9.23243641\n",
      "Iteration 31, loss = 9.18672871\n",
      "Iteration 32, loss = 9.17358343\n",
      "Iteration 33, loss = 9.08620391\n",
      "Iteration 34, loss = 9.12951038\n",
      "Iteration 35, loss = 9.08210617\n",
      "Iteration 36, loss = 9.07117890\n",
      "Iteration 37, loss = 9.04971223\n",
      "Iteration 38, loss = 9.00666085\n",
      "Iteration 39, loss = 9.00425820\n",
      "Iteration 40, loss = 8.96473352\n",
      "Iteration 41, loss = 8.93119539\n",
      "Iteration 42, loss = 8.93641178\n",
      "Iteration 43, loss = 8.84565122\n",
      "Iteration 44, loss = 8.88085145\n",
      "Iteration 45, loss = 8.86047637\n",
      "Iteration 46, loss = 8.82089764\n",
      "Iteration 47, loss = 8.88359725\n",
      "Iteration 48, loss = 8.86746450\n",
      "Iteration 49, loss = 8.80745678\n",
      "Iteration 50, loss = 8.79828835\n",
      "Iteration 51, loss = 8.77023689\n",
      "Iteration 52, loss = 8.77054209\n",
      "Iteration 53, loss = 8.78959596\n",
      "Iteration 54, loss = 8.70157335\n",
      "Iteration 55, loss = 8.74571633\n",
      "Iteration 56, loss = 8.72418896\n",
      "Iteration 57, loss = 8.69017686\n",
      "Iteration 58, loss = 8.72783149\n",
      "Iteration 59, loss = 8.72123572\n",
      "Iteration 60, loss = 8.69137696\n",
      "Iteration 61, loss = 8.68584648\n",
      "Iteration 62, loss = 8.63143448\n",
      "Iteration 63, loss = 8.62997603\n",
      "Iteration 64, loss = 8.71386308\n",
      "Iteration 65, loss = 8.67976264\n",
      "Iteration 66, loss = 8.67697104\n",
      "Iteration 67, loss = 8.61883703\n",
      "Iteration 68, loss = 8.62668905\n",
      "Iteration 69, loss = 8.55779967\n",
      "Iteration 70, loss = 8.61993305\n",
      "Iteration 71, loss = 8.60686851\n",
      "Iteration 72, loss = 8.51488002\n",
      "Iteration 73, loss = 8.62709341\n",
      "Iteration 74, loss = 8.52317832\n",
      "Iteration 75, loss = 8.65030165\n",
      "Iteration 76, loss = 8.56380645\n",
      "Iteration 77, loss = 8.57681158\n",
      "Iteration 78, loss = 8.55630822\n",
      "Iteration 79, loss = 8.59066836\n",
      "Iteration 80, loss = 8.51243027\n",
      "Iteration 81, loss = 8.51362254\n",
      "Iteration 82, loss = 8.58107172\n",
      "Iteration 83, loss = 8.55127377\n",
      "Iteration 84, loss = 8.53921389\n",
      "Iteration 85, loss = 8.51843145\n",
      "Iteration 86, loss = 8.48178689\n",
      "Iteration 87, loss = 8.55537711\n",
      "Iteration 88, loss = 8.53924499\n",
      "Iteration 89, loss = 8.53137484\n",
      "Iteration 90, loss = 8.59925487\n",
      "Iteration 91, loss = 8.50073648\n",
      "Iteration 92, loss = 8.53167760\n",
      "Iteration 93, loss = 8.44355107\n",
      "Iteration 94, loss = 8.48670524\n",
      "Iteration 95, loss = 8.46646431\n",
      "Iteration 96, loss = 8.48930877\n",
      "Iteration 97, loss = 8.44854480\n",
      "Iteration 98, loss = 8.44651737\n",
      "Iteration 99, loss = 8.51763806\n",
      "Iteration 100, loss = 8.41399157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = find_best_feature_reduction(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('area',): (5.002756464700468, 5.005793604991764), ('tmax',): (4.870996941658324, 4.863394208045973), ('rms',): (4.864068984153766, 4.859901141493429), ('area', 'tmax'): (4.846800950401072, 4.842250908994254), ('area', 'rms'): (4.854868918870851, 4.842508324509863), ('rms', 'tmax'): (4.739657690458377, 4.71447859779251), ('area', 'tmax', 'rms'): (4.667231364614155, 4.649811609963127)}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
