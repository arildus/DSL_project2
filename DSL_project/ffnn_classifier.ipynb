{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import helpers.processing_helpers as ph\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(\"./dataset/development.csv\")\n",
    "\n",
    "acc_idxs = [1,2,3,4,5,6,8,9,10,11,13,14]\n",
    "noise_indexes = [0,7,12,15,16,17]\n",
    "\n",
    "\n",
    "\n",
    "features = ['pmax', 'negpmax', 'area', 'tmax', 'rms']\n",
    "\n",
    "drop_features = ['area', 'tmax', 'rms']\n",
    "\n",
    "df_dev = df_dev.drop(columns=ph.get_column_names(features, noise_indexes)) # Drop noisy sensors\n",
    "df_dev = df_dev.drop(columns=ph.get_column_names(drop_features, acc_idxs))\n",
    "\n",
    "\n",
    "# std_threshold = 5\n",
    "# means_by_point = df_dev.groupby(['x', 'y']).mean().reset_index()\n",
    "# duplicate_means = means_by_point.loc[means_by_point.index.repeat(100)].reset_index(drop=True)\n",
    "# stds_by_point = df_dev.groupby(['x', 'y']).std().reset_index()\n",
    "# duplcate_stds = stds_by_point.loc[stds_by_point.index.repeat(100)].reset_index(drop=True)\n",
    "# df_dev = df_dev[(np.abs((df_dev-duplicate_means) / duplcate_stds) < std_threshold).all(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for value in acc_idxs:\n",
    "#     feature = \"diff[\" + str(value) + \"]\"\n",
    "#     pmax = \"pmax[\" + str(value) + \"]\"\n",
    "#     negpmax = \"negpmax[\" + str(value) + \"]\"\n",
    "#     df_dev[feature] = df_dev[pmax] - df_dev[negpmax]\n",
    "\n",
    "\n",
    "y_train_valid = df_dev[['x', 'y']].copy()\n",
    "\n",
    "X_train_valid = df_dev.drop(columns=['x', 'y'])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle=True)\n",
    "\n",
    "X = pd.concat([y_train, X_train], axis=1)\n",
    "\n",
    "means = X_train.mean()\n",
    "stds = X_train.std()\n",
    "\n",
    "# std_threshold = 10\n",
    "\n",
    "# means_by_point = X.copy().groupby(['x', 'y']).mean().reset_index()\n",
    "# duplicate_means = means_by_point.loc[means_by_point.index.repeat(100)].reset_index(drop=True)\n",
    "# stds_by_point = X.copy().groupby(['x', 'y']).std().reset_index()\n",
    "# duplcate_stds = stds_by_point.loc[stds_by_point.index.repeat(100)].reset_index(drop=True)\n",
    "# X = X[(np.abs((X-duplicate_means) / duplcate_stds) < std_threshold).all(axis=1)]\n",
    "\n",
    "# X_train = X.drop(columns=['x', 'y'])\n",
    "# y_train = X[['x', 'y']]\n",
    "\n",
    "\n",
    "\n",
    "X_train_normalized = (X_train - means) / stds\n",
    "\n",
    "X_valid_normalized = (X_valid - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ev = pd.read_csv(\"./evaluation.csv\")\n",
    "# features = [\"pmax\", \"negpmax\", 'area', 'tmax', 'rms']\n",
    "\n",
    "# eval_noise_removed = df_ev.drop(columns=ph.get_column_names(features, noise_indexes))\n",
    "# eval_noise_removed = eval_noise_removed.drop(columns=ph.get_column_names(drop_features, acc_idxs))\n",
    "\n",
    "# indices = eval_noise_removed[\"Id\"].copy()\n",
    "# eval_noise_removed = eval_noise_removed.drop(columns = [\"Id\"])\n",
    "# eval_normalized = (eval_noise_removed - means) / stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11381.05708185\n",
      "Iteration 2, loss = 269.95359636\n",
      "Iteration 3, loss = 47.79581344\n",
      "Iteration 4, loss = 26.06469339\n",
      "Iteration 5, loss = 19.92670762\n",
      "Iteration 6, loss = 16.99513227\n",
      "Iteration 7, loss = 15.65029260\n",
      "Iteration 8, loss = 14.77555107\n",
      "Iteration 9, loss = 14.57754484\n",
      "Iteration 10, loss = 14.38167706\n",
      "Iteration 11, loss = 13.83963165\n",
      "Iteration 12, loss = 13.83780611\n",
      "Iteration 13, loss = 13.97218472\n",
      "Iteration 14, loss = 13.85418108\n",
      "Iteration 15, loss = 13.57346706\n",
      "Iteration 16, loss = 13.79694479\n",
      "Iteration 17, loss = 13.61343305\n",
      "Iteration 18, loss = 13.66453007\n",
      "Iteration 19, loss = 13.60292126\n",
      "Iteration 20, loss = 13.52434478\n",
      "Iteration 21, loss = 13.53361616\n",
      "Iteration 22, loss = 13.37433156\n",
      "Iteration 23, loss = 13.44725974\n",
      "Iteration 24, loss = 13.55721935\n",
      "Iteration 25, loss = 13.42442527\n",
      "Iteration 26, loss = 13.37563519\n",
      "Iteration 27, loss = 13.34408814\n",
      "Iteration 28, loss = 13.09822705\n",
      "Iteration 29, loss = 13.19885965\n",
      "Iteration 30, loss = 13.50653660\n",
      "Iteration 31, loss = 13.24671738\n",
      "Iteration 32, loss = 13.24546997\n",
      "Iteration 33, loss = 12.98227070\n",
      "Iteration 34, loss = 12.91693165\n",
      "Iteration 35, loss = 13.10186599\n",
      "Iteration 36, loss = 13.16015733\n",
      "Iteration 37, loss = 12.98937939\n",
      "Iteration 38, loss = 13.04764193\n",
      "Iteration 39, loss = 12.75182135\n",
      "Iteration 40, loss = 12.63533207\n",
      "Iteration 41, loss = 12.58203011\n",
      "Iteration 42, loss = 12.45246880\n",
      "Iteration 43, loss = 12.62643572\n",
      "Iteration 44, loss = 12.69510318\n",
      "Iteration 45, loss = 12.52461721\n",
      "Iteration 46, loss = 12.48215258\n",
      "Iteration 47, loss = 12.60416114\n",
      "Iteration 48, loss = 12.24482493\n",
      "Iteration 49, loss = 12.25403501\n",
      "Iteration 50, loss = 12.13329049\n",
      "Iteration 51, loss = 12.10727282\n",
      "Iteration 52, loss = 12.12951842\n",
      "Iteration 53, loss = 12.16540683\n",
      "Iteration 54, loss = 11.92205832\n",
      "Iteration 55, loss = 12.03837470\n",
      "Iteration 56, loss = 11.76834222\n",
      "Iteration 57, loss = 11.63331877\n",
      "Iteration 58, loss = 11.68713301\n",
      "Iteration 59, loss = 11.73018751\n",
      "Iteration 60, loss = 11.43016371\n",
      "Iteration 61, loss = 11.73881332\n",
      "Iteration 62, loss = 11.70196420\n",
      "Iteration 63, loss = 11.84460969\n",
      "Iteration 64, loss = 11.53924764\n",
      "Iteration 65, loss = 11.70106250\n",
      "Iteration 66, loss = 12.10948880\n",
      "Iteration 67, loss = 12.03764905\n",
      "Iteration 68, loss = 12.11677744\n",
      "Iteration 69, loss = 12.16830219\n",
      "Iteration 70, loss = 12.06526263\n",
      "Iteration 71, loss = 11.71070247\n",
      "Iteration 72, loss = 11.47054584\n",
      "Iteration 73, loss = 11.55741541\n",
      "Iteration 74, loss = 11.42403796\n",
      "Iteration 75, loss = 11.39354825\n",
      "Iteration 76, loss = 11.32310537\n",
      "Iteration 77, loss = 11.30056740\n",
      "Iteration 78, loss = 11.23569019\n",
      "Iteration 79, loss = 11.34936293\n",
      "Iteration 80, loss = 11.38495972\n",
      "Iteration 81, loss = 11.40149523\n",
      "Iteration 82, loss = 11.32636805\n",
      "Iteration 83, loss = 11.15036777\n",
      "Iteration 84, loss = 10.99794177\n",
      "Iteration 85, loss = 11.36111238\n",
      "Iteration 86, loss = 11.42743779\n",
      "Iteration 87, loss = 11.41832550\n",
      "Iteration 88, loss = 11.55733934\n",
      "Iteration 89, loss = 11.67597193\n",
      "Iteration 90, loss = 11.47951606\n",
      "Iteration 91, loss = 11.65795467\n",
      "Iteration 92, loss = 11.38994300\n",
      "Iteration 93, loss = 11.56322321\n",
      "Iteration 94, loss = 11.51255941\n",
      "Iteration 95, loss = 11.14485563\n",
      "Iteration 96, loss = 11.19438889\n",
      "Iteration 97, loss = 11.22012760\n",
      "Iteration 98, loss = 10.98418516\n",
      "Iteration 99, loss = 11.15301254\n",
      "Iteration 100, loss = 10.83750836\n",
      "Iteration 101, loss = 11.08487095\n",
      "Iteration 102, loss = 11.15540115\n",
      "Iteration 103, loss = 11.18266715\n",
      "Iteration 104, loss = 11.04019054\n",
      "Iteration 105, loss = 11.28027430\n",
      "Iteration 106, loss = 10.78020966\n",
      "Iteration 107, loss = 10.73631220\n",
      "Iteration 108, loss = 10.74868079\n",
      "Iteration 109, loss = 10.66108699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(activation=&#x27;tanh&#x27;, learning_rate=&#x27;adaptive&#x27;,\n",
       "             learning_rate_init=0.01, n_iter_no_change=500, random_state=42,\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(activation=&#x27;tanh&#x27;, learning_rate=&#x27;adaptive&#x27;,\n",
       "             learning_rate_init=0.01, n_iter_no_change=500, random_state=42,\n",
       "             verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(activation='tanh', learning_rate='adaptive',\n",
       "             learning_rate_init=0.01, n_iter_no_change=500, random_state=42,\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(random_state=42, verbose=1, n_iter_no_change=500, max_iter=200, learning_rate_init=0.01, activation=\"logisitc\", learning_rate=\"adaptive\")\n",
    "mlp.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmlp\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_valid_normalized)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(ph\u001b[38;5;241m.\u001b[39mmean_euclid_dist(y_valid, y_pred))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlp' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = mlp.predict(X_valid_normalized)\n",
    "print(ph.mean_euclid_dist(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_5(n):\n",
    "    return np.round(n / 5) * 5\n",
    "\n",
    "\n",
    "rounding_vectorized = np.vectorize(round_to_nearest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rounded = round_to_nearest_5(y_pred)\n",
    "print(ph.mean_euclid_dist(y_valid, y_pred_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_run = 4.645704973901679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_eval_tms_rms_area = (eval_noise_removed - means) / stds\n",
    "y_pred_eval = mlp.predict(normalized_eval_tms_rms_area)\n",
    "\n",
    "\n",
    "f = open(\"predictions/no_outliers_tmax_rms_area.csv\", 'w')\n",
    "f.write(\"Id,Predicted\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "for ind in range(y_pred_eval.shape[0]):\n",
    "    line = str(indices[ind]) + \",\" + str(y_pred_eval[ind,0]) + \"|\" + str(y_pred_eval[ind,1])\n",
    "    f.write(line)\n",
    "    f.write(\"\\n\")\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_idxs = (1,2,3,4,5,6,8,9,10,11,13,14)\n",
    "# X_train_normalized =  X_train_normalized.drop(columns=ph.get_column_names(['tmax', 'rms'], acc_idxs))\n",
    "# X_valid_normalized = X_valid_normalized.drop(columns=ph.get_column_names(['tmax', 'rms'], acc_idxs))\n",
    "# print(X_train_normalized.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_tmax_rms_dropped = MLPRegressor(random_state=42, verbose=1, n_iter_no_change=500, max_iter=1500, learning_rate_init=0.001, activation=\"logistic\")\n",
    "# mlp_tmax_rms_dropped.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_tmax_rms_dropped = mlp_tmax_rms_dropped.predict(X_valid_normalized)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(ph.mean_euclid_dist(y_valid, y_pred_tmax_rms_dropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_eval_area = (eval_noise_removed - means) / stds\n",
    "\n",
    "# print(eval_noise_removed.columns)\n",
    "\n",
    "# normalized_eval_area =  eval_noise_removed.drop(columns=ph.get_column_names(['tmax', 'rms'], acc_idxs))\n",
    "\n",
    "# y_pred_eval = mlp_tmax_rms_dropped.predict(normalized_eval_area)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# f = open(\"predictions/no_outliers_area_.csv\", 'w')\n",
    "# f.write(\"Id,Predicted\")\n",
    "# f.write(\"\\n\")\n",
    "\n",
    "# for ind in range(y_pred_eval.shape[0]):\n",
    "#     line = str(indices[ind]) + \",\" + str(y_pred_eval[ind,0]) + \"|\" + str(y_pred_eval[ind,1])\n",
    "#     f.write(line)\n",
    "#     f.write(\"\\n\")\n",
    "# print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_idxs = (1,2,3,4,5,6,8,9,10,11,13,14)\n",
    "# X_train_normalized =  X_train_normalized.drop(columns=ph.get_column_names(['area'], acc_idxs))\n",
    "# X_valid_normalized = X_valid_normalized.drop(columns=ph.get_column_names(['area'], acc_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_tmax_rms_area_dropped = MLPRegressor(random_state=42, verbose=1, n_iter_no_change=500, max_iter=1500, learning_rate_init=0.001, activation=\"logistic\")\n",
    "# mlp_tmax_rms_area_dropped.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_tmax_rms_area_dropped = mlp_tmax_rms_area_dropped.predict(X_valid_normalized)\n",
    "# print(ph.mean_euclid_dist(y_valid, y_pred_tmax_rms_dropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_eval_area = (eval_noise_removed - means) / stds\n",
    "\n",
    "# print(eval_noise_removed.columns)\n",
    "\n",
    "# normalized_eval_area =  eval_noise_removed.drop(columns=ph.get_column_names(['tmax', 'rms', 'area'], acc_idxs))\n",
    "\n",
    "# y_pred_eval = mlp_tmax_rms_area_dropped.predict(normalized_eval_area)\n",
    "\n",
    "# f = open(\"predictions/no_outliers.csv\", 'w')\n",
    "# f.write(\"Id,Predicted\")\n",
    "# f.write(\"\\n\")\n",
    "\n",
    "# for ind in range(y_pred_eval.shape[0]):\n",
    "#     line = str(indices[ind]) + \",\" + str(y_pred_eval[ind,0]) + \"|\" + str(y_pred_eval[ind,1])\n",
    "#     f.write(line)\n",
    "#     f.write(\"\\n\")\n",
    "# print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_valid = pmax_negpmax_area[['x', 'y']].copy()\n",
    "\n",
    "# X_train_valid = pmax_negpmax_area.drop(columns=['x', 'y'])\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_valid = pmax_negpmax_area[['x', 'y']].copy()\n",
    "\n",
    "# area_idx = (1,2,3,4,5,6,8,9,10,11,13,14)\n",
    "# X_train_valid = pmax_negpmax_area.drop(columns=['x', 'y']).drop(columns=ph.get_column_names(['area'], area_idx))\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle=True, random_state=42, train_size=0.9)\n",
    "\n",
    "# print(X_valid.shape[0] / (X_train.shape[0] + X_valid.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = X_train.mean()\n",
    "# stds = X_train.std()\n",
    "# X_train_normalized = (X_train - means) / stds\n",
    "\n",
    "# print(X_train_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = MLPRegressor(random_state=42, verbose=1, n_iter_no_change=500, max_iter=1500, learning_rate_init=0.0001, activation=\"logistic\")\n",
    "# mlp.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_valid_normalized = (X_valid - means) / stds\n",
    "# y_hat = mlp.predict(X_valid_normalized)\n",
    "# mlp_med = ph.mean_euclid_dist(y_valid, y_hat)\n",
    "# print(mlp_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_5(n):\n",
    "    return np.round(n / 5) * 5\n",
    "\n",
    "\n",
    "rounding_vectorized = np.vectorize(round_to_nearest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ph.mean_euclid_dist(y_valid, y_pred))\n",
    "# print(ph.mean_euclid_dist(y_valid, y_pred_tmax_rms_dropped))\n",
    "# print(ph.mean_euclid_dist(y_valid, y_pred_tmax_rms_area_dropped))\n",
    "# print(ph.mean_euclid_dist(y_valid, round_to_nearest_5(y_pred)))\n",
    "# print(ph.mean_euclid_dist(y_valid, round_to_nearest_5(y_pred_tmax_rms_dropped)))\n",
    "# print(ph.mean_euclid_dist(y_valid, round_to_nearest_5(y_pred_tmax_rms_area_dropped)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ev = pd.read_csv(\"./evaluation.csv\")\n",
    "\n",
    "# print(df_ev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_indexes = [0,7,12,15,16,17]\n",
    "# features = [\"pmax\", \"negpmax\", 'area', 'tmax', 'rms']\n",
    "\n",
    "# eval_noise_removed = df_ev.drop(columns=ph.get_column_names(features, noise_indexes))\n",
    "\n",
    "# print(eval_noise_removed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = eval_noise_removed[\"Id\"].copy()\n",
    "# eval_noise_removed = eval_noise_removed.drop(columns = [\"Id\"])\n",
    "# print(eval_noise_removed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_idxs = [1,2,3,4,5,6,8,9,10,11,13,14]\n",
    "# eval_noise_removed = eval_noise_removed.drop(columns=ph.get_column_names(['tmax', 'rms'], acc_idxs))\n",
    "# print(eval_noise_removed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_eval = (eval_noise_removed - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_eval = mlp_tmax_rms_dropped.predict(normalized_eval)\n",
    "# print(y_pred_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"predictions/no_outliers_tmax_rms_area.csv\", 'w')\n",
    "# f.write(\"Id,Predicted\")\n",
    "# f.write(\"\\n\")\n",
    "\n",
    "# for ind in range(y_pred_eval.shape[0]):\n",
    "#     line = str(indices[ind]) + \",\" + str(y_pred_eval[ind,0]) + \"|\" + str(y_pred_eval[ind,1])\n",
    "#     f.write(line)\n",
    "#     f.write(\"\\n\")\n",
    "# print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "# rf.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_valid = pmax_negpmax_area[['x', 'y']].copy()\n",
    "\n",
    "# X_train_valid = pmax_negpmax_area.drop(columns=['x', 'y'])\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle=True, random_state=42)\n",
    "\n",
    "# noise_indexes = [0,7,12,15,16,17]\n",
    "# features = [\"pmax\", \"negpmax\", 'area', 'tmax', 'rms']\n",
    "\n",
    "# noise_removed = subset.drop(columns=ph.get_column_names(features, noise_indexes))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means_outliers_removed = X_train_outliers_removed.mean()\n",
    "# stds_outliers_removed = X_train_outliers_removed.std()\n",
    "# X_train_normalized = (X_train_outliers_removed - means) / stds\n",
    "\n",
    "# print(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = MLPRegressor(random_state=42, verbose=1, n_iter_no_change=500, max_iter=1500, learning_rate_init=0.001, activation=\"logistic\")\n",
    "# mlp.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat_outliers_removed = mlp.predict(X_valid_normalized)\n",
    "# print(ph.mean_euclid_dist(y_valid, y_hat_outliers_removed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
