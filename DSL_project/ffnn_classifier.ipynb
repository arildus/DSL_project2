{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import helpers.processing_helpers as ph\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(\"./development.csv\")\n",
    "\n",
    "acc_idxs = [1,2,3,4,5,6,8,9,10,11,13,14]\n",
    "noise_indexes = [0,7,12,15,16,17]\n",
    "\n",
    "\n",
    "\n",
    "features = ['pmax', 'negpmax', 'area', 'tmax', 'rms']\n",
    "\n",
    "drop_features = ['area', 'tmax', 'rms']\n",
    "\n",
    "df_dev = df_dev.drop(columns=ph.get_column_names(features, noise_indexes)) # Drop noisy sensors\n",
    "df_dev = df_dev.drop(columns=ph.get_column_names(drop_features, acc_idxs))\n",
    "\n",
    "\n",
    "# std_threshold = 5\n",
    "# means_by_point = df_dev.groupby(['x', 'y']).mean().reset_index()\n",
    "# duplicate_means = means_by_point.loc[means_by_point.index.repeat(100)].reset_index(drop=True)\n",
    "# stds_by_point = df_dev.groupby(['x', 'y']).std().reset_index()\n",
    "# duplcate_stds = stds_by_point.loc[stds_by_point.index.repeat(100)].reset_index(drop=True)\n",
    "# df_dev = df_dev[(np.abs((df_dev-duplicate_means) / duplcate_stds) < std_threshold).all(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for value in acc_idxs:\n",
    "#     feature = \"diff[\" + str(value) + \"]\"\n",
    "#     pmax = \"pmax[\" + str(value) + \"]\"\n",
    "#     negpmax = \"negpmax[\" + str(value) + \"]\"\n",
    "#     df_dev[feature] = df_dev[pmax] - df_dev[negpmax]\n",
    "\n",
    "\n",
    "y_train_valid = df_dev[['x', 'y']].copy()\n",
    "\n",
    "X_train_valid = df_dev.drop(columns=['x', 'y'])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle=True)\n",
    "\n",
    "X = pd.concat([y_train, X_train], axis=1)\n",
    "\n",
    "means = X_train.mean()\n",
    "stds = X_train.std()\n",
    "\n",
    "# std_threshold = 10\n",
    "\n",
    "# means_by_point = X.copy().groupby(['x', 'y']).mean().reset_index()\n",
    "# duplicate_means = means_by_point.loc[means_by_point.index.repeat(100)].reset_index(drop=True)\n",
    "# stds_by_point = X.copy().groupby(['x', 'y']).std().reset_index()\n",
    "# duplcate_stds = stds_by_point.loc[stds_by_point.index.repeat(100)].reset_index(drop=True)\n",
    "# X = X[(np.abs((X-duplicate_means) / duplcate_stds) < std_threshold).all(axis=1)]\n",
    "\n",
    "# X_train = X.drop(columns=['x', 'y'])\n",
    "# y_train = X[['x', 'y']]\n",
    "\n",
    "\n",
    "\n",
    "X_train_normalized = (X_train - means) / stds\n",
    "\n",
    "X_valid_normalized = (X_valid - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ev = pd.read_csv(\"./evaluation.csv\")\n",
    "features = [\"pmax\", \"negpmax\", 'area', 'tmax', 'rms']\n",
    "\n",
    "eval_noise_removed = df_ev.drop(columns=ph.get_column_names(features, noise_indexes))\n",
    "eval_noise_removed = eval_noise_removed.drop(columns=ph.get_column_names(drop_features, acc_idxs))\n",
    "\n",
    "indices = eval_noise_removed[\"Id\"].copy()\n",
    "eval_noise_removed = eval_noise_removed.drop(columns = [\"Id\"])\n",
    "eval_normalized = (eval_noise_removed - means) / stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(random_state=42, verbose=1, n_iter_no_change=500, max_iter=200, learning_rate_init=0.01, activation=\"logistic\", learning_rate=\"adaptive\")\n",
    "mlp.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_valid_normalized)\n",
    "print(ph.mean_euclid_dist(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_5(n):\n",
    "    return np.round(n / 5) * 5\n",
    "\n",
    "\n",
    "rounding_vectorized = np.vectorize(round_to_nearest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rounded = round_to_nearest_5(y_pred)\n",
    "print(ph.mean_euclid_dist(y_valid, y_pred_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_run = 4.645704973901679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_eval_tms_rms_area = (eval_noise_removed - means) / stds\n",
    "y_pred_eval = mlp.predict(normalized_eval_tms_rms_area)\n",
    "\n",
    "\n",
    "f = open(\"predictions/no_outliers_tmax_rms_area.csv\", 'w')\n",
    "f.write(\"Id,Predicted\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "for ind in range(y_pred_eval.shape[0]):\n",
    "    line = str(indices[ind]) + \",\" + str(y_pred_eval[ind,0]) + \"|\" + str(y_pred_eval[ind,1])\n",
    "    f.write(line)\n",
    "    f.write(\"\\n\")\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_idxs = (1,2,3,4,5,6,8,9,10,11,13,14)\n",
    "# X_train_normalized =  X_train_normalized.drop(columns=ph.get_column_names(['tmax', 'rms'], acc_idxs))\n",
    "# X_valid_normalized = X_valid_normalized.drop(columns=ph.get_column_names(['tmax', 'rms'], acc_idxs))\n",
    "# print(X_train_normalized.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_tmax_rms_dropped = MLPRegressor(random_state=42, verbose=1, n_iter_no_change=500, max_iter=1500, learning_rate_init=0.001, activation=\"logistic\")\n",
    "# mlp_tmax_rms_dropped.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_tmax_rms_dropped = mlp_tmax_rms_dropped.predict(X_valid_normalized)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(ph.mean_euclid_dist(y_valid, y_pred_tmax_rms_dropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_eval_area = (eval_noise_removed - means) / stds\n",
    "\n",
    "# print(eval_noise_removed.columns)\n",
    "\n",
    "# normalized_eval_area =  eval_noise_removed.drop(columns=ph.get_column_names(['tmax', 'rms'], acc_idxs))\n",
    "\n",
    "# y_pred_eval = mlp_tmax_rms_dropped.predict(normalized_eval_area)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# f = open(\"predictions/no_outliers_area_.csv\", 'w')\n",
    "# f.write(\"Id,Predicted\")\n",
    "# f.write(\"\\n\")\n",
    "\n",
    "# for ind in range(y_pred_eval.shape[0]):\n",
    "#     line = str(indices[ind]) + \",\" + str(y_pred_eval[ind,0]) + \"|\" + str(y_pred_eval[ind,1])\n",
    "#     f.write(line)\n",
    "#     f.write(\"\\n\")\n",
    "# print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_idxs = (1,2,3,4,5,6,8,9,10,11,13,14)\n",
    "# X_train_normalized =  X_train_normalized.drop(columns=ph.get_column_names(['area'], acc_idxs))\n",
    "# X_valid_normalized = X_valid_normalized.drop(columns=ph.get_column_names(['area'], acc_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_tmax_rms_area_dropped = MLPRegressor(random_state=42, verbose=1, n_iter_no_change=500, max_iter=1500, learning_rate_init=0.001, activation=\"logistic\")\n",
    "# mlp_tmax_rms_area_dropped.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_tmax_rms_area_dropped = mlp_tmax_rms_area_dropped.predict(X_valid_normalized)\n",
    "# print(ph.mean_euclid_dist(y_valid, y_pred_tmax_rms_dropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_eval_area = (eval_noise_removed - means) / stds\n",
    "\n",
    "# print(eval_noise_removed.columns)\n",
    "\n",
    "# normalized_eval_area =  eval_noise_removed.drop(columns=ph.get_column_names(['tmax', 'rms', 'area'], acc_idxs))\n",
    "\n",
    "# y_pred_eval = mlp_tmax_rms_area_dropped.predict(normalized_eval_area)\n",
    "\n",
    "# f = open(\"predictions/no_outliers.csv\", 'w')\n",
    "# f.write(\"Id,Predicted\")\n",
    "# f.write(\"\\n\")\n",
    "\n",
    "# for ind in range(y_pred_eval.shape[0]):\n",
    "#     line = str(indices[ind]) + \",\" + str(y_pred_eval[ind,0]) + \"|\" + str(y_pred_eval[ind,1])\n",
    "#     f.write(line)\n",
    "#     f.write(\"\\n\")\n",
    "# print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_valid = pmax_negpmax_area[['x', 'y']].copy()\n",
    "\n",
    "# X_train_valid = pmax_negpmax_area.drop(columns=['x', 'y'])\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_valid = pmax_negpmax_area[['x', 'y']].copy()\n",
    "\n",
    "# area_idx = (1,2,3,4,5,6,8,9,10,11,13,14)\n",
    "# X_train_valid = pmax_negpmax_area.drop(columns=['x', 'y']).drop(columns=ph.get_column_names(['area'], area_idx))\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle=True, random_state=42, train_size=0.9)\n",
    "\n",
    "# print(X_valid.shape[0] / (X_train.shape[0] + X_valid.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = X_train.mean()\n",
    "# stds = X_train.std()\n",
    "# X_train_normalized = (X_train - means) / stds\n",
    "\n",
    "# print(X_train_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = MLPRegressor(random_state=42, verbose=1, n_iter_no_change=500, max_iter=1500, learning_rate_init=0.0001, activation=\"logistic\")\n",
    "# mlp.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_valid_normalized = (X_valid - means) / stds\n",
    "# y_hat = mlp.predict(X_valid_normalized)\n",
    "# mlp_med = ph.mean_euclid_dist(y_valid, y_hat)\n",
    "# print(mlp_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_5(n):\n",
    "    return np.round(n / 5) * 5\n",
    "\n",
    "\n",
    "rounding_vectorized = np.vectorize(round_to_nearest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ph.mean_euclid_dist(y_valid, y_pred))\n",
    "# print(ph.mean_euclid_dist(y_valid, y_pred_tmax_rms_dropped))\n",
    "# print(ph.mean_euclid_dist(y_valid, y_pred_tmax_rms_area_dropped))\n",
    "# print(ph.mean_euclid_dist(y_valid, round_to_nearest_5(y_pred)))\n",
    "# print(ph.mean_euclid_dist(y_valid, round_to_nearest_5(y_pred_tmax_rms_dropped)))\n",
    "# print(ph.mean_euclid_dist(y_valid, round_to_nearest_5(y_pred_tmax_rms_area_dropped)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ev = pd.read_csv(\"./evaluation.csv\")\n",
    "\n",
    "# print(df_ev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_indexes = [0,7,12,15,16,17]\n",
    "# features = [\"pmax\", \"negpmax\", 'area', 'tmax', 'rms']\n",
    "\n",
    "# eval_noise_removed = df_ev.drop(columns=ph.get_column_names(features, noise_indexes))\n",
    "\n",
    "# print(eval_noise_removed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = eval_noise_removed[\"Id\"].copy()\n",
    "# eval_noise_removed = eval_noise_removed.drop(columns = [\"Id\"])\n",
    "# print(eval_noise_removed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_idxs = [1,2,3,4,5,6,8,9,10,11,13,14]\n",
    "# eval_noise_removed = eval_noise_removed.drop(columns=ph.get_column_names(['tmax', 'rms'], acc_idxs))\n",
    "# print(eval_noise_removed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_eval = (eval_noise_removed - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_eval = mlp_tmax_rms_dropped.predict(normalized_eval)\n",
    "# print(y_pred_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"predictions/no_outliers_tmax_rms_area.csv\", 'w')\n",
    "# f.write(\"Id,Predicted\")\n",
    "# f.write(\"\\n\")\n",
    "\n",
    "# for ind in range(y_pred_eval.shape[0]):\n",
    "#     line = str(indices[ind]) + \",\" + str(y_pred_eval[ind,0]) + \"|\" + str(y_pred_eval[ind,1])\n",
    "#     f.write(line)\n",
    "#     f.write(\"\\n\")\n",
    "# print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "# rf.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_valid = pmax_negpmax_area[['x', 'y']].copy()\n",
    "\n",
    "# X_train_valid = pmax_negpmax_area.drop(columns=['x', 'y'])\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle=True, random_state=42)\n",
    "\n",
    "# noise_indexes = [0,7,12,15,16,17]\n",
    "# features = [\"pmax\", \"negpmax\", 'area', 'tmax', 'rms']\n",
    "\n",
    "# noise_removed = subset.drop(columns=ph.get_column_names(features, noise_indexes))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means_outliers_removed = X_train_outliers_removed.mean()\n",
    "# stds_outliers_removed = X_train_outliers_removed.std()\n",
    "# X_train_normalized = (X_train_outliers_removed - means) / stds\n",
    "\n",
    "# print(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = MLPRegressor(random_state=42, verbose=1, n_iter_no_change=500, max_iter=1500, learning_rate_init=0.001, activation=\"logistic\")\n",
    "# mlp.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat_outliers_removed = mlp.predict(X_valid_normalized)\n",
    "# print(ph.mean_euclid_dist(y_valid, y_hat_outliers_removed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
